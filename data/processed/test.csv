text,label
"Loss functions are fundamental to learning accurate 3D point cloud models, yet common choices trade geometric fidelity for computational cost. Chamfer Distance is efficient but permits many-to-one correspondences, while Earth Mover Distance better reflects one-to-one transport at high computational cost. APML approximates transport with differentiable Sinkhorn iterations and an analytically derived temperature, but its dense formulation scales quadratically in memory. We present CUDA-APML, a sparse GPU implementation that thresholds negligible assignments and runs adaptive softmax, bidirectional symmetrization, and Sinkhorn normalization directly in COO form. This yields near-linear memory scaling and preserves gradients on the stored support, while pairwise distance evaluation remains quadratic in the current implementation. On ShapeNet and MM-Fi, CUDA-APML matches dense APML within a small tolerance while reducing peak GPU memory by 99.9%. Code available at: https://github.com/Multimodal-Sensing-Lab/apml",human
"Many recommender systems in long-form video streaming reply on batch-trained models and batch-updated features, where user features are updated daily and served statically throughout the day. While efficient, this approach fails to incorporate a user's most recent actions, often resulting in stale recommendations. In this work, we present a lightweight, model-agnostic approach for intra-day personalization that selectively injects recent watch history at inference time without requiring model retraining. Our approach selectively overrides stale user features at inference time using the recent watch history, allowing the system to adapt instantly to evolving preferences. By reducing the personalization feedback loop from daily to intra-day, we observed a statistically significant 0.47% increase in key user engagement metrics which ranked among the most substantial engagement gains observed in recent experimentation cycles. To our knowledge, this is the first published evidence that intra-day personalization can drive meaningful impact in long-form video streaming service, providing a compelling alternative to full real-time architectures where model retraining is required.",human
"Graph Neural Networks (GNNs) have emerged as a powerful paradigm for learning representations of graph-structured data, finding applications across diverse domains. However, training deep GNNs remains a challenge, often plagued by issues such as over-smoothing and vanishing gradients, hindering their ability to capture long-range dependencies and complex graph patterns. This paper addresses the problem of enhancing the representational capacity and trainability of deep GNNs by introducing a novel architecture based on Spectral Graph Wavelets (SGWs). Our method, SpectralWaveletGNN, leverages the multi-resolution analysis capabilities of SGWs to decompose graph signals into components representing different frequency bands. These components are then processed by separate, specialized GNN layers, allowing the network to learn features at various scales. Crucially, SpectralWaveletGNN incorporates residual connections and a learnable wavelet scale parameter to mitigate over-smoothing and facilitate gradient flow. Experimental results on several benchmark graph datasets, including Cora, Citeseer, and PubMed, demonstrate that SpectralWaveletGNN consistently outperforms state-of-the-art GNN architectures, achieving significant improvements in node classification accuracy and demonstrating enhanced robustness to over-smoothing effects. Ablation studies validate the effectiveness of the individual components of our architecture, highlighting the importance of multi-resolution analysis and learnable wavelet scales for deep GNN training.",ai
"While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level ""delta action"" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",human
"Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by -- and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.",human
"Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present , a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves (undergraduate-level), (graduate-level), and (PhD-level) problems. Notably, using our system, we solved from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.",human
"We address the challenge of efficiently solving large-scale Boolean satisfiability (SAT) instances with complex clause structures, a problem prevalent in various domains including formal verification, planning, and hardware design. Existing SAT solvers often struggle with instances exhibiting significant variable interdependencies and long-range clause interactions, leading to exponential runtime complexity. We propose a novel approach leveraging a hybrid reasoning framework integrating conflict-driven clause learning (CDCL) with a probabilistic graphical model, specifically a Markov Random Field (MRF). The CDCL component provides the core search mechanism, while the MRF learns and represents statistical dependencies between variables based on the evolving clause database. This learned structure is then utilized to guide variable selection heuristics within CDCL, prioritizing variables that are statistically more likely to contribute to solution or conflict. Empirical evaluation on a benchmark suite of industrial SAT instances demonstrates a significant performance improvement compared to state-of-the-art CDCL solvers. Specifically, our hybrid solver achieves a 15-25% reduction in runtime and solves a greater number of instances within a fixed time limit, indicating its effectiveness in navigating complex search spaces and exploiting underlying variable dependencies.",ai
"We address the challenge of sample inefficiency in reinforcement learning (RL) for complex, sparse-reward environments. Specifically, we consider the problem of efficiently learning control policies in environments where successful task completion is infrequent and exploration is difficult. To mitigate this, we propose a novel hierarchical RL framework that integrates intrinsic motivation based on state visitation entropy with a learned option critic. The lower level consists of a set of option policies trained to maximize exploration by encouraging diverse state transitions, quantified by a learned approximation of state visitation entropy. The higher level option critic then learns to select these options to optimize the extrinsic task reward. This allows the agent to first learn a diverse repertoire of exploratory behaviors, followed by efficient learning of the final task policy. We evaluate our approach on a set of challenging benchmark tasks with sparse rewards, demonstrating a significant improvement in sample efficiency compared to state-of-the-art RL algorithms, including both on-policy and off-policy methods. Furthermore, we show that the learned options can be transferred to related tasks, providing a basis for continual learning and generalization.",ai
"We investigate the problem of training machine learning models with rigorous differential privacy guarantees in the challenging setting of high-dimensional data. Existing differentially private stochastic gradient descent (DP-SGD) methods often suffer from significant utility degradation when applied to models with a large number of parameters, especially when the privacy budget is constrained. To address this limitation, we propose a novel approach, Differentially Private Adaptive Clipping and Noise (DP-ACN), which dynamically adjusts the clipping norm and noise scale during training. DP-ACN leverages a novel adaptive mechanism to estimate the optimal clipping norm based on the magnitude of gradients encountered during training. Furthermore, we introduce a correlated noise injection scheme that reduces the variance introduced by the privacy mechanism. We provide a rigorous theoretical analysis demonstrating that DP-ACN achieves tighter privacy bounds and improved convergence compared to standard DP-SGD and its variants. Empirical evaluations on several benchmark datasets, including image classification and language modeling tasks, demonstrate that DP-ACN significantly outperforms existing state-of-the-art DP-SGD approaches, achieving comparable utility with substantially lower privacy loss or superior utility with a fixed privacy budget. These results highlight the effectiveness of adaptive clipping and correlated noise in mitigating the trade-off between privacy and utility in high-dimensional differentially private machine learning.",ai
"Recent advances in natural language processing have demonstrated the potential of reinforcement learning (RL) for complex sequence generation tasks. However, optimizing language models with RL remains challenging due to the sparse and delayed rewards associated with evaluating the quality of generated text. This paper addresses the problem of sample inefficiency in RL-based language generation by introducing a novel curriculum learning framework that leverages pre-trained language models as a source of shaped reward signals. Our method, Curriculum-Guided Reinforcement Learning (CG-RL), first trains a policy to imitate the behavior of a pre-trained language model on a simplified task with dense rewards, iteratively increasing the complexity of the task and gradually transitioning towards the target environment with sparse rewards. Specifically, we employ a progressive unfreezing strategy, enabling the policy network to progressively learn from the pre-trained model's representations while simultaneously adapting to the nuances of the RL environment. We evaluate CG-RL on a text summarization task, where the goal is to generate concise and informative summaries of long documents. Experimental results demonstrate that CG-RL significantly outperforms standard RL training and imitation learning baselines in terms of both ROUGE scores and human evaluation metrics, suggesting that the curriculum learning approach effectively mitigates the sample inefficiency problem and leads to the discovery of higher-quality generation policies.",ai
"Robust optimization (RO) offers a framework for decision-making under uncertainty by optimizing against the worst-case realization of uncertain parameters within a predefined uncertainty set. A primary challenge in applying RO lies in the conservatism induced by these sets, often leading to suboptimal solutions in practice, particularly when the assumed uncertainty significantly deviates from observed data distributions. This paper introduces a novel data-driven robust optimization (DDRO) methodology that adaptively shapes the uncertainty set based on statistical inference from available data. Specifically, we propose a distributionally robust optimization (DRO) formulation where the ambiguity set is constructed using a kernel density estimate (KDE) of the empirical distribution. This allows for non-parametric estimation of the underlying probability density function, enabling a more flexible and adaptive representation of the uncertainty. We derive tractable reformulations of the resulting DRO problem using duality theory, leading to computationally efficient solution algorithms. Empirical results on benchmark portfolio optimization and inventory management problems demonstrate that our KDE-based DDRO method achieves significant improvements in out-of-sample performance compared to traditional RO approaches and competing DDRO techniques utilizing simpler ambiguity sets (e.g., box or ellipsoidal). The method exhibits robustness against misspecification of the uncertainty set while simultaneously reducing the degree of conservatism inherent in conventional RO.",ai
"A plethora of dimension reduction methods have been developed to visualize high-dimensional data in low dimensions. However, different dimension reduction methods often output different and possibly conflicting visualizations of the same data. This problem is further exacerbated by the choice of hyperparameters, which may substantially impact the resulting visualization. To obtain a more robust and trustworthy dimension reduction output, we advocate for a consensus approach, which summarizes multiple visualizations into a single consensus dimension reduction visualization. Here, we leverage ideas from multi-view learning in order to identify the patterns that are most stable or shared across the many different dimension reduction visualizations, or views, and subsequently visualize this shared structure in a single low-dimensional plot. We demonstrate that this consensus visualization effectively identifies and preserves the shared low-dimensional data structure through both simulated and real-world case studies. We further highlight our method's robustness to the choice of dimension reduction method and hyperparameters -- a highly-desirable property when working towards trustworthy and reproducible data science.",human
"We investigate the problem of automated reasoning within large knowledge graphs possessing inherent logical inconsistencies, a scenario frequently encountered in real-world datasets. Traditional reasoning techniques, such as rule-based engines and description logic reasoners, often falter in the presence of such inconsistencies, leading to unsound or incomplete inferences. This work proposes a novel method leveraging probabilistic soft logic (PSL) enhanced with adaptive weighting of constraints based on evidential support from the knowledge graph. Specifically, we introduce a mechanism that automatically adjusts the weights of logical rules according to the confidence associated with the facts used to instantiate those rules, derived from link prediction scores. This allows the system to prioritize more reliable information, mitigating the impact of conflicting evidence. We evaluate our approach on benchmark knowledge graphs containing known inconsistencies, demonstrating significant improvements in both precision and recall of inferred facts compared to standard PSL implementations and baseline rule-based systems. Empirical results showcase the method's robustness in handling noisy and contradictory information, yielding a substantial increase in the F1-score for link prediction tasks while maintaining computational efficiency. The findings indicate the potential of adaptive constraint weighting within PSL to enhance the accuracy and reliability of automated reasoning over large-scale, imperfect knowledge bases.",ai
"We address the challenge of automated detection and segmentation of pulmonary nodules in computed tomography (CT) scans, a critical task for early lung cancer diagnosis. Manual analysis of these scans is time-consuming and prone to inter-observer variability. Existing automated methods often struggle with nodules of varying sizes, shapes, and densities, particularly those located near the pleura or within complex anatomical structures. We propose a novel deep learning framework, termed Multi-Scale Attentive Segmentation Network (MSASN), leveraging a 3D U-Net architecture enhanced with multi-scale feature aggregation and spatial attention mechanisms. The multi-scale feature aggregation module captures information at different resolutions, enabling robust detection of nodules across a range of sizes. The spatial attention mechanism refines segmentation by adaptively weighting voxel contributions based on their relevance to the nodule boundaries. MSASN was trained and validated on the publicly available LUNA16 dataset. Experimental results demonstrate that MSASN achieves a state-of-the-art performance, with a competitive Free-Response Receiver Operating Characteristic (FROC) score of 0.897 at 1/8 false positives per scan. Furthermore, MSASN exhibits improved segmentation accuracy, achieving a Dice score of 0.782, surpassing existing methods by a significant margin. These results highlight the potential of MSASN for assisting radiologists in lung cancer screening and improving diagnostic accuracy.",ai
"We investigate the problem of enhancing the logical consistency of large language models (LLMs) in automated reasoning tasks. While LLMs exhibit impressive generative capabilities, their reasoning processes often suffer from inconsistencies when dealing with complex logical statements. Our method, Consistency-Reinforced Reasoning (CRR), leverages a two-stage approach. First, we employ a self-consistency decoding strategy, generating multiple candidate reasoning paths for a given problem. Second, we introduce a differentiable module that analyzes the logical entailment relationships between the steps in each reasoning path. This module penalizes inconsistencies detected across the path and assigns a consistency score, allowing for the selection of the most logically sound reasoning sequence. We evaluate CRR on challenging benchmark datasets requiring multi-hop logical inference, including TheoremQA and ProofWriter. Our results demonstrate a significant improvement in logical consistency and overall accuracy compared to standard LLM inference techniques, achieving state-of-the-art performance. Ablation studies highlight the effectiveness of both the self-consistency decoding and the differentiable consistency module in achieving these gains. Furthermore, we show that CRR exhibits improved robustness to adversarial perturbations designed to induce logical errors, suggesting a more reliable reasoning process.",ai
"We address the problem of training machine learning models on decentralized datasets while preserving the privacy of individual data owners. Federated learning (FL) offers a promising solution, yet existing FL algorithms are susceptible to privacy attacks, particularly inference attacks that reveal sensitive attributes of participating clients. To mitigate this risk, we propose a novel differentially private federated learning (DP-FL) framework incorporating a client-level privacy mechanism based on Gaussian differential privacy. Our method, termed DP-FedAvg-G, introduces a calibrated Gaussian noise injection at the client level to perturb local model updates before aggregation. We derive tight privacy bounds using Rényi Differential Privacy (RDP) analysis to accurately quantify the privacy loss incurred during the training process. Extensive experiments on benchmark datasets, including CIFAR-10 and MNIST, demonstrate that DP-FedAvg-G achieves a favorable trade-off between privacy and utility. Specifically, we observe a significant reduction in privacy leakage compared to non-private FL, while maintaining comparable model accuracy under varying levels of noise and data heterogeneity. Furthermore, we analyze the impact of different privacy parameters (ε, δ) on model performance, providing guidance for selecting appropriate privacy levels in practical applications. Our findings suggest that DP-FedAvg-G provides a robust and effective approach for privacy-preserving collaborative learning.",ai
"We address the problem of learning causal relationships from observational data in the presence of unobserved confounders and non-linear relationships. Traditional causal discovery algorithms often struggle with such complexities, leading to spurious causal inferences. To mitigate this, we propose a novel approach, Confounded Non-linear Causal Discovery (CNCD), which integrates a non-parametric causal function estimator with a latent variable discovery framework. CNCD leverages reproducing kernel Hilbert spaces (RKHS) to model complex, non-linear functional relationships between variables. Simultaneously, it employs a variational autoencoder (VAE) to infer latent confounding variables that explain residual dependencies unexplained by direct causal links. A key contribution lies in the joint optimization of causal structure learning and latent variable inference. The causal graph structure is learned via a score-based optimization procedure guided by the Hilbert-Schmidt Independence Criterion (HSIC), penalizing complex structures and favoring parsimony. We evaluate CNCD on synthetic datasets exhibiting varying degrees of non-linearity and confounding, demonstrating superior performance in recovering the true causal graph compared to state-of-the-art methods, including those based on linear assumptions or separate latent variable and causal structure inference. Furthermore, we apply CNCD to a real-world biological dataset, revealing plausible causal relationships consistent with domain knowledge, suggesting its practical utility in complex causal inference tasks.",ai
"We address the problem of enhancing logical inference in complex, multi-hop reasoning scenarios, specifically focusing on scenarios where intermediate steps require nuanced contextual understanding. Current automated reasoning systems often struggle with such problems due to limitations in capturing subtle semantic relationships between premises. We propose a novel approach, Contextual Graph Reasoning (CGR), which leverages graph neural networks to construct and traverse inference graphs, incorporating contextual information at each node representing a logical statement. This context is derived from transformer-based language models, fine-tuned on a task-specific corpus of logical arguments and counter-arguments. CGR differs from traditional graph-based reasoning methods by dynamically updating node embeddings based on the current inference path, allowing for adaptive and context-aware reasoning. We evaluate CGR on the ReClor and LogiQA datasets, benchmarks designed to assess logical reasoning capabilities in complex natural language scenarios. Our experiments demonstrate that CGR achieves significant performance gains compared to state-of-the-art baselines, yielding a 7.2% improvement in accuracy on ReClor and a 4.9% improvement on LogiQA. Ablation studies further confirm the importance of the contextual embedding updates and the graph traversal strategy in achieving these results, highlighting the efficacy of CGR in capturing and utilizing contextual information for enhanced automated reasoning.",ai
"Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.",human
"We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to and the step size goes to commensurately. Our main result is that these ODE limits are universal in that this limit is the same whenever the data is drawn from mixtures of arbitrary product distributions whose first two moments match the corresponding Gaussian distribution, provided the initialization and ground truth vectors are coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal.",human
"## Abstract We address the challenge of sample inefficiency in reinforcement learning (RL) for complex, sparse-reward environments, particularly those requiring long-term planning. Conventional RL algorithms often struggle to explore effectively in such settings, leading to protracted training times and suboptimal policies. To mitigate this, we propose a novel framework integrating a hierarchical goal-conditioned reinforcement learning (HGCRL) architecture with a learned intrinsic motivation module. The HGCRL decomposes the task into a hierarchy of sub-goals, facilitating efficient exploration by providing intermediate rewards and focusing the agent's attention on relevant state transitions. The intrinsic motivation module is trained to predict the change in the agent's state representation following an action, thereby encouraging the exploration of novel and informative state regions. We evaluate our approach on a suite of challenging simulated robotic manipulation tasks characterized by sparse rewards and long planning horizons. Experimental results demonstrate that our proposed method significantly outperforms state-of-the-art RL algorithms, achieving a substantial improvement in sample efficiency and final policy performance. Specifically, our agent learns effective policies in environments where baseline algorithms fail to make progress, showcasing the efficacy of our hierarchical, intrinsically motivated exploration strategy. The learned intrinsic reward signal guides the agent towards regions of high learning potential, leading to faster convergence and a more robust final policy.",ai
"Document chunking is a crucial component of Retrieval-Augmented Generation (RAG), as it directly affects the retrieval of relevant and precise context. Conventional fixed-length and recursive splitters often produce arbitrary, incoherent segments that fail to preserve semantic structure. Although semantic chunking has gained traction, its influence on generation quality remains underexplored. This paper introduces two efficient semantic chunking methods, Projected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), trained on PubMed data using three different embedding models. We further present an evaluation framework that measures the effect of chunking on both retrieval and generation by augmenting PubMedQA with full-text PubMed Central articles. Our results show substantial retrieval improvements (24x with PSC) in MRR and higher Hits@k on PubMedQA. We provide a comprehensive analysis, including statistical significance and response-time comparisons with common chunking libraries. Despite being trained on a single domain, PSC and MFC also generalize well, achieving strong out-of-domain generation performance across multiple datasets. Overall, our findings confirm that our semantic chunkers, especially PSC, consistently deliver superior performance.",human
"Dialogue Topic Segmentation (DTS) is crucial for understanding task-oriented public-channel communications, such as maritime VHF dialogues, which feature informal speech and implicit transitions. To address the limitations of traditional methods, we propose DASH-DTS, a novel LLM-based framework. Its core contributions are: (1) topic shift detection via dialogue handshake recognition; (2) contextual enhancement through similarity-guided example selection; and (3) the generation of selective positive and negative samples to improve model discrimination and robustness. Additionally, we release VHF-Dial, the first public dataset of real-world maritime VHF communications, to advance research in this domain. DASH-DTS provides interpretable reasoning and confidence scores for each segment. Experimental results demonstrate that our framework achieves several sota segmentation trusted accuracy on both VHF-Dial and standard benchmarks, establishing a strong foundation for stable monitoring and decision support in operational dialogues.",human
"## Emergent Communication and Coordination via Differentiable Message Passing in Partially Observable Multi-Agent Environments Achieving effective communication and coordination in multi-agent systems operating under partial observability remains a significant challenge. Existing approaches often rely on hand-crafted communication protocols or centralized training schemes, limiting their adaptability and scalability. This work introduces a novel differentiable message passing framework that enables agents to learn communication strategies alongside their individual policies in a fully decentralized manner. The framework leverages a graph neural network to encode local observations and generate latent messages. These messages are then exchanged between agents based on learned communication graphs, allowing for adaptive information sharing. We evaluate our approach on benchmark cooperative navigation and cooperative communication tasks in partially observable environments. Experimental results demonstrate that our framework achieves significantly improved coordination and performance compared to independent learners and existing communication protocols. Furthermore, we show that the learned communication graphs reflect meaningful relationships between agents and tasks, providing insights into the emergent communication strategies. This framework provides a robust and scalable approach for developing intelligent multi-agent systems capable of effective communication and coordination under realistic conditions.",ai
"We address the challenge of sample inefficiency in reinforcement learning (RL) for continuous control tasks with sparse rewards. Existing deep RL algorithms often require a prohibitive number of environment interactions to achieve satisfactory performance, particularly when the reward signal is infrequent. We propose a novel method, Curriculum-Guided Exploration with Self-Supervised Representation Learning (CGESR), that integrates curriculum learning with self-supervised representation learning to accelerate exploration and improve data efficiency. CGESR initially trains an agent on a simplified version of the target environment with a shaped reward function, designed to gradually increase in complexity and sparsity until it matches the original task. Simultaneously, a self-supervised autoencoder learns a low-dimensional, task-agnostic representation of the environment state space, enabling the agent to generalize across different curriculum stages. Experimental results on challenging MuJoCo locomotion tasks demonstrate that CGESR significantly outperforms state-of-the-art RL algorithms, including PPO, SAC, and TD3, in terms of sample efficiency and asymptotic performance. CGESR achieves a 2-5x reduction in the number of environment interactions required to reach similar levels of performance, highlighting its effectiveness in addressing the sample inefficiency problem in sparse-reward continuous control. Further ablation studies validate the contribution of both the curriculum learning and self-supervised representation learning components.",ai
"We investigate the problem of training machine learning models that exhibit robustness to adversarial perturbations of input data, focusing on scenarios with limited computational resources. Existing robust optimization methods often incur significant computational overhead, hindering their applicability to large-scale datasets and complex models. We propose a novel adaptive robust training framework that dynamically adjusts the perturbation size during training based on the model's instantaneous sensitivity. This adaptive approach aims to concentrate computational effort on regions of the input space where the model is most vulnerable, thereby improving robust generalization while minimizing computational cost. Our method leverages a computationally efficient approximation of the Lipschitz constant to estimate model sensitivity and modulate the perturbation budget. We conduct extensive experiments on standard image classification benchmarks (CIFAR-10, CIFAR-100) using convolutional neural networks. The results demonstrate that our adaptive robust training framework achieves comparable, and in some cases superior, robust accuracy compared to state-of-the-art robust optimization techniques, while significantly reducing training time and memory requirements. Specifically, we observe a reduction in training time of up to 30% compared to standard projected gradient descent (PGD) based robust training, without compromising robust accuracy against strong adversarial attacks. These findings suggest that adaptive robust optimization provides a promising pathway for deploying robust machine learning models in resource-constrained environments.",ai
"Large Language Models (LLMs) can perform many NLP tasks well, but fully fine-tuning them is expensive and requires a lot of memory. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA reduce this cost by adding small low-rank updates to frozen model weights. However, these methods restrict the training to a limited subspace, which can sometimes reduce performance. For Small Language Models (SLMs), where efficiency gains matter even more, we introduce AdaGradSelect, an adaptive method that selects which transformer blocks to update based on gradients. Early observations showed that updating only the transformer blocks with the highest gradient norms can achieve performance close to full fine-tuning. Building on this insight, AdaGradSelect adaptively chooses which blocks to train. It uses a combination of Dirichlet-based sampling, which depends on how frequently blocks were updated in the past, and an epsilon-greedy exploration strategy. This lets the method explore different blocks in early training and gradually focus on the most important ones in later epochs. Experiments show that AdaGradSelect trains about 12 percent faster and uses 35 percent less GPU memory while delivering performance very close to full fine-tuning. On the GSM8K dataset, it outperforms LoRA (rank 256) by about 3 percent on average across models such as Qwen2.5-0.5B, LLaMA3.2-1B, and Phi4-mini-3.8B. It also achieves similar accuracy on the MATH dataset. Overall, AdaGradSelect provides a more effective and resource-efficient alternative to traditional fine-tuning methods.",human
"We address the challenge of coordinating diverse agents in complex, partially observable environments. Current multi-agent reinforcement learning (MARL) algorithms often struggle with scalability and generalization in scenarios where agents possess heterogeneous capabilities and limited communication bandwidth. To mitigate these limitations, we introduce a novel decentralized policy learning framework predicated on a hierarchical attention mechanism. Our approach, termed Hierarchical Attention-based Policy Synthesis (HAPS), enables agents to selectively attend to relevant information from both local observations and aggregated higher-level representations of the global state. This allows for efficient credit assignment and the development of robust coordination strategies. Furthermore, HAPS incorporates a learned communication protocol that adapts to the agents' capabilities and the environmental context, optimizing information sharing while respecting bandwidth constraints. We evaluate HAPS on several challenging cooperative navigation and resource allocation tasks, demonstrating significant performance improvements over state-of-the-art MARL baselines. Specifically, HAPS achieves superior coordination efficiency, faster convergence, and improved generalization to novel environment configurations, highlighting its potential for application in real-world multi-agent systems. Quantitative results show an average increase of 15% in task completion rate compared to the best performing baseline across all evaluated environments.",ai
"Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.",human
"Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1",human
"We investigate a novel approach to robust optimization for machine learning models under distributional uncertainty, specifically addressing scenarios where the nominal data distribution is corrupted by adversarial perturbations affecting a subset of features. Existing robust optimization techniques often rely on distributional robustness bounds that can be overly conservative, leading to suboptimal performance on clean data. We propose a data-driven robust optimization framework, termed Feature-Specific Distributionally Robust Optimization (FS-DRO), which estimates ambiguity sets tailored to individual feature subsets. This is achieved by leveraging a differentiable proxy for feature attribution, enabling the computation of feature importance scores directly within the training loop. The estimated feature importances guide the construction of feature-specific Wasserstein ambiguity sets, effectively isolating the robustness constraint to only those features most vulnerable to adversarial attacks. We demonstrate that FS-DRO achieves superior empirical performance compared to standard DRO and nominal training on both synthetic and real-world datasets subject to feature-wise adversarial perturbations. Furthermore, we provide theoretical bounds on the suboptimality of FS-DRO, demonstrating its improved sample complexity compared to traditional DRO approaches. Our results suggest that feature-specific robustness constraints offer a promising avenue for developing more effective and efficient robust machine learning models.",ai
"Detecting sarcasm remains a challenging task in the areas of Natural Language Processing (NLP) despite recent advances in neural network approaches. Currently, Pre-trained Language Models (PLMs) and Large Language Models (LLMs) are the preferred approach for sarcasm detection. However, the complexity of sarcastic text, combined with linguistic diversity and cultural variation across communities, has made the task more difficult even for PLMs and LLMs. Beyond that, those models also exhibit unreliable detection of words or tokens that require extra grounding for analysis. Building on a state-of-the-art prompting method in LLMs for sarcasm detection called Pragmatic Metacognitive Prompting (PMP), we introduce a retrieval-aware approach that incorporates retrieved contextual information for each target text. Our pipeline explores two complementary ways to provide context: adding non-parametric knowledge using web-based retrieval when the model lacks necessary background, and eliciting the model's own internal knowledge for a self-knowledge awareness strategy. We evaluated our approach with three datasets, such as Twitter Indonesia Sarcastic, SemEval-2018 Task 3, and MUStARD. Non-parametric retrieval resulted in a significant 9.87% macro-F1 improvement on Twitter Indonesia Sarcastic compared to the original PMP method. Self-knowledge retrieval improves macro-F1 by 3.29% on Semeval and by 4.08% on MUStARD. These findings highlight the importance of context in enhancing LLMs performance in sarcasm detection task, particularly the involvement of culturally specific slang, references, or unknown terms to the LLMs. Future work will focus on optimizing the retrieval of relevant contextual information and examining how retrieval quality affects performance. The experiment code is available at: https://github.com/wllchrst/sarcasm-detection_pmp_knowledge-base.",human
"Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.",human
"Graph Neural Networks (GNNs) have emerged as a powerful paradigm for learning representations of graph-structured data, yet their performance is often limited by issues such as over-smoothing and inadequate node feature utilization. This work addresses the challenge of enhancing GNN expressiveness and robustness by proposing a novel architecture, the Adaptive Feature Aggregation Graph Neural Network (AFA-GNN). AFA-GNN introduces an adaptive aggregation mechanism that dynamically weights the contributions of neighboring nodes’ features based on learned attention coefficients. These coefficients are derived from a multi-layer perceptron (MLP) that considers both node and edge attributes, enabling the network to focus on relevant connections and features during message passing. Furthermore, AFA-GNN incorporates a residual connection scheme that allows for the retention of original node features, mitigating the over-smoothing problem commonly encountered in deep GNNs. We evaluate AFA-GNN on a diverse set of benchmark datasets for node classification and graph classification tasks, including citation networks (Cora, CiteSeer, PubMed) and social networks (Reddit, IMDB-BINARY). Empirical results demonstrate that AFA-GNN consistently outperforms state-of-the-art GNN architectures, exhibiting significant improvements in classification accuracy and generalization ability. Ablation studies further validate the efficacy of the adaptive aggregation and residual connection components.",ai
"We address the problem of improving automated theorem proving performance in first-order logic with equality through learned premise selection and proof guidance. Existing systems often struggle with large, complex theories, leading to inefficient search and failed proof attempts. Our method introduces a novel neural architecture, the Graph Reasoning Network (GRN), designed to encode the relationships between clauses in a first-order theory as a graph, leveraging message passing to propagate information and identify relevant premises for a given conjecture. The GRN is trained using reinforcement learning, where the reward signal is derived from the success or failure of a theorem prover using the GRN-suggested premise selection strategy and proof actions. We further incorporate a learned proof guidance mechanism that biases the search strategy within the theorem prover based on the evolving proof state, effectively pruning unproductive branches. Empirical evaluation on a benchmark suite of mathematical theorems demonstrates a significant improvement in both proof success rate and proof length compared to state-of-the-art automated theorem provers using heuristic-based premise selection and unguided search. Specifically, we observe a relative increase of 15% in the number of theorems proven and a 20% reduction in average proof length across a diverse set of mathematical domains. These results suggest that learned reasoning strategies can substantially enhance the capabilities of automated theorem proving systems.",ai
"Constructing a Pareto set is pivotal for navigating the capability-efficiency trade-offs in Large Language Models (LLMs); however, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the ""curse of dimensionality,"" rendering the search space computationally intractable. To resolve this dichotomy, we propose BAMBO (Bayesian Adaptive Multi-objective Block-wise Optimization), a novel framework that automatically constructs the LLM Pareto set. BAMBO renders the search tractable by introducing a Hybrid Optimal Block Partitioning strategy. Formulated as a 1D clustering problem, this strategy leverages a dynamic programming approach to optimally balance intra-block homogeneity and inter-block information distribution, thereby dramatically reducing dimensionality without sacrificing critical granularity. The entire process is automated within an evolutionary loop driven by the q-Expected Hypervolume Improvement (qEHVI) acquisition function. Experiments demonstrate that BAMBO discovers a superior and more comprehensive Pareto frontier than baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/xin8coder/BAMBO.",human
"We address the challenge of emergent communication complexity in multi-agent reinforcement learning (MARL) systems operating in partially observable environments. Specifically, we focus on scenarios where agents must coordinate to achieve a common goal but are limited in their ability to directly observe the world state or the actions of other agents. Existing approaches often struggle with generating efficient and interpretable communication protocols, leading to suboptimal performance or a reliance on hand-engineered communication channels. We propose a novel MARL framework that incorporates a differentiable symbolic communication module, trained jointly with the agents' policies. This module allows agents to communicate using discrete, symbolic messages which are decoded and incorporated into the agents’ decision-making processes. We employ a combination of reinforcement learning and variational autoencoders to encourage the emergence of compositional and informative communication protocols. Furthermore, we introduce a regularisation term to promote sparsity in the communication channel, thereby reducing the dimensionality of the message space and improving interpretability. We evaluate our framework on a challenging cooperative navigation task and demonstrate that it significantly outperforms baseline MARL algorithms that rely on continuous communication or no communication at all. Empirical results indicate that the learned symbolic communication protocols enable more effective coordination among agents, leading to improved task completion rates and reduced communication overhead. Analysis of the emergent language reveals the development of compositional structures that map to specific environmental features and agent roles, offering insights into the underlying coordination strategies. We further show that the sparsity-inducing regularisation enhances the robustness of the communication protocols to noise and variations in the environment.",ai
"This work addresses the challenge of efficiently representing structured knowledge to facilitate robust reasoning and inference, particularly in domains characterized by uncertainty and incompleteness. Existing knowledge representation formalisms often struggle to balance expressiveness, computational tractability, and the ability to handle noisy or ambiguous data. We propose a novel framework, termed Attentive Relational Graph Embeddings (ARGE), which leverages graph neural networks and attention mechanisms to learn contextualized representations of entities and relations within a knowledge graph. ARGE incorporates a relational attention mechanism that dynamically weights the importance of neighboring entities and relations during the embedding process, enabling the model to focus on the most relevant aspects of the graph structure for a given inference task. Furthermore, we introduce a differentiable soft logic layer that allows for approximate reasoning over the learned embeddings, mitigating the impact of incomplete or inconsistent knowledge. Empirical evaluations on benchmark knowledge graph completion and link prediction tasks demonstrate that ARGE achieves state-of-the-art performance, significantly outperforming existing embedding-based and rule-based approaches. Ablation studies validate the effectiveness of both the relational attention mechanism and the soft logic layer in enhancing the model's ability to capture complex relational dependencies and handle uncertainty. The results suggest that ARGE provides a promising avenue for developing more robust and scalable knowledge representation systems.",ai
"Transformer models have achieved state-of-the-art performance across a wide range of natural language processing (NLP) tasks, yet their computational complexity presents a significant bottleneck for deployment in resource-constrained environments and scaling to longer sequences. This work investigates novel methods for improving the efficiency of transformer architectures while maintaining high levels of accuracy. We introduce a sparse attention mechanism based on learned orthogonal bases that reduces the quadratic complexity of standard attention to linearithmic complexity. This is achieved by projecting query and key vectors into a lower-dimensional space spanned by the learned orthogonal basis before computing attention weights, effectively pruning less relevant context. We further explore the application of knowledge distillation techniques, transferring knowledge from a large, pre-trained transformer model to a smaller, specialized model using our sparse attention mechanism. Experiments on benchmark NLP datasets, including text classification and machine translation, demonstrate that our proposed approach achieves comparable or improved performance relative to existing sparse attention methods, while significantly reducing computational cost. Specifically, our model achieves a 2x speedup in inference time with minimal loss in accuracy on a benchmark machine translation task, demonstrating the potential for efficient and scalable transformer-based NLP systems.",ai
"We present a method that uses the Bloom filter transform to preprocess data for machine learning. Each sample is encoded into a compact, privacy-preserving bit array. This reduces memory use and protects the original data while keeping enough structure for accurate classification. We test the method on six datasets: SMS Spam Collection, ECG200, Adult 50K, CDC Diabetes, MNIST, and Fashion MNIST. Four classifiers are used: Extreme Gradient Boosting, Deep Neural Networks, Convolutional Neural Networks, and Logistic Regression. Results show that models trained on Bloom filter encodings achieve accuracy similar to models trained on raw data or other transforms. At the same time, the method provides memory savings while enhancing privacy. These results suggest that the Bloom filter transform is an efficient preprocessing approach for diverse machine learning tasks.",human
"Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures. Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny Code: https://github.com/microsoft/ltp-megatron-lm",human
"We address the problem of *adversarial robustness* in natural language understanding (NLU) models, specifically focusing on the vulnerability of such models to imperceptible input perturbations. Existing defense strategies often prioritize robustness on specific, known attack surfaces, leading to limited generalization when faced with novel or more sophisticated adversarial examples. We propose a novel *distributionally robust optimization* (DRO) framework leveraging a Wasserstein ambiguity set to explicitly model uncertainty over possible input distributions. This framework allows us to optimize model performance against the *worst-case* distribution within a specified distance of the empirical training data. We introduce a computationally efficient algorithm based on stochastic gradient descent to solve the resulting DRO problem. Empirical evaluations on benchmark NLU datasets (e.g., SST-2, MNLI) demonstrate that our proposed DRO-based training significantly improves the robustness of NLU models against a variety of adversarial attacks, including synonym substitution, character-level perturbations, and universal adversarial triggers. Furthermore, our approach achieves superior generalization compared to existing adversarial training methods, evidenced by improved performance on unseen adversarial examples and maintaining strong performance on clean data. Our findings suggest that DRO provides a principled and effective approach for enhancing the robustness of NLU models in the face of adversarial attacks.",ai
"Hybrid quantum-classical models represent a crucial step toward leveraging near-term quantum devices for sequential data processing. We present Quantum Recurrent Neural Networks (QRNNs) and Quantum Convolutional Neural Networks (QCNNs) as hybrid quantum language models, reporting the first empirical demonstration of generative language modeling trained and evaluated end-to-end on real quantum hardware. Our architecture combines hardware-optimized parametric quantum circuits with a lightweight classical projection layer, utilizing a multi-sample SPSA strategy to efficiently train quantum parameters despite hardware noise. To characterize the capabilities of these models, we introduce a synthetic dataset designed to isolate syntactic dependencies in a controlled, low-resource environment. Experiments on IBM Quantum processors reveal the critical trade-offs between circuit depth and trainability, demonstrating that while noise remains a significant factor, observable-based readout enables the successful learning of sequential patterns on NISQ devices. These results establish a rigorous engineering baseline for generative quantum natural language processing, validating the feasibility of training complex sequence models on current quantum hardware.",human
"We address the challenge of sample-efficient reinforcement learning in environments with sparse and delayed rewards. Traditional reinforcement learning algorithms often struggle in such settings, requiring extensive exploration to discover rewarding trajectories. This paper introduces a novel hierarchical reinforcement learning framework, termed ""Modular Option Discovery with Intrinsic Rewards"" (MODIR), designed to overcome this limitation. MODIR decomposes the learning problem into two levels: a high-level policy learning to select reusable options and a low-level policy learning to execute these options. Critically, we introduce an intrinsic reward signal based on state visitation entropy and inter-option coherence to guide option discovery in the low-level. This encourages the exploration of diverse and useful state-space regions, facilitating faster learning of effective options. We further incorporate a modular neural network architecture to represent both the high-level and low-level policies, promoting knowledge transfer and generalization. Empirical evaluations on a suite of challenging benchmark tasks, including sparse-reward navigation and manipulation environments, demonstrate that MODIR significantly outperforms state-of-the-art reinforcement learning algorithms in terms of sample efficiency and asymptotic performance. Ablation studies confirm the effectiveness of the proposed intrinsic reward and modular architecture. These results suggest that hierarchical decomposition coupled with targeted intrinsic reward shaping can significantly improve the performance of reinforcement learning agents in complex and sparse-reward environments.",ai
"A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.",human
"We investigate the problem of sample inefficiency in reinforcement learning (RL) for sparse-reward environments, specifically focusing on robotic manipulation tasks. Traditional deep RL algorithms often struggle to learn effectively when rewards are infrequent, requiring a prohibitively large number of interactions with the environment. To address this challenge, we propose a novel framework, Hierarchical Exploration via Intrinsic Motivation and Skill Discovery (HEIMS), which integrates hierarchical reinforcement learning with intrinsic motivation signals and unsupervised skill discovery. HEIMS first learns a library of reusable skills through unsupervised exploration driven by an intrinsic curiosity reward based on state-novelty. Subsequently, a higher-level policy leverages these skills as building blocks to solve the target task, guided by the sparse extrinsic reward. This hierarchical structure allows for exploration at a coarser timescale, facilitating more efficient learning of long-horizon tasks. We evaluate HEIMS on a set of challenging robotic manipulation benchmarks with sparse rewards. Our results demonstrate that HEIMS significantly outperforms state-of-the-art RL baselines in terms of sample efficiency and final performance. Specifically, HEIMS achieves comparable or superior performance with up to an order of magnitude fewer environment interactions, highlighting the benefits of hierarchical exploration and skill reuse in sparse-reward settings.",ai
"Due to the restricted resources, efficient scheduling in vertiports has received much more attention in the field of Urban Air Mobility (UAM). For the scheduling problem, we utilize a Mixed Integer Linear Programming (MILP), which is often formulated in a resource-restricted project scheduling problem (RCPSP). In this paper, we show our approach to handle both dynamic operation requirements and vague rescheduling requests from humans. Particularly, we utilize a three-valued logic for interpreting ambiguous user intents and a decision tree, proposing a newly integrated system that combines Answer Set Programming (ASP) and MILP. This integrated framework optimizes schedules and supports human inputs transparently. With this system, we provide a robust structure for explainable, adaptive UAM scheduling.",human
"Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.",human
"Sycophantic response patterns in Large Language Models (LLMs) have been increasingly claimed in the literature. We review methodological challenges in measuring LLM sycophancy and identify five core operationalizations. Despite sycophancy being inherently human-centric, current research does not evaluate human perception. Our analysis highlights the difficulties in distinguishing sycophantic responses from related concepts in AI alignment and offers actionable recommendations for future research.",human
"We address the problem of automating complex logical inferences in first-order logic with equality, a domain traditionally requiring significant human expertise. Specifically, we focus on efficiently proving theorems within the TPTP (Thousands of Problems for Theorem Provers) benchmark library, which presents a diverse set of challenging reasoning problems. Our method introduces a novel reinforcement learning approach leveraging deep neural networks to guide the selection of inference rules within a resolution-based theorem prover. The agent is trained using policy gradients, where the reward signal is derived from the success or failure of proving theorems within a fixed time budget. Crucially, we incorporate a curriculum learning strategy that progressively increases the difficulty of training problems, enabling the agent to learn more robust and generalizable strategies. Empirical evaluations on a subset of the TPTP library demonstrate that our learned inference rule selection policy significantly outperforms hand-crafted heuristics and baseline reinforcement learning approaches. The system achieves a demonstrable increase in the number of theorems proven within a given time limit and also exhibits a reduction in the average length of proofs, indicating improved reasoning efficiency. These results highlight the potential of reinforcement learning for automating and optimizing complex logical reasoning tasks.",ai
"Generating adversarial examples (AEs) can be formulated as an optimization problem. Among various optimization-based attacks, the gradient-based PGD and the momentum-based MI-FGSM have garnered considerable interest. However, all these attacks use the sign function to scale their perturbations, which raises several theoretical concerns from the point of view of optimization. In this paper, we first reveal that PGD is actually a specific reformulation of the projected gradient method using only the current gradient to determine its step-size. Further, we show that when we utilize a conventional adaptive matrix with the accumulated gradients to scale the perturbation, PGD becomes AdaGrad. Motivated by this analysis, we present a novel momentum-based attack AdaMI, in which the perturbation is optimized with an interesting momentum-based adaptive matrix. AdaMI is proved to attain optimal convergence for convex problems, indicating that it addresses the non-convergence issue of MI-FGSM, thereby ensuring stability of the optimization process. The experiments demonstrate that the proposed momentum-based adaptive matrix can serve as a general and effective technique to boost adversarial transferability over the state-of-the-art methods across different networks while maintaining better stability and imperceptibility.",human
"Bone Age Assessment (BAA) is a widely used clinical technique that can accurately reflect an individual's growth and development level, as well as maturity. In recent years, although deep learning has advanced the field of bone age assessment, existing methods face challenges in efficiently balancing global features and local skeletal details. This study aims to develop an automated bone age assessment system based on a two-stream deep learning architecture to achieve higher accuracy in bone age assessment. We propose the BoNet+ model incorporating global and local feature extraction channels. A Transformer module is introduced into the global feature extraction channel to enhance the ability in extracting global features through multi-head self-attention mechanism. A RFAConv module is incorporated into the local feature extraction channel to generate adaptive attention maps within multiscale receptive fields, enhancing local feature extraction capabilities. Global and local features are concatenated along the channel dimension and optimized by an Inception-V3 network. The proposed method has been validated on the Radiological Society of North America (RSNA) and Radiological Hand Pose Estimation (RHPE) test datasets, achieving mean absolute errors (MAEs) of 3.81 and 5.65 months, respectively. These results are comparable to the state-of-the-art. The BoNet+ model reduces the clinical workload and achieves automatic, high-precision, and more objective bone age assessment.",human
"Individual fairness, which requires that similar individuals should be treated similarly by algorithmic systems, has become a central principle in fair machine learning. Individual fairness has garnered traction in graph representation learning due to its practical importance in high-stakes Web areas such as user modeling, recommender systems, and search. However, existing methods assume the existence of predefined similarity information over all node pairs, an often unrealistic requirement that prevents their operationalization in practice. In this paper, we assume the similarity information is only available for a limited subset of node pairs and introduce FairExpand, a flexible framework that promotes individual fairness in this more realistic partial information scenario. FairExpand follows a two-step pipeline that alternates between refining node representations using a backbone model (e.g., a graph neural network) and gradually propagating similarity information, which allows fairness enforcement to effectively expand to the entire graph. Extensive experiments show that FairExpand consistently enhances individual fairness while preserving performance, making it a practical solution for enabling graph-based individual fairness in real-world applications with partial similarity information.",human
"Mathematical Word Problems (MWPs) are among the most challenging tasks in natural language processing because they require both linguistic understanding and multi-step numerical reasoning. While Chain-of-Thought (CoT) prompting has shown promise, its linear structure often propagates errors, limiting overall effectiveness. To address this limitation, we present the a systematic study of Tree-of-Thought (ToT) reasoning for Bengali MWPs using the SOMADHAN dataset. Owing to computational and token-cost constraints, we evaluate a curated set of 100 representative problems across multiple large language models (LLMs), including GPT-OSS and LLaMA variants, under standard prompting, CoT, and ToT strategies. Our results show that CoT improves baseline accuracy from 78% (standard prompting) to 83% on average, while ToT further increases performance by up to 5 percentage points, achieving 88% accuracy with GPT-OSS-120B. These improvements highlight that ToT is particularly effective in medium-to-large-scale models but may offer less advantage for smaller ones. Overall, our findings establish ToT as a robust framework for solving mathematical problems in low-resource languages such as Bengali. More broadly, this study shows that structured reasoning methods like ToT can provide more reliable and globally consistent outcomes than CoT, paving the way for better reasoning strategies in multilingual NLP.",human
"## Automated Detection of Pulmonary Nodules in CT Scans via Deep Learning with Multi-Scale Feature Fusion Accurate and timely detection of pulmonary nodules from computed tomography (CT) scans is crucial for early diagnosis and improved patient outcomes in lung cancer. However, the subtle nature of these nodules and variations in size, shape, and density pose significant challenges for radiologists. This paper presents a novel deep learning framework for automated pulmonary nodule detection employing a multi-scale feature fusion strategy. The proposed method leverages a 3D convolutional neural network (CNN) backbone, specifically designed to capture contextual information from volumetric CT data. To address the variability in nodule sizes, we incorporate a feature pyramid network (FPN) to extract features at multiple scales. These multi-scale features are then fused through an attention mechanism, allowing the model to focus on the most relevant features for nodule identification. The model was trained and evaluated on the publicly available LUNA16 dataset. Experimental results demonstrate that the proposed approach achieves a Competitive Performance Metric (CPM) score of 0.882, surpassing existing state-of-the-art methods while also demonstrating improved sensitivity in detecting small nodules. This indicates a significant advancement in automated pulmonary nodule detection, potentially aiding radiologists in improving diagnostic accuracy and efficiency.",ai
"Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.",human
"We address the challenge of efficiently representing and reasoning with complex relational knowledge in large-scale knowledge graphs. Existing methods often struggle with scalability and computational complexity when dealing with graphs containing millions of entities and billions of relations. We propose a novel approach leveraging Hierarchical Tensor Decomposition (HTD) coupled with a learned embedding space. HTD enables the compact representation of high-dimensional relational data by decomposing the knowledge graph tensor into a hierarchical tree structure, capturing latent semantic relationships at different levels of granularity. The learned embedding space, trained using a contrastive loss function, allows for efficient similarity-based reasoning and link prediction. We evaluate our approach on several benchmark datasets, including WordNet and Freebase. Experimental results demonstrate that HTD significantly reduces the memory footprint and computational cost compared to traditional tensor decomposition methods, while maintaining or improving link prediction accuracy. Specifically, we observe a 30% reduction in memory usage and a 20% speedup in query processing, while achieving state-of-the-art results on several link prediction tasks. This approach offers a promising direction for representing and reasoning with knowledge in resource-constrained environments and enabling more efficient knowledge graph applications.",ai
"Diffusion-based generative models demonstrate state-of-the-art performance across various image synthesis tasks, yet their tendency to replicate and amplify dataset biases remains poorly understood. Although previous research has viewed bias amplification as an inherent characteristic of diffusion models, this work provides the first analysis of how sampling algorithms and their hyperparameters influence bias amplification. We empirically demonstrate that samplers for diffusion models -- commonly optimized for sample quality and speed -- have a significant and measurable effect on bias amplification. Through controlled studies with models trained on Biased MNIST, Multi-Color MNIST and BFFHQ, and with Stable Diffusion, we show that sampling hyperparameters can induce both bias reduction and amplification, even when the trained model is fixed. Source code is available at https://github.com/How-I-met-your-bias/how_i_met_your_bias.",human
"Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/",human
"We address the problem of optimizing long-horizon, sparse-reward language generation tasks, specifically focusing on the generation of coherent argumentative essays. Traditional sequence-to-sequence models often struggle to produce high-quality essays due to the lack of explicit reward signals for crucial aspects like argumentation structure and logical coherence. To overcome this limitation, we propose a novel reinforcement learning (RL) framework incorporating a hierarchical reward function that decomposes the overall essay quality into sub-goals, such as topic relevance, stance consistency, and argumentative strength. Our method, which we term Hierarchical Argumentation Reinforcement Learning (HARL), leverages a transformer-based language model trained initially on a large corpus of argumentative texts. The model is then fine-tuned using a proximal policy optimization (PPO) algorithm guided by the hierarchical reward signal. Empirical evaluations demonstrate that HARL significantly outperforms baseline models trained with standard sequence-to-sequence learning and reward shaping based on single-metric evaluations (e.g., BLEU score). Specifically, human evaluations reveal a substantial improvement in the overall coherence, argumentative strength, and persuasiveness of essays generated by HARL, indicating its effectiveness in addressing the challenges of long-horizon, sparse-reward language generation. We further analyze the learned policy and demonstrate that HARL encourages the generation of essays with well-defined argumentative structures and logically consistent reasoning.",ai
"We address the problem of designing robust policies for sequential decision-making under uncertainty, specifically focusing on cases where the distributional information about the uncertain parameters is limited. Traditional robust optimization often relies on defining fixed uncertainty sets, which can lead to overly conservative solutions or be difficult to construct accurately. We propose a novel approach that adaptively refines the uncertainty set during the policy optimization process. Our method leverages a sample-based approximation of the worst-case expected cost, iteratively constructing confidence regions for the unknown distribution based on observed data. These confidence regions are then used to tighten the uncertainty set, progressively reducing the conservatism of the resulting policy. We formulate the problem as a saddle-point problem and develop a computationally efficient algorithm based on stochastic gradient descent-ascent to find the robust policy and the corresponding worst-case distribution within the learned uncertainty set. We demonstrate the efficacy of our approach on benchmark inventory management and portfolio optimization problems. Empirical results show that our method achieves significantly improved out-of-sample performance compared to classical robust optimization techniques with static uncertainty sets, while maintaining robustness against distributional shifts. The adaptive uncertainty set refinement allows for a better trade-off between optimality and robustness, leading to more practical and effective decision-making.",ai
"Transformer models have achieved state-of-the-art performance across a wide range of natural language processing (NLP) tasks. However, their computational complexity and large parameter size present challenges for deployment in resource-constrained environments and hinder efficient training and inference. This paper investigates novel techniques for improving the efficiency of transformer models while maintaining performance on downstream tasks. We explore a combination of knowledge distillation and parameter quantization to reduce model size and computational cost. Specifically, we distill knowledge from a large pre-trained transformer model into a smaller student model using a multi-objective loss function that considers both task-specific performance and intermediate layer representations. Subsequently, we apply post-training quantization to the student model, converting model weights and activations to lower precision formats. Experimental results on benchmark NLP datasets, including sentiment analysis and text classification, demonstrate that our proposed approach achieves significant reductions in model size (up to 4x) and inference latency (up to 2x) with minimal degradation in accuracy compared to the original transformer model. These findings suggest that knowledge distillation and quantization offer a promising pathway towards deploying efficient transformer models in practical applications.",ai
"We address the problem of designing robust decision policies for sequential decision-making under uncertainty, specifically focusing on settings where the uncertainty arises from adversarial perturbations to the system dynamics. Traditional robust optimization approaches often suffer from computational intractability when applied to complex, high-dimensional systems, particularly when considering dynamic uncertainty sets that evolve over time. We propose a novel algorithmic framework leveraging a data-driven approach to approximate the robust value function, thereby mitigating the computational burden. This framework employs a combination of reinforcement learning techniques and scenario-based robust optimization. Specifically, we train a policy using a set of simulated adversarial scenarios, and then refine the policy using a distributionally robust optimization objective that accounts for uncertainty in the scenario distribution. Our method employs a kernel density estimation approach to estimate the Wasserstein ambiguity set around the empirical scenario distribution, allowing for a tractable approximation of the robust optimization problem. We demonstrate the efficacy of our approach on a simulated robotic manipulation task with adversarial disturbances to the robot's joint torques. Results show that our data-driven robust policy significantly outperforms policies obtained through standard reinforcement learning and baseline robust optimization methods, achieving superior performance and robustness against a variety of adversarial perturbations while maintaining computational tractability.",ai
"This paper addresses the challenge of distributionally robust optimization (DRO) when faced with limited and potentially biased data. Traditional DRO formulations often rely on strong assumptions about the ambiguity set, leading to overly conservative solutions or computational intractability, especially in high-dimensional settings. We propose a novel data-driven DRO approach grounded in the concept of Wasserstein barycenters. Our method constructs an ambiguity set centered around a regularized empirical distribution, with the radius determined by the Wasserstein distance to a set of plausible candidate distributions. These candidate distributions are derived from bootstrapping techniques, thereby incorporating information about the data's uncertainty and potential biases. This barycentric construction allows us to adaptively adjust the size and shape of the ambiguity set based on the observed data characteristics. We further develop a computationally efficient solution algorithm based on Lagrangian duality and cutting-plane methods. This approach enables us to solve the resulting DRO problem for a variety of loss functions and constraint structures. We demonstrate the efficacy of our method on several benchmark problems, including portfolio optimization and regression, showing that it achieves superior out-of-sample performance compared to existing DRO and robust optimization techniques, particularly when the training data is scarce or non-representative of the true underlying distribution. The results highlight the method's ability to balance robustness against distributional uncertainty with sensitivity to the available data.",ai
"We address the challenge of automated tumor segmentation in magnetic resonance imaging (MRI) of the brain, a critical but time-consuming task in clinical oncology. Manual segmentation is prone to inter-observer variability and significantly burdens radiologists. We propose a novel deep learning architecture, termed Multi-Scale Feature Fusion Network (MSFFN), designed to effectively leverage both local and global contextual information for improved segmentation accuracy. MSFFN incorporates a 3D U-Net backbone augmented with residual multi-kernel convolutions within each encoder block to capture features at varying scales. Additionally, we introduce a spatial attention mechanism at the decoder stages to selectively emphasize salient regions of interest and suppress irrelevant background noise. The proposed MSFFN was evaluated on a publicly available brain tumor segmentation dataset (BraTS 2020). Quantitative analysis demonstrates that MSFFN achieves state-of-the-art performance, outperforming existing methods in terms of Dice score and Hausdorff distance. Specifically, MSFFN achieved a mean Dice score of 0.89 for whole tumor, 0.82 for tumor core, and 0.77 for enhancing tumor, representing a statistically significant improvement (p < 0.01) compared to a standard 3D U-Net baseline. These results indicate the potential of MSFFN to enhance the efficiency and accuracy of tumor segmentation in clinical practice, facilitating improved diagnosis and treatment planning.",ai
"With the growing popularity of electric vehicles as a means of addressing climate change, concerns have emerged regarding their impact on electric grid management. As a result, predicting EV charging demand has become a timely and important research problem. While substantial research has addressed energy load forecasting in transportation, relatively few studies systematically compare multiple forecasting methods across different temporal horizons and spatial aggregation levels in diverse urban settings. This work investigates the effectiveness of five time series forecasting models, ranging from traditional statistical approaches to machine learning and deep learning methods. Forecasting performance is evaluated for short-, mid-, and long-term horizons (on the order of minutes, hours, and days, respectively), and across spatial scales ranging from individual charging stations to regional and city-level aggregations. The analysis is conducted on four publicly available real-world datasets, with results reported independently for each dataset. To the best of our knowledge, this is the first work to systematically evaluate EV charging demand forecasting across such a wide range of temporal horizons and spatial aggregation levels using multiple real-world datasets.",human
"Graph Neural Networks (GNNs) have emerged as a powerful paradigm for learning representations of graph-structured data, yet their performance often degrades when dealing with graphs exhibiting significant homophily, where connected nodes tend to have similar labels. This presents a critical challenge in various applications, including social network analysis and bioinformatics, where heterogeneous relationships are prevalent. We address this problem by introducing a novel adaptive propagation mechanism that dynamically adjusts the receptive field of each node based on local graph structure and feature similarity. This approach, termed Adaptive Neighborhood Aggregation Network (ANAN), leverages attention mechanisms to selectively aggregate information from neighboring nodes, prioritizing those that contribute most effectively to the node's representation. Specifically, ANAN employs a learnable metric to quantify node similarity, allowing for the attenuation of signals from dissimilar neighbors and the amplification of signals from relevant ones. We evaluate ANAN on a suite of benchmark datasets characterized by varying levels of homophily. Our results demonstrate that ANAN consistently outperforms state-of-the-art GNN models, achieving significant improvements in node classification accuracy, particularly in low-homophily settings. Furthermore, ablation studies validate the effectiveness of the adaptive propagation mechanism and highlight its crucial role in mitigating the detrimental effects of noisy or irrelevant neighbor information.",ai
"We address the challenge of learning robust machine learning models in the presence of adversarial input perturbations. Many existing robust optimization techniques rely on solving computationally expensive inner maximization problems to identify the worst-case perturbation. We propose a novel approach, termed Projected Gradient Descent with Momentum Regularization (PGD-MR), which incorporates a momentum-based regularization term into the projected gradient descent (PGD) algorithm used for adversarial attack generation. This regularization encourages the attacker to explore a broader region of the input space, leading to more effective adversarial training. Specifically, the momentum term is designed to prevent the attacker from becoming trapped in local optima and facilitates the discovery of stronger, more diverse adversarial examples. Empirical evaluations on standard image classification benchmarks, including CIFAR-10 and CIFAR-100, demonstrate that PGD-MR significantly improves the robustness of adversarially trained models against various white-box and black-box attacks. Compared to traditional PGD-based adversarial training and other state-of-the-art robust optimization methods, our approach achieves superior performance, measured by both clean accuracy and robust accuracy under a range of perturbation budgets. Furthermore, we provide theoretical analysis to support the effectiveness of momentum regularization in escaping local optima within the adversarial attack landscape.",ai
"This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: , , and the critical need to synthesize data from , such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills. Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a , where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.",human
"Objective: To evaluate large language models (LLMs) against supervised baselines for fine-grained, lesion-level detection of incidentalomas requiring follow-up, addressing the limitations of current document-level classification systems. Methods: We utilized a dataset of 400 annotated radiology reports containing 1,623 verified lesion findings. We compared three supervised transformer-based encoders (BioClinicalModernBERT, ModernBERT, Clinical Longformer) against four generative LLM configurations (Llama 3.1-8B, GPT-4o, GPT-OSS-20b). We introduced a novel inference strategy using lesion-tagged inputs and anatomy-aware prompting to ground model reasoning. Performance was evaluated using class-specific F1-scores. Results: The anatomy-informed GPT-OSS-20b model achieved the highest performance, yielding an incidentaloma-positive macro-F1 of 0.79. This surpassed all supervised baselines (maximum macro-F1: 0.70) and closely matched the inter-annotator agreement of 0.76. Explicit anatomical grounding yielded statistically significant performance gains across GPT-based models (p < 0.05), while a majority-vote ensemble of the top systems further improved the macro-F1 to 0.90. Error analysis revealed that anatomy-aware LLMs demonstrated superior contextual reasoning in distinguishing actionable findings from benign lesions. Conclusion: Generative LLMs, when enhanced with structured lesion tagging and anatomical context, significantly outperform traditional supervised encoders and achieve performance comparable to human experts. This approach offers a reliable, interpretable pathway for automated incidental finding surveillance in radiology workflows.",human
"We present a novel approach to enhancing automated reasoning capabilities in complex knowledge graphs through the integration of path-based reasoning with subgraph isomorphism techniques. The inherent challenge lies in efficiently identifying valid inference paths and confirming their consistency within the context of the broader knowledge graph structure. Our method, termed ""IsoPath,"" addresses this by first employing a learned path ranking model to prioritize semantically relevant inference paths originating from the query entities. Subsequently, these paths are subjected to rigorous subgraph isomorphism verification, ensuring that the relationships implied by the paths are supported by the existing knowledge graph topology, mitigating the risk of spurious inferences. We evaluate IsoPath on a benchmark knowledge graph completion task, specifically FB15k-237, demonstrating a significant improvement in link prediction accuracy compared to several state-of-the-art path-based reasoning systems. Specifically, IsoPath achieves a 12% increase in Hits@10 and a 9% increase in Mean Reciprocal Rank (MRR) over the strongest baseline, indicating a substantial advancement in the ability to accurately infer missing relationships. These results suggest that IsoPath provides a robust and scalable framework for automated reasoning in large-scale knowledge graphs.",ai
"High-performance Radar-Camera 3D object detection can be achieved by leveraging knowledge distillation without using LiDAR at inference time. However, existing distillation methods typically transfer modality-specific features directly to each sensor, which can distort their unique characteristics and degrade their individual strengths. To address this, we introduce IMKD, a radar-camera fusion framework based on multi-level knowledge distillation that preserves each sensor's intrinsic characteristics while amplifying their complementary strengths. IMKD applies a three-stage, intensity-aware distillation strategy to enrich the fused representation across the architecture: (1) LiDAR-to-Radar intensity-aware feature distillation to enhance radar representations with fine-grained structural cues, (2) LiDAR-to-Fused feature intensity-guided distillation to selectively highlight useful geometry and depth information at the fusion level, fostering complementarity between the modalities rather than forcing them to align, and (3) Camera-Radar intensity-guided fusion mechanism that facilitates effective feature alignment and calibration. Extensive experiments on the nuScenes benchmark show that IMKD reaches 67.0% NDS and 61.0% mAP, outperforming all prior distillation-based radar-camera fusion methods. Our code and models are available at https://github.com/dfki-av/IMKD/.",human
"We introduce Eff-GRot, an approach for efficient and generalizable rotation estimation from RGB images. Given a query image and a set of reference images with known orientations, our method directly predicts the object's rotation in a single forward pass, without requiring object- or category-specific training. At the core of our framework is a transformer that performs a comparison in the latent space, jointly processing rotation-aware representations from multiple references alongside a query. This design enables a favorable balance between accuracy and computational efficiency while remaining simple, scalable, and fully end-to-end. Experimental results show that Eff-GRot offers a promising direction toward more efficient rotation estimation, particularly in latency-sensitive applications.",human
"We address the challenge of performing machine learning tasks on sensitive data while maintaining rigorous privacy guarantees. Specifically, we focus on distributed learning scenarios where data is partitioned across multiple agents who are unwilling to share their raw data due to privacy concerns. We propose a novel differentially private federated learning (DP-FL) algorithm that leverages a combination of local stochastic gradient descent, secure aggregation, and calibrated noise injection. Our method introduces a dynamic, data-dependent noise scaling mechanism that minimizes the impact of noise on model accuracy while adhering to a pre-defined privacy budget. This scaling adapts to the inherent variability in data across different agents and training epochs. We formally prove the (ε, δ)-differential privacy guarantees of our algorithm using a rigorous composition theorem. Empirical evaluations on benchmark datasets, including MNIST and CIFAR-10, demonstrate that our adaptive DP-FL algorithm significantly outperforms existing DP-FL approaches in terms of model accuracy, particularly in heterogeneous data settings. Our results show a substantial improvement in utility while preserving strong privacy guarantees, thereby enabling more practical deployments of privacy-preserving machine learning in real-world applications.",ai
"An adaptive sampling approach for efficient detection of bifurcation boundaries in parametrized fluid flow problems is presented herein. The study extends the machine-learning approach of Silvester (Machine Learning for Hydrodynamic Stability, arXiv:2407.09572), where a classifier network was trained on preselected simulation data to identify bifurcated and nonbifurcated flow regimes. In contrast, the proposed methodology introduces adaptivity through a flow-based deep generative model that automatically refines the sampling of the parameter space. The strategy has two components: a classifier network maps the flow parameters to a bifurcation probability, and a probability density estimation technique (KRnet) for the generation of new samples at each adaptive step. The classifier output provides a probabilistic measure of flow stability, and the Shannon entropy of these predictions is employed as an uncertainty indicator. KRnet is trained to approximate a probability density function that concentrates sampling in regions of high entropy, thereby directing computational effort towards the evolving bifurcation boundary. This coupling between classification and generative modeling establishes a feedback-driven adaptive learning process analogous to error-indicator based refinement in contemporary partial differential equation solution strategies. Starting from a uniform parameter distribution, the new approach achieves accurate bifurcation boundary identification with significantly fewer Navier--Stokes simulations, providing a scalable foundation for high-dimensional stability analysis.",human
"### Abstract We address the challenge of representing and reasoning with complex, structured knowledge in dynamic environments. Traditional knowledge representation formalisms often struggle to efficiently capture intricate relationships, handle uncertainty, and adapt to evolving information landscapes. This paper introduces a novel knowledge representation framework, the Attributed Relational Concept Network (ARCN), which integrates concepts, relations, and attributes within a unified graph-based structure. Concepts represent entities, relations define their connections, and attributes describe their properties, each equipped with associated uncertainty measures derived from observed data. ARCN employs a Bayesian inference mechanism for reasoning, allowing for probabilistic predictions and belief updates in response to new evidence. We demonstrate the effectiveness of ARCN through experiments in a simulated supply chain management scenario, where the system must track product provenance, predict potential disruptions, and recommend mitigation strategies. Results show that ARCN outperforms existing methods, including standard knowledge graphs and probabilistic logic programming, in terms of accuracy, robustness to noisy data, and adaptability to changing operational conditions. Specifically, ARCN achieves a 15% improvement in prediction accuracy of supply chain disruptions and a 20% reduction in response time for recommending mitigation strategies compared to benchmark methods. The ARCN framework provides a promising approach for building intelligent systems capable of reasoning effectively with complex, evolving knowledge.",ai
"Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.",human
"Graph Neural Networks (GNNs) have demonstrated remarkable success in various node-level and graph-level prediction tasks. However, the performance of GNNs often degrades when applied to large, complex graphs, particularly those exhibiting heterophily, where connected nodes possess dissimilar labels. This degradation stems from the prevalent assumption of homophily in many GNN architectures, leading to ineffective message aggregation across dissimilar neighborhoods. We introduce a novel adaptive filtering mechanism designed to mitigate the adverse effects of heterophily in GNNs. This mechanism leverages node feature similarity to dynamically adjust the receptive field of each node, effectively prioritizing information from more similar neighbors while suppressing the influence of dissimilar ones during message passing. This adaptive filtering is integrated into a learnable aggregation function, enabling the GNN to selectively incorporate information based on the local graph structure and node feature profiles. We evaluate our approach on several benchmark datasets exhibiting varying degrees of heterophily, demonstrating significant improvements in classification accuracy compared to state-of-the-art GNN models, including those specifically designed for heterophilic graphs. Our results indicate that adaptive filtering provides a robust and effective strategy for enhancing the performance of GNNs in challenging graph environments.",ai
"VR in the treatment of clinical concerns such as generalized anxiety disorder or social anxiety. VR has created additional pathways to support patient well-being and care. Understanding online discussion of what users think about this technology may further support its efficacy. The purpose of this study was to employ a corpus linguistic methodology to identify the words and word networks that shed light on the online discussion of virtual reality and anxiety. Using corpus linguistics, frequently used words in discussion along with collocation were identified by utilizing Sketch Engine software. The results of the study, based upon the English Trends corpus, identified VR, Oculus, and headset as the most frequently discussed within the VR and anxiety subcorpus. These results point to the development of the virtual system, along with the physical apparatus that makes viewing and engaging with the virtual environment possible. Additional results point to collocation of prepositional phrases such as of virtual reality, in virtual reality, and for virtual reality relating to the design, experience, and development, respectively. These findings offer new perspective on how VR and anxiety together are discussed in general discourse and offer pathways for future opportunities to support counseling needs through development and accessibility. Keywords: anxiety disorders, corpus linguistics, Sketch Engine, and virtual reality VR",human
"Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.",human
"Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.",human
"Graph Neural Networks (GNNs) have demonstrated remarkable efficacy in learning representations for graph-structured data. However, existing GNN architectures often struggle with long-range dependencies, leading to suboptimal performance on tasks requiring global graph understanding. This limitation stems from the inherent message-passing mechanism, which exponentially increases computational complexity as the number of hops grows. To address this challenge, we propose a novel GNN framework, Hierarchical Global Attention Network (HGAN), which leverages hierarchical graph partitioning and global attention mechanisms to efficiently capture long-range relationships. HGAN first decomposes the graph into a hierarchy of increasingly coarse-grained representations using a community detection algorithm. Subsequently, a global attention module is applied at each level of the hierarchy to selectively aggregate information from all nodes within that level. This hierarchical approach allows for efficient computation and facilitates the propagation of information across distant nodes. We evaluate HGAN on a suite of graph classification and node classification benchmarks, including the challenging long-range graph benchmark, Long Range Graph Benchmark (LRGB). Experimental results demonstrate that HGAN significantly outperforms existing GNN architectures, achieving state-of-the-art performance on several datasets and exhibiting superior scalability to large graphs. The improvements underscore the importance of hierarchical information aggregation and global attention for capturing long-range dependencies in graph-structured data.",ai
"This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.",human
"We address the challenge of coordinating language-guided actions in cooperative multi-agent systems operating in complex, partially observable environments. Specifically, we investigate the problem of achieving efficient and robust team performance when agents must interpret and execute natural language instructions from a central instructor while maintaining decentralized decision-making. Our method introduces a novel hierarchical reinforcement learning framework incorporating a shared intention network and individual policy adaptation modules. The shared intention network encodes the global language instruction and infers a team-level goal, which is then broadcast to individual agents. Each agent utilizes its policy adaptation module to tailor its local policy based on the received team goal, its local observations, and a learned understanding of the other agents' roles and capabilities. This allows for flexible and adaptive role specialization without explicit role assignment. We evaluate our approach on a challenging simulated environment requiring coordinated navigation, object manipulation, and communication. Experimental results demonstrate that our method significantly outperforms baseline approaches that rely on independent learning or centralized control, achieving higher success rates and faster task completion times. Furthermore, our analysis reveals that the learned intention representations are highly correlated with the underlying task structure, suggesting that the agents are effectively learning to reason about the global objective and their individual contributions to it.",ai
"Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.",human
"We investigate the problem of learning decentralized policies for cooperative multi-agent systems in environments with sparse and delayed rewards. Traditional reinforcement learning approaches in such settings often suffer from high variance and the difficulty of credit assignment, hindering effective coordination. We introduce a novel algorithm, Counterfactual Advantage Decomposition with Learned Communication (CADLC), which addresses these challenges by explicitly modeling counterfactual rewards and employing a learned communication channel. CADLC decomposes the advantage function into individual agent contributions, conditioned on both observed actions and hypothetical counterfactual actions of other agents. Furthermore, agents learn to communicate relevant information through a differentiable communication protocol, allowing for adaptive coordination strategies. We evaluate CADLC on a suite of challenging multi-agent benchmark environments, including StarCraft II micromanagement and cooperative navigation tasks. Empirical results demonstrate that CADLC significantly outperforms existing state-of-the-art methods, achieving superior performance in terms of both reward and sample efficiency. Ablation studies validate the effectiveness of the counterfactual advantage decomposition and the learned communication mechanism in facilitating effective multi-agent coordination under sparse reward settings. The proposed framework offers a promising approach for scaling reinforcement learning to complex, decentralized multi-agent systems.",ai
"We address the challenge of coordinating decentralized agents with heterogeneous objectives in complex, partially observable environments. Traditional multi-agent reinforcement learning (MARL) algorithms often struggle with scalability and convergence due to non-stationarity arising from the concurrent learning of multiple agents. We propose a novel distributed policy optimization framework that incorporates a learned inter-agent communication channel and a centralized critic with decentralized execution. This framework, termed CommNet-Guided Decentralized Policy Optimization (CDPO), leverages graph neural networks to model agent relationships and facilitate efficient information aggregation for improved decision-making. Specifically, CommNets are used to infer communication policies that dynamically adapt to the evolving environment and agent strategies. The centralized critic utilizes this aggregated information to provide global state-action value estimates, which are then used to guide the decentralized policy updates for each agent. We evaluate CDPO on a suite of benchmark cooperative and competitive MARL tasks, including the StarCraft Multi-Agent Challenge (SMAC). Empirical results demonstrate that CDPO significantly outperforms existing state-of-the-art MARL algorithms in terms of both sample efficiency and asymptotic performance. Furthermore, ablation studies validate the importance of the learned communication channel and the centralized critic in achieving robust and coordinated behavior.",ai
"Finetuning pre-trained language models with small amounts of data is a commonly-used method to create translators for ultra-low resource languages such as endangered Indigenous languages. However, previous works have reported substantially different performances with translators created using similar methodology and data. In this work we systematically explored possible causes of the performance difference, aiming to determine whether it was a product of different cleaning procedures, limitations of the pre-trained models, the size of the base model, or the size of the training dataset, studying both directions of translation. Our studies, using two Brazilian Indigenous languages, related but with significant structural linguistic characteristics, indicated none or very limited influence from those training factors, suggesting differences between languages may play a significant role in the ability to produce translators by fine-tuning pre-trained models.",human
"We investigate the challenge of sample-efficient exploration in sparse-reward reinforcement learning environments. Specifically, we address the problem of an agent becoming trapped in suboptimal regions of the state space due to a lack of initial positive reinforcement, hindering the discovery of more rewarding behaviors. To mitigate this, we propose a novel intrinsic motivation framework termed ""Curiosity-Driven State-Space Partitioning"" (CDSP). CDSP combines a curiosity-based exploration bonus with a dynamic partitioning of the state space. The curiosity bonus encourages the agent to visit novel states, while the partitioning mechanism divides the state space into regions based on visitation frequency and the magnitude of observed rewards. Regions with low visitation and potentially high reward potential are assigned higher exploration priority. We evaluate CDSP across a suite of challenging benchmark environments with sparse rewards, including navigation tasks with deceptive local optima. Our results demonstrate that CDSP significantly outperforms baseline methods, including standard curiosity-driven exploration and count-based exploration techniques. Specifically, CDSP achieves a 30-50% improvement in asymptotic performance and exhibits faster learning rates, indicating enhanced sample efficiency and improved robustness to suboptimal exploration trajectories. These findings suggest that strategic state-space partitioning, guided by curiosity, is a promising approach for addressing the exploration problem in sparse-reward reinforcement learning.",ai
"## Emergent Coordination in Decentralized Multi-Agent Navigation through Differentiable Communication Cooperative navigation in multi-agent systems presents a significant challenge, particularly in decentralized settings where agents possess limited communication capabilities and partial observability. This work addresses the problem of achieving efficient and collision-free navigation for a team of agents in complex, dynamic environments without explicit coordination mechanisms. We propose a novel differentiable communication framework leveraging Graph Neural Networks (GNNs) to enable agents to learn implicit coordination strategies. Each agent constructs a local observation graph encompassing nearby agents and environmental features, and then iteratively refines its action plan based on message passing within this graph. Crucially, the communication protocol and individual agent policies are learned end-to-end through reinforcement learning, without requiring predefined communication languages or explicit coordination rules. We demonstrate the efficacy of our approach in simulated environments involving varying agent densities and complex obstacle configurations. Empirical results indicate that our method achieves significantly improved navigation success rates and reduced collision rates compared to baseline methods relying on independent learning or fixed communication strategies. Furthermore, analysis of the learned communication patterns reveals the emergence of specialized roles and efficient information propagation within the agent team, contributing to a robust and scalable decentralized navigation system.",ai
"We address the challenge of coordinating decentralized agents in complex, partially observable environments where explicit communication is costly or infeasible. Existing multi-agent reinforcement learning (MARL) algorithms often struggle with scalability and exploration in such settings. To overcome these limitations, we propose a novel method, Variational Information Bottleneck for Decentralized Policy Learning (VIB-DPL), which leverages the information bottleneck principle to encourage agents to learn compressed and informative representations of their local observations. Specifically, each agent learns a variational approximation of the posterior distribution over relevant latent states conditioned on its observation history. This compressed representation is then used to inform its policy, reducing reliance on irrelevant information and promoting more robust coordination. We empirically evaluate VIB-DPL on a set of challenging cooperative navigation and resource allocation tasks. Results demonstrate that VIB-DPL significantly outperforms state-of-the-art decentralized MARL algorithms, achieving higher cumulative rewards and exhibiting improved generalization capabilities, particularly in environments with noisy observations or varying agent populations. Furthermore, we provide an analysis of the learned latent representations, demonstrating that VIB-DPL successfully identifies and extracts the most crucial information for effective decentralized decision-making.",ai
"This paper addresses the problem of optimizing machine learning models under distributional uncertainty, specifically focusing on scenarios where the precise data distribution is unknown but known to reside within an ambiguity set. Traditional empirical risk minimization often falters in the presence of unforeseen distributional shifts, leading to degraded performance in real-world deployments. To mitigate this, we propose a novel distributionally robust optimization (DRO) framework leveraging a Wasserstein ambiguity set centered around the empirical distribution. Our method incorporates a scalable algorithm that combines stochastic gradient descent with a novel regularization term derived from the Kantorovich-Rubinstein duality, enabling efficient training even with large-scale datasets and complex model architectures. We demonstrate the efficacy of our approach across several benchmark classification tasks, including image classification and natural language processing, using both synthetic and real-world distribution shifts. Experimental results show that our Wasserstein DRO method significantly outperforms standard empirical risk minimization and other state-of-the-art robust optimization techniques in terms of out-of-distribution generalization performance. Specifically, we observe a consistent improvement in accuracy and F1-score under various adversarial perturbations and domain adaptation scenarios, indicating the enhanced robustness of models trained with our proposed framework. Furthermore, our method exhibits favorable computational scalability compared to existing DRO approaches, making it a practical solution for real-world applications requiring reliable performance under uncertainty.",ai
"Graph Neural Networks (GNNs) have demonstrated significant promise in learning representations for graph-structured data across diverse domains. However, many existing GNN architectures struggle with the over-smoothing problem, where node representations converge to similar values after multiple message-passing iterations, leading to diminished discriminative power, particularly in deeper networks. This work introduces a novel GNN architecture, the Differentiated Edge and Node Aggregation Network (DENAN), designed to mitigate over-smoothing by employing distinct aggregation schemes for node and edge features. DENAN decouples the aggregation processes, allowing for separate control over the information flow related to node and edge attributes. Specifically, we incorporate a learned attention mechanism for edge aggregation, prioritizing salient edges and reducing the influence of less informative neighbors. Simultaneously, node aggregation utilizes a skip-connection based residual learning framework to preserve initial node features and prevent information loss. Empirically, DENAN achieves state-of-the-art results on several benchmark graph classification datasets, including MUTAG, PTC, and PROTEINS. Furthermore, we demonstrate through ablation studies that the differentiated aggregation strategy and the learned edge attention contribute significantly to improved performance and enhanced robustness against over-smoothing, allowing for the construction of deeper and more expressive GNN models. These findings suggest that carefully designed aggregation mechanisms can effectively address the limitations of conventional GNN architectures.",ai
"Addressing selection bias in latent variable causal discovery is important yet underexplored, largely due to a lack of suitable statistical tools: While various tools beyond basic conditional independencies have been developed to handle latent variables, none have been adapted for selection bias. We make an attempt by studying rank constraints, which, as a generalization to conditional independence constraints, exploits the ranks of covariance submatrices in linear Gaussian models. We show that although selection can significantly complicate the joint distribution, interestingly, the ranks in the biased covariance matrices still preserve meaningful information about both causal structures and selection mechanisms. We provide a graph-theoretic characterization of such rank constraints. Using this tool, we demonstrate that the one-factor model, a classical latent variable model, can be identified under selection bias. Simulations and real-world experiments confirm the effectiveness of using our rank constraints.",human
"We address the problem of causal effect identification in observational studies with unobserved confounding, where instrumental variables (IVs) are unavailable and traditional backdoor adjustment methods are insufficient. Specifically, we propose a novel approach leveraging the principles of proxy variable identification. We introduce a two-stage estimation framework, termed Proxy-assisted Causal Effect Recovery (PACER), which utilizes observable proxy variables correlated with the unobserved confounders to achieve consistent estimation of the average treatment effect (ATE). In the first stage, PACER employs a modified regression adjustment technique to estimate the relationship between the treatment, outcome, and the identified proxy variables. This step implicitly accounts for the unobserved confounding by conditioning on the proxy variables. In the second stage, we apply a reweighting scheme based on the estimated propensity scores of the proxy variables to further mitigate residual confounding bias. Through rigorous theoretical analysis, we establish the conditions under which PACER provides consistent ATE estimates. We demonstrate the effectiveness of PACER via extensive simulation studies, illustrating its superior performance compared to existing methods under various confounding scenarios and sample sizes. Furthermore, we apply PACER to a real-world healthcare dataset to estimate the causal effect of a treatment on patient outcomes, providing empirically validated results.",ai
"We address the challenge of coordinating decentralized agents in complex, partially observable environments with non-stationary dynamics. Specifically, we consider the problem of learning robust communication protocols that enable agents to achieve a common goal without relying on a central coordinator or shared knowledge. Existing approaches often struggle with the instability introduced by the changing policies of other agents during training, leading to suboptimal performance or convergence failures. To mitigate this, we propose a novel learning framework that combines recurrent communication with a differentiable inter-agent learning signal based on counterfactual reasoning. Our method allows agents to infer the impact of their individual actions on the overall group performance, even in the presence of noisy or incomplete observations. Furthermore, we introduce a communication regularization term designed to promote the emergence of interpretable and efficient communication strategies. Empirical evaluations on a suite of challenging multi-agent benchmark environments demonstrate that our approach significantly outperforms state-of-the-art methods in terms of both final performance and training stability. We observe that the learned communication protocols exhibit emergent properties such as role specialization and task allocation, leading to improved coordination and robustness to environmental perturbations.",ai
"We address the challenge of efficient exploration in reinforcement learning (RL) for environments with sparse and delayed rewards. Standard RL algorithms often struggle in such scenarios due to the difficulty in discovering rewarding states. This paper introduces a novel exploration strategy, Hierarchical Intrinsic Curiosity Module (HICM), which combines hierarchical reinforcement learning with intrinsic motivation driven by state visitation entropy. HICM decomposes the environment into a hierarchy of subgoals, where higher-level agents learn to set abstract goals for lower-level agents. The intrinsic reward signal is generated based on the novelty of visited states within each subgoal, promoting diverse exploration at multiple levels of abstraction. We evaluate HICM on several challenging benchmark environments with sparse rewards, including the Montezuma's Revenge game and a robotic manipulation task involving long-horizon planning. Empirical results demonstrate that HICM significantly outperforms state-of-the-art exploration methods, achieving a substantial improvement in sample efficiency and ultimately leading to higher cumulative rewards. Furthermore, analysis reveals that HICM's hierarchical structure facilitates the discovery of long-term dependencies and accelerates learning in complex environments.",ai
"We address the problem of effectively capturing long-range dependencies in natural language understanding tasks using graph neural networks (GNNs). Existing GNN approaches often struggle with computational complexity when applied to large text corpora, limiting their ability to model relationships between distant words or concepts. We propose a novel Graph Attention Transformer (GATr) architecture that leverages the strengths of both graph attention networks and transformer networks. GATr constructs a dynamic graph based on token embeddings and employs a multi-head attention mechanism to propagate information across graph edges. Furthermore, we integrate the global context awareness of transformers by incorporating positional encodings and self-attention layers. We evaluate our approach on a suite of benchmark datasets for semantic role labeling and text classification. Experimental results demonstrate that GATr achieves significant improvements in F1-score and accuracy compared to several state-of-the-art GNN and transformer models, particularly on tasks requiring the modeling of complex long-range dependencies. The observed performance gains highlight the effectiveness of the proposed hybrid architecture in capturing contextual relationships within textual data.",ai
"We address the challenge of coordinating situated agents in complex, dynamic environments with sparse reward signals. Traditional reinforcement learning (RL) approaches in multi-agent systems often suffer from non-stationarity and credit assignment problems, leading to suboptimal emergent behavior and inefficient exploration. This paper introduces a novel hierarchical reinforcement learning framework, termed Coordinated Option Discovery (COD), which combines option discovery with a decentralized execution policy. COD agents learn a shared set of reusable skills (options) through intrinsic motivation, promoting exploration and facilitating coordinated action. Critically, the learned options are parameterized by a context vector broadcast from a central coordinator, enabling dynamic adjustment of agent behavior based on the global state. We evaluate COD on a suite of challenging multi-agent navigation and resource gathering tasks. Empirical results demonstrate that COD significantly outperforms state-of-the-art multi-agent RL algorithms, achieving higher cumulative rewards and improved coordination efficiency. Ablation studies confirm the importance of both option discovery and the contextual communication mechanism for effective learning. Our findings suggest that hierarchical approaches, coupled with efficient communication strategies, can alleviate the inherent difficulties of multi-agent reinforcement learning in complex settings.",ai
"Control policies in deep reinforcement learning are often implemented with fixed-capacity multilayer perceptrons trained by backpropagation, which lack structural plasticity and depend on global error signals. This paper introduces the Self-Motivated Growing Neural Network (SMGrNN), a controller whose topology evolves online through a local Structural Plasticity Module (SPM). The SPM monitors neuron activations and edge-wise weight update statistics over short temporal windows and uses these signals to trigger neuron insertion and pruning, while synaptic weights are updated by a standard gradient-based optimizer. This allows network capacity to be regulated during learning without manual architectural tuning. SMGrNN is evaluated on control benchmarks via policy distillation. Compared with multilayer perceptron baselines, it achieves similar or higher returns, lower variance, and task-appropriate network sizes. Ablation studies with growth disabled and growth-only variants isolate the role of structural plasticity, showing that adaptive topology improves reward stability. The local and modular design of SPM enables future integration of a Hebbian plasticity module and spike-timing-dependent plasticity, so that SMGrNN can support both artificial and spiking neural implementations driven by local rules.",human
"The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because bold language can stem from either strong empirical results or mere rhetorical style, it is often difficult to distinguish between the two. To disentangle rhetorical style from substantive content, we introduce a counterfactual, LLM-based framework: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. We find that visionary framing significantly predicts downstream attention, including citations and media attention, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, and provide empirical evidence showing that this increase is largely driven by the adoption of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments to measure and improve scientific evaluation.",human
"We present a novel approach to enhancing the robustness and accuracy of automated reasoning systems in complex, multi-hop inference scenarios. The central problem addressed is the propagation of uncertainty and error accumulation that frequently leads to incorrect conclusions when chaining multiple reasoning steps. Our method, termed ""Probabilistic Attentive Inference Network"" (PAIN), integrates probabilistic logic programming with an attention mechanism to selectively focus on the most relevant premises in each inference step, while simultaneously quantifying and propagating uncertainty estimates. Specifically, PAIN represents facts and rules as probability distributions, and utilizes an attention module to weight the contribution of each premise based on its relevance to the query and its associated uncertainty. The propagated uncertainty is then incorporated into subsequent inference steps, mitigating the impact of noisy or unreliable information. Empirical evaluations on benchmark datasets for question answering and knowledge graph reasoning demonstrate that PAIN significantly outperforms existing rule-based and neural reasoning systems, achieving improvements of up to 15% in accuracy and a substantial reduction in the number of incorrect inferences attributed to error propagation. Furthermore, we analyze the learned attention weights to provide insights into the model's reasoning process and identify potentially flawed premises within the knowledge base. This work contributes a principled framework for reasoning under uncertainty, promoting more reliable and interpretable automated reasoning systems.",ai
"Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel ""token rounding"" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top- routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.",human
"We introduce TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation designed to enable fair, reproducible comparison across methods. While current state-of-the-art systems rely on closed data or compute-heavy models, TalkVerse offers 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours. These are curated from over 60k hours of video via a transparent pipeline that includes scene-cut detection, aesthetic assessment, strict audio-visual synchronization checks, and comprehensive annotations including 2D skeletons and structured visual/audio-style captions. Leveraging TalkVerse, we present a reproducible 5B DiT baseline built on Wan2.2-5B. By utilizing a video VAE with a high downsampling ratio and a sliding window mechanism with motion-frame context, our model achieves minute-long generation with low drift. It delivers comparable lip-sync and visual quality to the 14B Wan-S2V model but with 10 lower inference cost. To enhance storytelling in long videos, we integrate an MLLM director to rewrite prompts based on audio and visual cues. Furthermore, our model supports zero-shot video dubbing via controlled latent noise injection. We open-source the dataset, training recipes, and 5B checkpoints to lower barriers for research in audio-driven human video generation. Project Page: https://zhenzhiwang.github.io/talkverse/",human
"This thesis work presents a more efficient and effective approach to training control-related tasks for humanoid robots using Reinforcement Learning (RL). The traditional RL methods are limited in adapting to real-world environments, complexity, and natural motions, but the proposed approach overcomes these limitations by using curriculum training and Adversarial Motion Priors (AMP) technique. The results show that the developed RL policies for kicking, walking, and jumping are more dynamic, and adaptive, and outperformed previous methods. However, the transfer of the learned policy from simulation to the real world was unsuccessful, highlighting the limitations of current RL methods in fully adapting to real-world scenarios.",human
"Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.",human
"Graph Neural Networks (GNNs) have demonstrated proficiency in learning representations of graph-structured data, enabling performance gains in diverse applications. However, effectively handling heterogeneous graphs, characterized by multiple node and edge types with complex relational patterns, remains a significant challenge. Current GNN architectures often struggle to capture the nuances of these heterogeneous relationships, leading to suboptimal performance. This work introduces a novel Heterogeneous Graph Transformer Network (HGTN) that leverages a modified self-attention mechanism to explicitly model type-specific transformations and interactions. HGTN incorporates learnable type embeddings and attention weights, allowing the network to dynamically adjust its focus based on the types of interacting nodes and edges. Furthermore, we employ a hierarchical aggregation scheme to capture both local and global relational patterns. We evaluate HGTN on several benchmark heterogeneous graph datasets across node classification and link prediction tasks. Results demonstrate that HGTN consistently outperforms state-of-the-art heterogeneous GNNs, achieving significant improvements in predictive accuracy. Ablation studies validate the effectiveness of the proposed type-aware attention mechanism and hierarchical aggregation strategy. These findings highlight the potential of HGTN for addressing the challenges of representation learning on heterogeneous graph data.",ai
"Seismology faces fundamental challenges in state forecasting and reconstruction (e.g., earthquake early warning and ground motion prediction) and managing the parametric variability of source locations, mechanisms, and Earth models (e.g., subsurface structure and topography effects). Addressing these with simulations is hindered by their massive scale, both in synthetic data volumes and numerical complexity, while real-data efforts are constrained by models that inadequately reflect the Earth's complexity and by sparse sensor measurements from the field. Recent machine learning (ML) efforts offer promise, but progress is obscured by a lack of proper characterization, fair reporting, and rigorous comparisons. To address this, we introduce a Common Task Framework (CTF) for ML for seismic wavefields, starting with three distinct wavefield datasets. Our CTF features a curated set of datasets at various scales (global, crustal, and local) and task-specific metrics spanning forecasting, reconstruction, and generalization under realistic constraints such as noise and limited data. Inspired by CTFs in fields like natural language processing, this framework provides a structured and rigorous foundation for head-to-head algorithm evaluation. We illustrate the evaluation procedure with scores reported for two of the datasets, showcasing the performance of various methods and foundation models for reconstructing seismic wavefields from both simulated and real-world sensor measurements. The CTF scores reveal the strengths, limitations, and suitability for specific problem classes. Our vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets, raising the bar for rigor and reproducibility in scientific ML.",human
"We address the challenge of accurate and efficient diagnosis in medical imaging, specifically focusing on automated pulmonary nodule detection in computed tomography (CT) scans. Manual review of these scans is labor-intensive and prone to inter-observer variability. We propose a novel multi-scale convolutional neural network (CNN) architecture incorporating a self-attention mechanism to improve nodule detection sensitivity and reduce false positives. The network utilizes a hierarchical feature extraction pathway, capturing both fine-grained and coarse-grained contextual information relevant to nodule characterization. The self-attention module allows the network to adaptively weigh feature maps based on their relevance to the presence of nodules, suppressing irrelevant background noise. We evaluated the proposed method on the LIDC-IDRI dataset, a publicly available database of lung CT scans. Our results demonstrate a significant improvement in nodule detection performance compared to baseline CNN architectures. Specifically, we achieved a competitive sensitivity of 89.5% at a false positive rate of 0.25 per scan, indicating a substantial reduction in the number of scans requiring extensive radiologist review. The findings suggest the potential for this approach to enhance the efficiency and accuracy of lung cancer screening programs.",ai
"We address the problem of training machine learning models on sensitive data while providing rigorous privacy guarantees. Specifically, we focus on the challenge of maintaining utility when applying differential privacy (DP) to gradient-based optimization algorithms commonly used in deep learning. Traditional DP-SGD approaches often require significant noise addition to gradients, leading to substantial performance degradation, particularly for complex models and high privacy budgets. To mitigate this, we propose a novel differentially private training framework incorporating a gradient compression technique based on sparse random projection. Our method, termed Differentially Private Sparse Random Projection Gradient Descent (DP-SRPGD), reduces the dimensionality of gradient updates before noise addition, thereby decreasing the variance introduced by the DP mechanism. We provide a rigorous theoretical analysis demonstrating that DP-SRPGD satisfies ε-differential privacy, with explicit bounds on the privacy loss. Empirical evaluations on image classification tasks using benchmark datasets (e.g., CIFAR-10, MNIST) reveal that DP-SRPGD achieves significantly higher accuracy compared to standard DP-SGD implementations for a given privacy budget (ε), especially at lower privacy levels. Furthermore, our experimental results highlight the adaptability of DP-SRPGD to various model architectures, showcasing its potential for broader application in privacy-preserving machine learning.",ai
"The protection of Intellectual Property (IP) in Large Language Models (LLMs) represents a critical challenge in contemporary AI research. While fingerprinting techniques have emerged as a fundamental mechanism for detecting unauthorized model usage, existing methods -- whether behavior-based or structural -- suffer from vulnerabilities such as false claim attacks or susceptible to weight manipulations. To overcome these limitations, we propose SELF, a novel intrinsic weight-based fingerprinting scheme that eliminates dependency on input and inherently resists false claims. SELF achieves robust IP protection through two key innovations: 1) unique, scalable and transformation-invariant fingerprint extraction via singular value and eigenvalue decomposition of LLM attention weights, and 2) effective neural network-based fingerprint similarity comparison based on few-shot learning and data augmentation. Experimental results demonstrate SELF maintains high IP infringement detection accuracy while showing strong robustness against various downstream modifications, including quantization, pruning, and fine-tuning attacks. Our code is available at https://github.com/HanxiuZhang/SELF_v2.",human
"Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in 4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa",human
"While machine learning-based weather prediction (MLWP) has achieved significant advancements, research on assimilating real observations or ensemble forecasts within MLWP models remains limited. We introduce ClimaX-LETKF, the first purely data-driven ML-based ensemble weather forecasting system. It operates stably over multiple years, independently of numerical weather prediction (NWP) models, by assimilating the NCEP ADP Global Upper Air and Surface Weather Observations. The system demonstrates greater stability and accuracy with relaxation to prior perturbation (RTPP) than with relaxation to prior spread (RTPS), while NWP models tend to be more stable with RTPS. RTPP replaces an analysis perturbation with a weighted blend of analysis and background perturbations, whereas RTPS simply rescales the analysis perturbation. Our experiments reveal that MLWP models are less capable of restoring the atmospheric field to its attractor than NWP models. This work provides valuable insights for enhancing MLWP ensemble forecasting systems and represents a substantial step toward their practical applications.",human
"Humans excel at solving novel reasoning problems from minimal exposure, guided by inductive biases, assumptions about which entities and relationships matter. Yet the computational form of these biases and their neural implementation remain poorly understood. We introduce a framework that combines Graph Theory and Graph Neural Networks (GNNs) to formalize inductive biases as explicit, manipulable priors over structure and abstraction. Using a human behavioral dataset adapted from the Abstraction and Reasoning Corpus (ARC), we show that differences in graph-based priors can explain individual differences in human solutions. Our method includes an optimization pipeline that searches over graph configurations, varying edge connectivity and node abstraction, and a visualization approach that identifies the computational graph, the subset of nodes and edges most critical to a model's prediction. Systematic ablation reveals how generalization depends on specific prior structures and internal processing, exposing why human like errors emerge from incorrect or incomplete priors. This work provides a principled, interpretable framework for modeling the representational assumptions and computational dynamics underlying generalization, offering new insights into human reasoning and a foundation for more human aligned AI systems.",human
"Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to at matched accuracy.",human
"Cardinality estimation (CE), the task of predicting the result size of queries is a critical component of query optimization. Accurate estimates are essential for generating efficient query execution plans. Recently, machine learning techniques have been applied to CE, broadly categorized into query-driven and data-driven approaches. Data-driven methods learn the joint distribution of data, while query-driven methods construct regression models that map query features to cardinalities. Ideally, a CE technique should strike a balance among three key factors: accuracy, efficiency, and memory footprint. However, existing state-of-the-art models often fail to achieve this balance. To address this, we propose CoLSE, a hybrid learned approach for single-table cardinality estimation. CoLSE directly models the joint probability over queried intervals using a novel algorithm based on copula theory and integrates a lightweight neural network to correct residual estimation errors. Experimental results show that CoLSE achieves a favorable trade-off among accuracy, training time, inference latency, and model size, outperforming existing state-of-the-art methods.",human
"In hard-label black-box adversarial attacks, where only the top-1 predicted label is accessible, the prohibitive query complexity poses a major obstacle to practical deployment. In this paper, we focus on optimizing a representative class of attacks that search for the optimal ray direction yielding the minimum -norm perturbation required to move a benign image into the adversarial region. Inspired by Nesterov's Accelerated Gradient (NAG), we propose a momentum-based algorithm, ARS-OPT, which proactively estimates the gradient with respect to a future ray direction inferred from accumulated momentum. We provide a theoretical analysis of its convergence behavior, showing that ARS-OPT enables more accurate directional updates and achieves faster, more stable optimization. To further accelerate convergence, we incorporate surrogate-model priors into ARS-OPT's gradient estimation, resulting in PARS-OPT with enhanced performance. The superiority of our approach is supported by theoretical guarantees under standard assumptions. Extensive experiments on ImageNet and CIFAR-10 demonstrate that our method surpasses 13 state-of-the-art approaches in query efficiency.",human
"We investigate the efficacy of transformer-based architectures in addressing challenges inherent in natural language processing (NLP) tasks. Specifically, we focus on the persistent problem of capturing long-range dependencies and contextual understanding within sequential text data. Traditional recurrent neural networks (RNNs) often struggle with vanishing gradients, limiting their ability to model relationships across extended text passages. Our methodology involves employing various transformer models, including BERT, RoBERTa, and DistilBERT, pre-trained on massive text corpora and fine-tuned on a diverse set of downstream NLP tasks, encompassing sentiment analysis, text classification, and question answering. We rigorously evaluate the performance of these models against established baselines, such as Long Short-Term Memory networks (LSTMs) and Convolutional Neural Networks (CNNs). Experimental results demonstrate a significant improvement in performance metrics across all evaluated tasks. Notably, transformer-based models achieved an average accuracy increase of 8% compared to LSTM-based approaches in sentiment analysis and a 5% improvement in F1-score for question answering. Furthermore, we observe enhanced contextual understanding, evidenced by improved performance in tasks requiring nuanced interpretation of text. These findings underscore the superior capabilities of transformer architectures in effectively modeling complex linguistic patterns and capturing long-range dependencies, thereby advancing the state-of-the-art in various NLP applications.",ai
"We address the problem of enhancing automated reasoning capabilities within complex question answering systems, specifically targeting scenarios requiring multi-hop inference over large-scale knowledge bases. Existing approaches often struggle with efficiently exploring the vast search space and maintaining contextual relevance across multiple reasoning steps, leading to suboptimal performance and scalability limitations. We introduce a novel method, the Context-Aware Graph Exploration and Reasoning (CAGER) framework, which integrates a dynamically updated context vector into the path-finding process. CAGER leverages a combination of graph neural networks (GNNs) to encode entity relationships and a reinforcement learning (RL) agent trained to navigate the knowledge graph while maximizing a reward function that incorporates both answer accuracy and path efficiency. The context vector, updated at each hop, provides the RL agent with a summarized representation of the traversed path, enabling it to prioritize relevant edges and prune unproductive branches. Experimental results on benchmark datasets such as MetaQA and ComplexWebQuestions demonstrate that CAGER significantly outperforms state-of-the-art baselines in terms of both answer accuracy (achieving a relative improvement of up to 15% in F1 score) and average path length. Further analysis reveals that the learned context representations effectively guide the reasoning process towards more accurate and efficient solutions.",ai
"The implementation of the AI Act requires practical mechanisms to verify compliance with legal obligations, yet concrete and operational mappings from high-level requirements to verifiable assessment activities remain limited, contributing to uneven readiness across Member States. This paper presents a structured mapping that translates high-level AI Act requirements into concrete, implementable verification activities applicable across the AI lifecycle. The mapping is derived through a systematic process in which legal requirements are decomposed into operational sub-requirements and grounded in authoritative standards and recognised practices. From this basis, verification activities are identified and characterised along two dimensions: the type of verification performed and the lifecycle target to which it applies. By making explicit the link between regulatory intent and technical and organisational assurance practices, the proposed mapping reduces interpretive uncertainty and provides a reusable reference for consistent, technology-agnostic compliance verification under the AI Act.",human
"The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving ""sink"" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios, which can greatly and fundamentally accelerate LLM long-context inference speed by up to 100%. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation",human
"Attention improves representation learning over RNNs, but its discrete nature limits continuous-time (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates attention logits computation as the solution to a linear first-order ODE with nonlinear interlinked gates derived from repurposing Neuronal Circuit Policies (NCPs) wiring mechanism. NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse backbone network with two heads for computing and gates, enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i) explicit Euler integration, (ii) exact closed-form solution, and (iii) steady-state approximation. To improve memory intensity, we implemented a sparse Top- pairwise concatenation scheme that selectively curates key-query interactions. We provide rigorous theoretical guarantees, including state stability, bounded approximation errors, and universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for autonomous vehicles, and industrial prognostics. We observed that NAC matches or outperforms competing baselines in accuracy and occupies an intermediate position in runtime and memory efficiency compared with several CT baselines.",human
"The development of nuclear fusion requires materials that can withstand extreme conditions. The IFMIF-DONES facility, a high-power particle accelerator, is being designed to qualify these materials. A critical testbed for its development is the MuVacAS prototype, which replicates the final segment of the accelerator beamline. Precise regulation of argon gas pressure within its ultra-high vacuum chamber is vital for this task. This work presents a fully data-driven approach for autonomous pressure control. A Deep Learning Surrogate Model, trained on real operational data, emulates the dynamics of the argon injection system. This high-fidelity digital twin then serves as a fast-simulation environment to train a Deep Reinforcement Learning agent. The results demonstrate that the agent successfully learns a control policy that maintains gas pressure within strict operational limits despite dynamic disturbances. This approach marks a significant step toward the intelligent, autonomous control systems required for the demanding next-generation particle accelerator facilities.",human
"Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at {Github}.",human
"Sequence modeling has produced diverse architectures -- from classical recurrent neural networks to modern Transformers and state space models (SSMs) -- yet a unified theoretical understanding of expressivity and trainability trade-offs remains limited. We introduce a unified framework that represents a broad class of sequence maps via an input-dependent effective interaction operator , making explicit two recurring construction patterns: (i) the Unified Factorized Framework (Explicit) (attention-style mixing), in which varies through scalar coefficients applied to shared value maps, and (ii) Structured Dynamics (Implicit) (state-space recurrences), in which is induced by a latent dynamical system. Using this framework, we derive three theoretical results. First, we establish the Interaction Rank Gap: models in the Unified Factorized Framework, such as single-head attention, are constrained to a low-dimensional operator span and cannot represent certain structured dynamical maps. Second, we prove an Equivalence (Head-Count) Theorem showing that, within our multi-head factorized class, representing a linear SSM whose lag operators span a -dimensional subspace on length- sequences requires and is achievable with heads. Third, we prove a Gradient Highway Result, showing that attention layers admit inputs with distance-independent gradient paths, whereas stable linear dynamics exhibit distance-dependent gradient attenuation. Together, these results formalize a fundamental trade-off between algebraic expressivity (interaction/operator span) and long-range gradient propagation, providing theoretical grounding for modern sequence architecture design.",human
"We address the challenge of sample inefficiency in reinforcement learning (RL) for continuous control tasks, specifically those requiring exploration in high-dimensional state spaces. Traditional RL algorithms often struggle to learn effective policies within a reasonable timeframe due to the vast exploration space and the sparse reward signals typical of these environments. To mitigate this issue, we introduce a novel approach combining imitation learning (IL) with model-based reinforcement learning (MBRL). Our method, termed ""Guided Exploration with Learned Dynamics"" (GELD), leverages IL to pre-train a policy that provides a strong exploratory bias towards promising regions of the state space. Subsequently, a learned dynamics model, trained on data collected by this biased policy, is used within a MBRL framework to further refine the policy and optimize for long-term rewards. We evaluate GELD on a suite of challenging continuous control benchmarks, including MuJoCo locomotion tasks and a simulated robotic manipulation environment. Experimental results demonstrate that GELD significantly outperforms state-of-the-art model-free and model-based RL algorithms in terms of sample efficiency and asymptotic performance. Specifically, GELD achieves comparable or superior performance to existing methods while requiring substantially fewer interactions with the environment, indicating its potential for real-world applications.",ai
"We address the problem of representing and reasoning with incomplete and uncertain relational knowledge in dynamic environments. Existing knowledge representation formalisms often struggle to maintain consistency and efficiency when faced with evolving datasets and imprecise information. Our approach introduces a novel framework, Probabilistic Relational Concept Graphs (PRCGs), which combines the expressive power of concept graphs with probabilistic logic to represent entities, relations, and their associated uncertainties. PRCGs leverage Bayesian networks to model probabilistic dependencies between concepts and relationships, allowing for efficient belief propagation and inference under uncertainty. We propose a novel algorithm for dynamically updating PRCGs in response to new information or observed changes in the environment, focusing on maintaining consistency and minimizing computational cost. We evaluate PRCGs on benchmark knowledge base completion and reasoning tasks, demonstrating significant improvements in accuracy and efficiency compared to state-of-the-art approaches, particularly in scenarios involving noisy or incomplete data. Specifically, we observed a 15% increase in link prediction accuracy and a 20% reduction in inference time on a benchmark dataset with a 30% data missing rate. The results indicate the potential of PRCGs to facilitate robust and efficient knowledge representation and reasoning in complex, real-world applications.",ai
"We address the problem of identifying causal effects from observational data in the presence of unobserved confounding and selection bias. Existing methods often rely on strong assumptions regarding the functional form of the relationships between variables or the absence of certain causal pathways. To overcome these limitations, we propose a novel approach, Causal Identification with Moment Restrictions (CIMR), which leverages conditional moment restrictions derived from the causal structure. CIMR exploits instrumental variables present within the causal graph, even if they are conditionally dependent on the unobserved confounders given observed covariates. We formulate the identification problem as solving a system of nonlinear equations derived from these moment restrictions, using a generalized method of moments (GMM) estimator. We demonstrate the efficacy of CIMR through extensive simulations on complex causal graphs, demonstrating superior performance compared to existing methods such as front-door adjustment and inverse probability weighting in scenarios with strong confounding and selection bias. Furthermore, we apply CIMR to a real-world dataset concerning the effect of a medical intervention on patient outcomes, revealing a more accurate estimate of the treatment effect than previously obtained. These results highlight CIMR's ability to robustly identify causal effects under weaker assumptions, enhancing the reliability of causal inference from observational data.",ai
"This work contributes towards balancing the inclusivity and global applicability of natural language processing techniques by proposing the first 'name entity recognition' dataset for Kurdish Sorani, a low-resource and under-represented language, that consists of 64,563 annotated tokens. It also provides a tool for facilitating this task in this and many other languages and performs a thorough comparative analysis, including classic machine learning models and neural systems. The results obtained challenge established assumptions about the advantage of neural approaches within the context of NLP. Conventional methods, in particular CRF, obtain F1-scores of 0.825, outperforming the results of BiLSTM-based models (0.706) significantly. These findings indicate that simpler and more computationally efficient classical frameworks can outperform neural architectures in low-resource settings.",human
"We address the challenge of efficiently solving complex reasoning problems characterized by large search spaces and intricate logical dependencies. Traditional automated reasoning systems often struggle with such problems, exhibiting scalability limitations and requiring substantial manual tuning. To overcome these limitations, we introduce a novel hybrid reasoning framework that integrates symbolic reasoning with neural-guided search. Our method leverages a pre-trained language model fine-tuned on a dataset of logical statements and proofs to generate candidate inferences, effectively pruning the search space explored by a first-order logic theorem prover. The neural component provides a heuristic guidance function, prioritizing promising lines of reasoning and reducing the number of unproductive derivations. Empirical evaluations on a benchmark suite of mathematical and logical reasoning tasks demonstrate a significant improvement in both solution rate and runtime compared to state-of-the-art symbolic reasoners. Specifically, our approach achieves a 30% increase in the number of problems solved within a fixed time budget and a 5x reduction in average solution time on successfully solved problems. These results highlight the potential of integrating neural guidance with symbolic reasoning to tackle challenging problems in automated theorem proving and knowledge representation.",ai
"Accurately and efficiently extracting main content from general web pages is of great significance for obtaining training data for large models. Using well-pre-trained decoder-only generative language models offers excellent document comprehension capabilities, thereby effectively enhancing parsing quality. However, it remains constrained by issues such as context window length, inference cost, and format hallucination. We present Dripper, an efficient HTML main content extraction framework powered by lightweight language models, which addresses these challenges through four key innovations: (1) We design a specialized HTML simplification algorithm that reduces input token count to 22\% compared to raw HTML while preserving critical structural information; (2) We reformulate main content extraction as a semantic block sequence classification task, significantly reducing inference cost; (3) We introduce a controlled decoding mechanism that strictly constrains the output space through logits processors, effectively eliminating hallucination issues common in small-scale models; (4) We propose WebMainBench, an evaluation dataset containing over 7,800 web pages with meticulously human-annotated main content extraction labels. Experimental results demonstrate that using only a 0.6B parameter model, Dripper achieves state-of-the-art performance across all evaluation benchmarks and outperforms all baseline methods, attaining an ROUGE-N F1 score of 81.58\%( 83.13\% with fall-back strategy) on our proposed WebMainBench dataset.",human
"The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak (""model gaps"") and (ii) imbalanced coverage in the benchmarks themselves (""benchmark gaps""). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and computing saliency-weighted performance scores across benchmark data, the method grounds evaluation in the model's internal representations and enables comparison across benchmarks. As examples demonstrating our approach, we applied the method to two popular open-source models and ten benchmarks. We found that these models consistently underperformed on concepts that stand in contrast to sycophantic behaviors (e.g., politely refusing a request or asserting boundaries) and concepts connected to safety discussions. These model gaps align with observations previously surfaced in the literature; our automated, unsupervised method was able to recover them without manual supervision. We also observed benchmark gaps: many of the evaluated benchmarks over-represented concepts related to obedience, authority, or instruction-following, while missing core concepts that should fall within their intended scope. In sum, our method offers a representation-grounded approach to evaluation, enabling concept-level decomposition of benchmark scores. Rather than replacing conventional aggregated metrics, CG complements them by providing a concept-level decomposition that can reveal why a model scored as it did and how benchmarks could evolve to better reflect their intended scope. Code is available at https://competency-gaps.github.io.",human
"Graph Neural Networks (GNNs) have emerged as a powerful paradigm for learning on non-Euclidean data, exhibiting success in various domains such as social network analysis, drug discovery, and recommender systems. However, the performance of GNNs is highly sensitive to the choice of aggregation scheme and architectural design, often requiring extensive hyperparameter tuning and architectural search. This work addresses the problem of efficiently optimizing GNN architectures by introducing a novel differentiable neural architecture search (NAS) framework specifically tailored for graph-structured data. Our method leverages a continuous relaxation of the architectural search space, allowing for gradient-based optimization of both the GNN topology and its constituent operations. We propose a novel search space that encompasses diverse aggregation mechanisms and connectivity patterns, enabling the discovery of architectures tailored to the specific characteristics of the graph. Empirical evaluations on a suite of benchmark graph learning datasets demonstrate that our proposed NAS framework consistently identifies GNN architectures that outperform manually designed counterparts and existing NAS methods. Specifically, we achieve significant improvements in node classification accuracy and graph classification performance, showcasing the effectiveness of our approach in automating the design of high-performing GNNs. The discovered architectures exhibit interesting properties, revealing insights into the importance of specific aggregation schemes and connectivity patterns for different graph structures.",ai
"Statistical inference in contextual bandits is complicated by the adaptive, non-i.i.d. nature of the data. A growing body of work has shown that classical least-squares inference may fail under adaptive sampling, and that constructing valid confidence intervals for linear functionals of the model parameter typically requires paying an unavoidable inflation of order $$ price of adaptivity. In this paper, we propose and analyze a penalized EXP4 algorithm for linear contextual bandits. Our first main result shows that this procedure satisfies the Lai--Wei stability condition and therefore admits valid Wald-type confidence intervals for linear functionals. Our second result establishes that the same algorithm achieves regret guarantees that are minimax optimal up to logarithmic factors, demonstrating that stability and statistical efficiency can coexist within a single contextual bandit method. Finally, we complement our theory with simulations illustrating the empirical normality of the resulting estimators and the sharpness of the corresponding confidence intervals.",human
"Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.",human
"Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called ""hallucinations"" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.",human
"Transformer architectures have become ubiquitous in natural language processing, achieving state-of-the-art results on a wide range of tasks. However, the computational expense associated with training and deploying large transformer models remains a significant impediment, particularly for resource-constrained environments and applications requiring real-time inference. This work investigates the effectiveness of knowledge distillation in mitigating this computational burden. We propose a novel distillation framework that leverages both intermediate layer representations and attention probabilities from a pre-trained, large-scale transformer teacher model to guide the training of a smaller student model. Specifically, we minimize the discrepancy between the teacher's and student's attention maps and hidden states at multiple layers, alongside the conventional prediction loss. Experiments on benchmark datasets for text classification and question answering demonstrate that our distillation approach yields student models that achieve comparable performance to larger models while exhibiting a substantial reduction in parameters and inference time. Quantitatively, we observe a 40% reduction in model size and a 30% speedup in inference, with a minimal drop in accuracy compared to the teacher model. These findings suggest that our distillation framework offers a promising pathway for deploying high-performing transformer models in resource-constrained settings.",ai
"We address the problem of efficiently representing and reasoning with complex, hierarchical knowledge structures in resource-constrained environments. Existing knowledge representation formalisms often struggle with scalability and computational complexity, hindering their application in real-world scenarios involving large datasets and limited computational resources. To mitigate these limitations, we propose a novel framework, Hierarchical Compositional Embeddings (HCE), which combines distributional and symbolic representation techniques. HCE employs a hierarchical clustering algorithm to automatically discover latent hierarchical structure within a knowledge graph. Each node in the hierarchy is then associated with a learned embedding that captures both its semantic content and its position within the overall knowledge organization. Reasoning is performed via compositional operations on these embeddings, enabling efficient inference of relationships between entities at varying levels of abstraction. We evaluate HCE on several benchmark knowledge graph completion and relation prediction tasks. Our results demonstrate that HCE achieves comparable or superior performance to state-of-the-art methods, while exhibiting significantly reduced computational complexity, particularly in inference tasks involving hierarchical relationships. Furthermore, ablation studies confirm the importance of both the hierarchical clustering and the compositional embedding components of our framework.",ai
"This paper addresses the challenge of efficiently encoding and reasoning with complex relational knowledge in large-scale knowledge graphs. Existing embedding-based approaches often struggle to capture higher-order relationships and compositional semantics, leading to limitations in knowledge graph completion and downstream reasoning tasks. We propose a novel framework, Hierarchical Relational Encoding (HRE), which leverages a hierarchical aggregation scheme over subgraph patterns to construct expressive knowledge representations. HRE iteratively refines entity and relation embeddings by aggregating information from increasingly larger and more complex subgraph structures centered around each entity. This process allows the model to implicitly learn the significance of different relational paths and their compositional effects. We evaluate HRE on standard knowledge graph completion benchmarks (WN18RR, FB15k-237, and NELL-995) and demonstrate significant improvements in link prediction accuracy compared to state-of-the-art methods. Specifically, HRE achieves a 5-10% increase in Mean Reciprocal Rank (MRR) and Hits@1 on these datasets. Furthermore, we analyze the learned embeddings and show that HRE captures more nuanced relational semantics, leading to improved performance on complex reasoning tasks involving multi-hop inference. Our findings suggest that hierarchical subgraph aggregation provides a powerful mechanism for learning effective knowledge representations from large relational datasets.",ai
"We address the problem of training machine learning models on decentralized datasets while guaranteeing differential privacy (DP) for individual data owners. Traditional DP-SGD methods, while effective, often suffer from significant accuracy degradation, especially when dealing with heterogeneous data distributions across participants. To mitigate this, we propose a novel federated learning framework, Privacy-Preserving Adaptive Gradient Aggregation (PAGA), which dynamically adjusts the aggregation weights of local gradients based on their estimated contributions to the global model's convergence. PAGA leverages a privacy-preserving mechanism based on randomized response to estimate these contributions, ensuring that no individual participant's data is revealed during the weight adaptation process. We theoretically analyze the privacy guarantees of PAGA and empirically demonstrate its efficacy on several benchmark datasets, including CIFAR-10 and Fashion-MNIST, under varying degrees of data heterogeneity. Experimental results indicate that PAGA achieves significantly higher accuracy compared to baseline DP-SGD and FedAvg approaches, particularly in non-IID settings, while maintaining rigorous DP guarantees. Specifically, PAGA exhibits improvements of up to 15% in test accuracy for comparable privacy budgets, demonstrating the effectiveness of adaptive gradient aggregation in privacy-preserving federated learning.",ai
"We present a novel approach to enhancing the robustness of automated theorem provers when faced with ambiguous or underspecified natural language theorem statements. Current systems often struggle with the inherent vagueness and contextual dependencies present in such formulations, leading to failure in subsequent formalization and proof search. Our method integrates a probabilistic semantic parser with a large language model (LLM) fine-tuned on a corpus of formal and informal theorem statements, enabling the generation of multiple candidate formalizations weighted by their likelihood given the natural language input. We then employ a Monte Carlo Tree Search (MCTS) algorithm to explore the space of possible proofs, prioritizing branches corresponding to higher-probability formalizations. The MCTS is augmented with a heuristic function that estimates the potential provability of a given formalization based on syntactic similarity to known theorems. Experiments conducted on a benchmark dataset of undergraduate-level mathematical theorems demonstrate a significant improvement in the success rate of our system compared to a baseline employing a single, deterministically parsed formalization. Specifically, we observe a 27% increase in the number of theorems successfully proven, suggesting that our probabilistic approach effectively mitigates the challenges posed by ambiguous natural language. Further analysis reveals a correlation between the diversity of generated formalizations and the overall success rate, highlighting the importance of exploring a wider range of interpretations.",ai
"Comprehension of ancient texts plays an important role in archaeology and understanding of Chinese history and civilization. The rapid development of large language models needs benchmarks that can evaluate their comprehension of ancient characters. Existing Chinese benchmarks are mostly targeted at modern Chinese and transmitted documents in ancient Chinese, but the part of excavated documents in ancient Chinese is not covered. To meet this need, we propose the AncientBench, which aims to evaluate the comprehension of ancient characters, especially in the scenario of excavated documents. The AncientBench is divided into four dimensions, which correspond to the four competencies of ancient character comprehension: glyph comprehension, pronunciation comprehension, meaning comprehension, and contextual comprehension. The benchmark also contains ten tasks, including radical, phonetic radical, homophone, cloze, translation, and more, providing a comprehensive framework for evaluation. We convened archaeological researchers to conduct experimental evaluations, proposed an ancient model as baseline, and conducted extensive experiments on the currently best-performing large language models. The experimental results reveal the great potential of large language models in ancient textual scenarios as well as the gap with humans. Our research aims to promote the development and application of large language models in the field of archaeology and ancient Chinese language.",human
"Current embodied AI systems face severe engineering impediments, primarily characterized by poor cross-scenario adaptability, rigid inter-module coupling, and fragmented inference acceleration. To overcome these limitations, we propose RoboNeuron, a universal deployment framework for embodied intelligence. RoboNeuron is the first framework to deeply integrate the cognitive capabilities of Large Language Models (LLMs) and Vision-Language-Action (VLA) models with the real-time execution backbone of the Robot Operating System (ROS). We utilize the Model Context Protocol (MCP) as a semantic bridge, enabling the LLM to dynamically orchestrate underlying robotic tools. The framework establishes a highly modular architecture that strictly decouples sensing, reasoning, and control by leveraging ROS's unified communication interfaces. Crucially, we introduce an automated tool to translate ROS messages into callable MCP functions, significantly streamlining development. RoboNeuron significantly enhances cross-scenario adaptability and component flexibility, while establishing a systematic platform for horizontal performance benchmarking, laying a robust foundation for scalable real-world embodied applications.",human
"Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned ""harm vector"" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.",human
"The proliferation of machine learning (ML) models trained on sensitive user data raises significant privacy concerns. Existing privacy-preserving techniques, such as differential privacy (DP), often incur substantial accuracy degradation, particularly for complex models and high privacy budgets. This paper addresses the challenge of achieving strong privacy guarantees without sacrificing utility in the context of natural language processing. We propose a novel framework, PrivBERT-Adapt, which leverages parameter-efficient transfer learning in conjunction with a differentially private fine-tuning strategy. Specifically, we adapt a pre-trained BERT model using a small set of tunable parameters, effectively reducing the parameter space requiring DP noise addition. Furthermore, we introduce a dynamic clipping mechanism that adapts to the observed gradient norms during training, mitigating the impact of noisy gradients on model convergence. We evaluate PrivBERT-Adapt on a range of text classification tasks, demonstrating significant improvements in accuracy compared to traditional DP fine-tuning methods with equivalent privacy budgets. Empirical results show that our approach can achieve up to a 15% increase in accuracy while maintaining strong (ε, δ)-differential privacy. The findings suggest that parameter-efficient adaptation and dynamic clipping offer a promising avenue for developing practical and privacy-preserving NLP models.",ai
"Advanced LLMs have achieved near-ceiling instruction-following accuracy on benchmarks such as IFEval. However, these impressive scores do not necessarily translate to reliable services in real-world use, where users often vary their phrasing, contextual framing, and task formulations. In this paper, we study nuance-oriented reliability: whether models exhibit consistent competence across cousin prompts that convey analogous user intents but with subtle nuances. To quantify this, we introduce a new metric, reliable@k, and develop an automated pipeline that generates high-quality cousin prompts via data augmentation. Building upon this, we construct IFEval++ for systematic evaluation. Across 20 proprietary and 26 open-source LLMs, we find that current models exhibit substantial insufficiency in nuance-oriented reliability -- their performance can drop by up to 61.8% with nuanced prompt modifications. What's more, we characterize it and explore three potential improvement recipes. Our findings highlight nuance-oriented reliability as a crucial yet underexplored next step toward more dependable and trustworthy LLM behavior. Our code and benchmark are accessible: https://github.com/jianshuod/IFEval-pp.",human
"Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.",human
"Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.",human
"Graph Neural Networks (GNNs) have emerged as a powerful paradigm for representation learning on graph-structured data. However, training GNNs on large-scale graphs remains a significant challenge due to the computational complexity associated with neighborhood aggregation. Existing approaches often rely on sampling strategies to mitigate this issue, potentially sacrificing accuracy. We address the problem of efficient and scalable GNN training by introducing a novel framework that leverages spectral graph theory. Our method, SpectralGNN++, decomposes the graph Laplacian into a set of orthogonal eigenvectors, enabling efficient message passing in the spectral domain. This allows us to approximate the GNN convolution operation using a truncated eigenvalue expansion, significantly reducing the computational burden while preserving critical structural information. We theoretically analyze the approximation error introduced by the truncation and demonstrate its dependence on the eigenvalue decay rate of the graph Laplacian. Empirical evaluations on several benchmark datasets, including citation networks and social networks, demonstrate that SpectralGNN++ achieves comparable or superior accuracy compared to state-of-the-art GNNs, while exhibiting a substantial reduction in training time, particularly on large and sparse graphs. Our results indicate that SpectralGNN++ offers a promising avenue for scaling GNNs to increasingly complex and large-scale graph datasets.",ai
"We investigate the problem of enhancing automated reasoning capabilities within complex logical systems, specifically focusing on first-order logic with equality and function symbols. Traditional theorem provers often struggle with scalability due to the exponential search space inherent in proof construction. This limitation impedes their applicability to domains requiring intricate reasoning processes. To address this challenge, we introduce a novel reinforcement learning framework, termed Proof-Guided Exploration (PGE), which combines the strengths of Monte Carlo Tree Search (MCTS) with learned heuristics derived from successful proof trajectories. PGE learns to prioritize promising inference steps based on a value network trained on a corpus of proven theorems. This value network estimates the likelihood of a given state (logical formula) leading to a valid proof. Furthermore, we incorporate a policy network that proposes actions (inference rules) informed by the current state's logical structure. We evaluate PGE on a diverse benchmark suite of theorem proving problems drawn from the TPTP library and the Mizar Mathematical Library. Our results demonstrate that PGE significantly outperforms state-of-the-art automated theorem provers, achieving a 20-30% increase in the number of proven theorems within a fixed time limit. The learned heuristics effectively guide the search process, enabling the discovery of more concise and efficient proofs.",ai
"We investigate the challenge of training reinforcement learning agents for complex, multi-stage tasks characterized by sparse and delayed rewards. Traditional methods often struggle in such environments due to the difficulty in effectively propagating reward signals back through long temporal sequences. This work introduces a novel approach that combines Hierarchical Reinforcement Learning (HRL) with intrinsic motivation based on prediction error. Specifically, we propose a two-level hierarchy: a meta-controller responsible for setting high-level goals and a lower-level controller trained to achieve these goals. The meta-controller receives extrinsic rewards from the environment, while the lower-level controller is driven by an intrinsic reward signal derived from its ability to predict future states. We hypothesize that this hierarchical structure, coupled with prediction-based intrinsic motivation, facilitates more efficient exploration and learning in sparse-reward environments. We empirically evaluate our proposed method on a benchmark set of challenging navigation and manipulation tasks. Results demonstrate that our approach significantly outperforms both flat reinforcement learning baselines and alternative HRL implementations that rely solely on extrinsic reward shaping. We observe a substantial improvement in sample efficiency and overall task performance, indicating the effectiveness of prediction error as a driver for exploration and learning in hierarchical reinforcement learning frameworks.",ai
"Reduced order models (ROM) can represent spatiotemporal processes in significantly fewer dimensions and can be solved many orders faster than their governing partial differential equations (PDEs). For example, using a proper orthogonal decomposition produces a ROM that is a small linear combination of fixed features and weights, but that is constrained to the given process it models. In this work, we explore a new type of ROM that is not constrained to fixed weights, based on neural Galerkin-Projections, which is an initial value problem that encodes the physics of the governing PDEs, calibrated via neural networks to accurately model the trajectory of these weights. Then using a statistical hierarchical pooling technique to learn a distribution on the initial values of the temporal weights, we can create new, statistically interpretable and physically justified weights that are generalized to many similar problems. When recombined with the spatial features, we form a complete physics surrogate, called a randPROM, for generating simulations that are consistent in distribution to a neighborhood of initial conditions close to those used to construct the ROM. We apply the randPROM technique to the study of tsunamis, which are unpredictable, catastrophic, and highly-detailed non-linear problems, modeling both a synthetic case of tsunamis near Fiji and the real-world Tohoku 2011 disaster. We demonstrate that randPROMs may enable us to significantly reduce the number of simulations needed to generate a statistically calibrated and physically defensible prediction model for arrival time and height of tsunami waves.",human
"The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase. In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.",human
"We address the problem of enhancing the logical consistency and inferential accuracy of large language models (LLMs) when applied to complex reasoning tasks requiring multi-hop deduction. While LLMs excel at capturing statistical relationships in language, they often falter when faced with problems requiring systematic application of logical rules and constraints. We propose a novel method that integrates LLM-generated intermediate reasoning steps with a formal theorem prover. Specifically, we leverage LLMs to decompose complex problems into smaller, potentially more manageable sub-problems, represented as logical statements in a formal language. These statements, along with the original problem's premises, are then processed by a first-order logic theorem prover (specifically, a resolution-based prover). The theorem prover exhaustively explores the logical consequence space, verifying the validity of the LLM-suggested solution path and identifying potential inconsistencies. If inconsistencies are detected, the prover generates counter-examples, which are then fed back to the LLM as feedback to refine its reasoning process. Empirical evaluations on a suite of benchmark logical reasoning datasets demonstrate a significant improvement in accuracy compared to LLMs operating in a standalone fashion. Furthermore, the integration with the theorem prover provides a verifiable justification for the solution, enhancing transparency and trustworthiness in automated reasoning systems. Our approach achieves a 20-35% increase in problem-solving accuracy across various dataset difficulties, demonstrating the efficacy of combining LLM's generative capabilities with formal verification techniques.",ai
"We address the challenge of efficiently representing and reasoning with complex relational knowledge graphs, a critical component for many downstream machine learning tasks. Existing approaches often struggle to balance expressive power with computational tractability, leading to either limited knowledge capture or high inference costs. This work introduces a novel knowledge representation framework, Relational Attentive Graph Embeddings (RAGE), that integrates attentional mechanisms within graph neural networks to selectively aggregate information from relevant neighboring entities and relations. RAGE employs a multi-head attention mechanism, parameterized by relation-specific learned embeddings, to dynamically weight the importance of neighboring nodes during message passing. Furthermore, we incorporate a novel regularization term based on mutual information minimization between attention heads to promote diversity and reduce redundancy in the learned attention distributions. We evaluate RAGE on several benchmark knowledge graph completion tasks, including link prediction and triple classification, using datasets such as FB15k-237 and WN18RR. Experimental results demonstrate that RAGE outperforms state-of-the-art methods in terms of Mean Reciprocal Rank (MRR) and Hits@N metrics, while maintaining comparable computational efficiency. Ablation studies validate the efficacy of the attention mechanism and the mutual information regularization. This highlights RAGE's ability to effectively capture nuanced relational dependencies within knowledge graphs, enabling more accurate and efficient knowledge representation and reasoning.",ai
"Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering ""How accurate is the RM's preference perception for given samples?"", it employs scientific auditing to answer: ""Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?"". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.",human
"We address the challenge of effectively representing and reasoning with complex causal knowledge within natural language processing systems. Existing knowledge representation schemes often struggle to capture the nuances of causal relationships expressed in unstructured text, leading to limitations in downstream tasks such as question answering and causal inference. We propose a novel approach, the Causal Relation Graph Augmentation (CRGA) framework, which leverages graph neural networks to enrich pre-trained language models with structured causal knowledge extracted from textual corpora. CRGA constructs a knowledge graph of causal relations, where nodes represent entities and edges represent causal dependencies, weighted by the strength and type of the relation. This graph is then integrated into a language model via an attention mechanism, allowing the model to selectively attend to relevant causal relations during processing. We evaluate CRGA on two benchmark datasets for causal reasoning: the Causal Reasoning in Language Models (CRLM) dataset and a novel dataset specifically designed to test the understanding of complex causal chains. Results demonstrate that CRGA significantly improves performance over state-of-the-art baselines, achieving gains of up to 15% in accuracy on the CRLM dataset and 22% on our newly introduced dataset. These findings suggest that explicitly representing and integrating causal knowledge graphs can substantially enhance the reasoning capabilities of language models.",ai
"Deep Neural Networks (DNNs), as valuable intellectual property, face unauthorized use. Existing protections, such as digital watermarking, are largely passive; they provide only post-hoc ownership verification and cannot actively prevent the illicit use of a stolen model. This work proposes a proactive protection scheme, dubbed ``Authority Backdoor,"" which embeds access constraints directly into the model. In particular, the scheme utilizes a backdoor learning framework to intrinsically lock a model's utility, such that it performs normally only in the presence of a specific trigger (e.g., a hardware fingerprint). But in its absence, the DNN's performance degrades to be useless. To further enhance the security of the proposed authority scheme, the certifiable robustness is integrated to prevent an adaptive attacker from removing the implanted backdoor. The resulting framework establishes a secure authority mechanism for DNNs, combining access control with certifiable robustness against adversarial attacks. Extensive experiments on diverse architectures and datasets validate the effectiveness and certifiable robustness of the proposed framework.",human
"Many large language models (LLMs) are trained on a massive body of knowledge present on the Internet. Darth Vecdor (DV) was designed to extract this knowledge into a structured, terminology-mapped, SQL database (""knowledge base"" or ""knowledge graph""). Knowledge graphs may be useful in many domains, including healthcare. Although one might query an LLM directly rather than a SQL-based knowledge graph, concerns such as cost, speed, safety, and confidence may arise, especially in high-volume operations. These may be mitigated when the information is pre-extracted from the LLM and becomes query-able through a standard database. However, the author found the need to address several issues. These included erroneous, off-topic, free-text, overly general, and inconsistent LLM responses, as well as allowing for multi-element responses. DV was built with features intended to mitigate these issues. To facilitate ease of use, and to allow for prompt engineering by those with domain expertise but little technical background, DV provides a simple, browser-based graphical user interface. DV has been released as free, open-source, extensible software, on an ""as is"" basis, without warranties or conditions of any kind, either express or implied. Users need to be cognizant of the potential risks and benefits of using DV and its outputs, and users are responsible for ensuring any use is safe and effective. DV should be assumed to have bugs, potentially very serious ones. However, the author hopes that appropriate use of current and future versions of DV and its outputs can help improve healthcare.",human
"Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.",human
"**Abstract:** Accurate and efficient segmentation of anatomical structures in medical images is crucial for diagnosis, treatment planning, and post-treatment evaluation. Manual segmentation by expert radiologists is a time-consuming and subjective process. This work addresses the challenge of automating robust and precise segmentation in multi-modal medical imaging using a novel deep learning architecture. We propose a hierarchical multi-scale convolutional neural network (HM-CNN) that leverages both local and global contextual information for improved segmentation performance. The HM-CNN incorporates a cascaded architecture, where the initial stage provides a coarse segmentation, which is subsequently refined by subsequent stages focusing on finer details. Furthermore, the network integrates attention mechanisms to emphasize salient features and mitigate the impact of image artifacts and anatomical variations. We evaluated the proposed approach on two publicly available datasets: the Medical Segmentation Decathlon (MSD) and the LIDC-IDRI dataset. Results demonstrate that the HM-CNN achieves state-of-the-art performance in segmenting complex anatomical structures such as the liver and lung nodules. Specifically, our method yields a mean Dice Similarity Coefficient (DSC) of 0.92 for liver segmentation and 0.85 for lung nodule segmentation, outperforming existing methods by a significant margin. The proposed HM-CNN offers a promising solution for automated medical image segmentation, potentially reducing the workload of radiologists and improving the accuracy and consistency of clinical decision-making.",ai
"Dynamic graphs are prevalent in real-world scenarios, where continuous structural changes induce catastrophic forgetting in graph neural networks (GNNs). While continual learning has been extended to dynamic graphs, existing methods overlook the effects of topological changes on existing nodes. To address it, we propose a novel framework for continual learning on dynamic graphs, named Condensation-Concatenation-based Continual Learning (CCC). Specifically, CCC first condenses historical graph snapshots into compact semantic representations while aiming to preserve the original label distribution and topological properties. Then it concatenates these historical embeddings with current graph representations selectively. Moreover, we refine the forgetting measure (FM) to better adapt to dynamic graph scenarios by quantifying the predictive performance degradation of existing nodes caused by structural updates. CCC demonstrates superior performance over state-of-the-art baselines across four real-world datasets in extensive experiments.",human
"We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.",human
"Predictive maintenance of rotating machinery increasingly relies on vibration signals, yet most learning-based approaches either discard phase during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information. This paper introduces two phase-aware preprocessing strategies to address random phase variations in multi-axis vibration data: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Using a newly constructed rotor dataset acquired with a synchronized three-axis sensor, we evaluate six deep learning architectures under a two-stage learning framework. Results demonstrate architecture-independent improvements: the three-axis independent method achieves consistent gains (+2.7\% for Transformer), while the single-axis reference approach delivers superior performance with up to 96.2\% accuracy (+5.4\%) by preserving spatial phase relationships. These findings establish both phase alignment strategies as practical and scalable enhancements for predictive maintenance systems.",human
"Recent advancements in cooperative multi-agent systems have demonstrated promise in addressing complex tasks; however, scaling these systems to environments with high-dimensional state spaces and sparse rewards remains a significant challenge. Traditional reinforcement learning techniques often struggle with exploration and credit assignment in such scenarios, leading to suboptimal coordination and learning inefficiency. This paper introduces a novel approach leveraging a hierarchical communication framework coupled with a decentralized actor-critic architecture. Agents are organized into dynamic sub-groups, each electing a leader responsible for aggregating local information and communicating high-level intentions to other group leaders. This hierarchical communication structure reduces the communication bottleneck and facilitates more efficient information dissemination. We further augment the actor-critic agents with a shaped reward function derived from a potential-based difference reward, encouraging agents to contribute positively to the overall team performance. Empirical evaluations in benchmark multi-agent environments, including StarCraft II micromanagement scenarios and cooperative navigation tasks, demonstrate that our approach significantly outperforms state-of-the-art baselines in terms of both sample efficiency and final performance. The results highlight the effectiveness of hierarchical communication and difference rewards in promoting effective coordination and accelerating learning in complex multi-agent systems.",ai
"Endmember extraction from hyperspectral images aims to identify the spectral signatures of materials present in a scene. Recent studies have shown that self-dictionary methods can achieve high extraction accuracy; however, their high computational cost limits their applicability to large-scale hyperspectral images. Although several approaches have been proposed to mitigate this issue, it remains a major challenge. Motivated by this situation, this paper pursues a data reduction approach. Assuming that the hyperspectral image follows the linear mixing model with the pure-pixel assumption, we develop a data reduction technique that removes pixels that do not contain endmembers. We analyze the theoretical properties of this reduction step and show that it preserves pixels that lie close to the endmembers. Building on this result, we propose a data-reduced self-dictionary method that integrates the data reduction with a self-dictionary method based on a linear programming formulation. Numerical experiments demonstrate that the proposed method can substantially reduce the computational time of the original self-dictionary method without sacrificing endmember extraction accuracy.",human
"We address the challenge of training machine learning models on sensitive data while providing rigorous privacy guarantees. Specifically, we investigate differentially private federated learning (DP-FL) with a focus on mitigating the trade-off between privacy, accuracy, and communication efficiency. Existing DP-FL algorithms often rely on strong gradient clipping or aggressive noise addition to ensure privacy, leading to significant utility degradation, particularly in heterogeneous data settings. To address this, we propose a novel DP-FL framework incorporating adaptive client selection and dynamic noise calibration. Our method leverages client-side data heterogeneity measures to prioritize the selection of clients with more representative local data distributions, thereby reducing the impact of noisy gradients. Furthermore, we dynamically adjust the level of Gaussian noise added to the aggregated gradients based on the observed empirical sensitivity, informed by a privacy accountant that carefully tracks the privacy cost accrued throughout training. We theoretically analyze the privacy guarantees of our framework using Rényi Differential Privacy (RDP) and demonstrate improved utility bounds compared to standard DP-FL approaches. Empirical evaluations on benchmark federated learning datasets, including CIFAR-10 and MNIST, confirm that our method achieves significantly higher accuracy and faster convergence rates, while maintaining comparable privacy levels to existing state-of-the-art DP-FL algorithms. These results highlight the efficacy of adaptive client selection and dynamic noise calibration in improving the performance of privacy-preserving federated learning.",ai
"We address the challenge of robust causal effect estimation in the presence of unobserved confounding and selection bias, a prevalent issue in observational studies. Traditional causal inference methods, such as propensity score matching and inverse probability weighting, often yield biased estimates when the assumption of no unobserved confounding is violated. To mitigate this, we propose a novel instrumental variable (IV) method leveraging auxiliary variables exhibiting weak, potentially non-linear, relationships with both the treatment and outcome. Our approach, termed Weak Instrument Regularized Causal Estimation (WIRCE), introduces a regularized optimization framework that simultaneously identifies valid weak instruments and estimates the causal effect while penalizing solutions sensitive to instrument misspecification. WIRCE employs a differentiable relaxation of instrument validity criteria within the objective function, allowing for efficient gradient-based optimization. We demonstrate through extensive simulations and experiments on real-world datasets that WIRCE consistently outperforms state-of-the-art methods, including standard IV estimators and confounder adjustment techniques, particularly in scenarios with weak instruments and complex confounding structures. Empirical results indicate a significant reduction in bias and improved robustness to instrument invalidity, enabling more reliable causal effect inference from observational data.",ai
"We address the problem of automated lesion detection and characterization in medical imaging, specifically focusing on improving diagnostic accuracy and reducing inter-observer variability in pulmonary nodule assessment from computed tomography (CT) scans. Current methods often suffer from limitations in sensitivity to subtle nodule characteristics and robustness to image noise and anatomical variations. We propose a novel multi-scale convolutional neural network (CNN) architecture incorporating attention mechanisms to selectively emphasize diagnostically relevant image features. The network leverages a hierarchical feature extraction scheme that aggregates information from varying receptive fields, enabling both fine-grained texture analysis and contextual understanding of nodule surroundings. Furthermore, the attention modules dynamically weight feature maps based on their relevance to the classification task, mitigating the impact of irrelevant background structures. Experimental results on a large, publicly available dataset of lung CT scans demonstrate a significant improvement in nodule detection sensitivity at a clinically relevant false positive rate compared to state-of-the-art methods. Specifically, we achieve a 90% sensitivity for nodule detection with an average of 2 false positives per scan, surpassing the performance of existing approaches by a margin of 5%. These findings suggest the potential for our method to enhance the efficiency and accuracy of pulmonary nodule screening and diagnosis.",ai
"Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations. This mismatch leaves no suitable method to solve surface PDEs directly within the neural domain, forcing explicit mesh extraction or per-instance residual training, preventing end-to-end workflows. We present a novel, mesh-free formulation that learns a local update operator conditioned on neural (local) shape attributes, enabling surface PDEs to be solved directly where the (neural) data lives. The operator integrates naturally with prevalent neural surface representations, is trained once on a single representative shape, and generalizes across shape and topology variations, enabling accurate, fast inference without explicit meshing or per-instance optimization while preserving differentiability. Across analytic benchmarks (heat equation and Poisson solve on sphere) and real neural assets across different representations, our method slightly outperforms CPM while remaining reasonably close to FEM, and, to our knowledge, delivers the first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations. Code will be released on acceptance.",human
"## Privacy-Preserving Language Model Training via Federated Averaging with Differential Privacy Training large language models often requires access to sensitive user data, raising significant privacy concerns. Federated Learning (FL) offers a decentralized approach to training models without directly accessing the raw data, but it can still be vulnerable to inference attacks. This work addresses the problem of training language models within a privacy-preserving FL framework by combining federated averaging with differential privacy (DP). We propose a novel approach that integrates gradient clipping and noise addition at both the client and server levels to guarantee user-level DP. Specifically, we analyze the trade-off between model utility and privacy budget expenditure in the context of language model training. We conduct experiments on a variety of text datasets, including a sensitive medical text corpus, to evaluate the performance of our DP-FL framework. Our results demonstrate that we can achieve a meaningful balance between model accuracy and strong privacy guarantees, as measured by ε and δ, compared to standard federated averaging. Furthermore, we investigate the impact of different DP parameters, such as clipping norm and noise scale, on model performance and privacy leakage, providing guidelines for parameter selection in practical applications. This work offers a practical and effective solution for privacy-preserving language model training in federated settings.",ai
"We propose Love First, Know Later: a paradigm shift in computational matching that simulates interactions first, then assesses compatibility. Instead of comparing static profiles, our framework leverages LLMs as text world engines that operate in dual capacity-as persona-driven agents following behavioral policies and as the environment modeling interaction dynamics. We formalize compatibility assessment as a reward-modeling problem: given observed matching outcomes, we learn to extract signals from simulations that predict human preferences. Our key insight is that relationships hinge on responses to critical moments-we translate this observation from relationship psychology into mathematical hypotheses, enabling effective simulation. Theoretically, we prove that as LLM policies better approximate human behavior, the induced matching converges to optimal stable matching. Empirically, we validate on speed dating data for initial chemistry and divorce prediction for long-term stability. This paradigm enables interactive, personalized matching systems where users iteratively refine their agents, unlocking future possibilities for transparent and interactive compatibility assessment.",human
"We investigate the problem of distributionally robust optimization (DRO) within machine learning contexts, specifically addressing scenarios where the training data exhibits significant distributional shift compared to the deployment environment. Traditional empirical risk minimization (ERM) often falters under such conditions, leading to poor generalization performance. We propose a novel DRO framework leveraging a Wasserstein ambiguity set centered around the empirical distribution, but critically augmented with a learned representation of the underlying data manifold. This manifold-aware ambiguity set allows us to constrain the worst-case risk over distributions that are close to the training data *and* respect the intrinsic structure of the data, mitigating the effect of adversarial examples and improving robustness to distributional shifts. We present an efficient algorithmic solution based on stochastic gradient descent with proximal updates, enabling scalability to high-dimensional datasets. Experimental results on benchmark image classification and regression tasks, under various simulated distributional shifts, demonstrate that our manifold-aware DRO approach consistently outperforms standard ERM and existing DRO methods. Specifically, we observe a significant improvement in out-of-distribution accuracy, achieving up to a 15% increase compared to state-of-the-art baselines while maintaining competitive in-distribution performance. These findings highlight the importance of incorporating data structure information into DRO frameworks for enhanced robustness in machine learning models.",ai
"We investigate the challenge of efficient exploration in reinforcement learning (RL) within environments characterized by sparse, delayed rewards and high-dimensional state spaces. Such environments often present significant obstacles for traditional RL algorithms, leading to protracted learning times and suboptimal policies. To address this, we propose a novel exploration strategy, termed ""Variance-Aware Bayesian Upper Confidence Bound"" (VAB-UCB), which integrates a Bayesian neural network (BNN) for state-value function approximation with an exploration bonus derived from the predictive variance of the BNN. Unlike conventional UCB methods that rely on aleatoric uncertainty, VAB-UCB leverages the epistemic uncertainty captured by the BNN to prioritize exploration in regions where the model exhibits high disagreement, indicative of unexplored or poorly understood state-action pairs. We derive theoretical regret bounds demonstrating that VAB-UCB achieves a sublinear regret performance. Empirically, we evaluate VAB-UCB across a suite of benchmark environments, including challenging robotic manipulation tasks and navigation problems with sparse rewards. Results demonstrate that VAB-UCB consistently outperforms existing state-of-the-art exploration strategies, exhibiting faster learning rates and achieving higher asymptotic performance. Specifically, VAB-UCB demonstrates a significant improvement in sample efficiency compared to ε-greedy, UCB1, and Thompson Sampling baselines, particularly in environments with deceptive local optima.",ai
"We address the problem of identifying causal effects from observational data in the presence of unobserved confounding and selection bias. Traditional methods often rely on strong assumptions about the causal structure or the absence of selection bias. We propose a novel instrumental variable (IV) approach, termed Conditional Independence Restricted Instrumental Variable (CIR-IV), which leverages conditional independence relations to identify valid instruments even when selection bias is present. Specifically, CIR-IV employs a two-stage procedure. First, we utilize constraint-based causal discovery to identify potential instruments satisfying conditional independence criteria given observed covariates. This step mitigates the impact of selection bias by conditioning on appropriate variables. Second, we estimate the causal effect using a generalized method of moments (GMM) estimator, exploiting the identified instruments and accounting for potential non-linear relationships. We establish theoretical conditions for the identifiability of the causal effect under CIR-IV. We demonstrate the performance of CIR-IV through extensive simulations, illustrating its robustness to various forms of selection bias and unobserved confounding. Furthermore, we apply CIR-IV to a real-world dataset concerning the effect of education on income, demonstrating its practical applicability and potential to uncover causal relationships in complex observational settings. Empirical results suggest CIR-IV can provide more accurate causal effect estimates compared to standard IV methods under selection bias.",ai
"**Abstract:** Transformer networks have emerged as a dominant architecture in Natural Language Processing (NLP), demonstrating state-of-the-art performance across a wide range of tasks. However, the computational complexity inherent in processing long sequences remains a significant challenge, limiting applicability to resource-constrained environments and impeding the processing of lengthy documents. This work investigates the application of knowledge distillation to mitigate this limitation. We propose a novel distillation framework that leverages a large, pre-trained transformer model (the teacher) to train a smaller, more efficient transformer model (the student). Our approach focuses on distilling not only the final prediction probabilities but also intermediate representations, specifically attention weights, to guide the student's learning process. We evaluate our method on several benchmark NLP tasks, including text classification and question answering. Results indicate that the distilled student models achieve comparable performance to larger models with significantly reduced computational costs, exhibiting a reduction in parameters of up to 40% and inference time improvements of up to 30%, without substantial degradation in accuracy. This demonstrates the efficacy of knowledge distillation in producing efficient transformer models suitable for deployment in resource-limited settings.",ai
"We address the challenge of accurately estimating causal effects in observational studies when confounding is present and the true causal relationships are unknown. Traditional methods, such as regression adjustment and propensity score matching, rely on strong assumptions regarding the absence of unmeasured confounding and the correct functional form of the relationships between variables. In this work, we propose a novel approach that leverages the principles of instrumental variable (IV) estimation, combined with a learned representation of the data designed to be invariant to the confounders. Our method employs a deep neural network to learn a latent representation that disentangles the effects of the treatment and confounders on the outcome. This representation is then used to identify valid instruments within the observed variables, which are subsequently used in a two-stage least squares (2SLS) procedure to estimate the causal effect. We demonstrate the efficacy of our approach through extensive simulations and experiments on real-world datasets. Our results show that the proposed method significantly outperforms existing causal inference techniques, particularly in scenarios with high-dimensional confounders and non-linear relationships, achieving more accurate and robust causal effect estimates. We also present ablation studies that demonstrate the importance of the invariance constraint in learning the latent representation.",ai
"We address the problem of training machine learning models on sensitive data while providing rigorous privacy guarantees. Specifically, we consider the scenario where data is distributed across multiple parties, and a centralized model is desired without revealing individual data points. Existing approaches based on differential privacy often suffer from significant utility loss, especially in high-dimensional settings. We propose a novel differentially private federated learning algorithm leveraging a combination of gradient compression techniques and a privacy amplification mechanism. Our method, termed ""Private Compressed Federated Averaging"" (PCFA), employs structured random matrices for gradient sparsification, reducing the communication overhead and the magnitude of noise required for privacy protection. Furthermore, we derive a tighter privacy accounting bound that exploits the composition properties of our algorithm, leading to improved privacy-utility trade-offs. We empirically evaluate PCFA on benchmark datasets for image classification and natural language processing tasks. Our results demonstrate that PCFA achieves comparable or superior performance to state-of-the-art differentially private federated learning algorithms, particularly in scenarios with high communication constraints or stringent privacy requirements. The observed improvements highlight the efficacy of combining gradient compression with refined privacy accounting for privacy-preserving distributed learning.",ai
"We investigate the application of transformer-based architectures to the problem of long-range dependency modeling in natural language processing. While transformers have demonstrated state-of-the-art performance in various NLP tasks, their computational complexity scales quadratically with sequence length, posing a significant challenge for processing extended textual inputs. This work explores a novel adaptation of the sparse transformer architecture, incorporating a learned sparsity pattern informed by linguistic structure. Specifically, we introduce a dependency-guided attention mechanism that selectively attends to words connected via syntactic dependencies, alongside a global attention component for capturing broader contextual relationships. We evaluate our approach on a suite of benchmark datasets for long document classification and question answering, focusing on settings where capturing distant dependencies is crucial. Experimental results demonstrate that our dependency-guided sparse transformer achieves comparable or superior performance to full attention transformers, while significantly reducing computational cost and memory footprint. Furthermore, analysis of the learned attention patterns reveals an alignment with linguistically motivated connections, suggesting that incorporating explicit structural information can improve the efficiency and interpretability of transformer models. The findings indicate the potential for structurally informed sparsity to facilitate the scaling of transformers to longer sequences without sacrificing performance.",ai
"Transformer architectures have become the de facto standard for numerous natural language processing tasks. However, their substantial computational demands and memory footprint present significant challenges, particularly when applied to long sequences or deployed in resource-constrained environments. This work investigates strategies for improving the efficiency of transformer models without sacrificing performance. We introduce a novel approach, Hierarchical Attention Pruning (HAP), that combines layer-wise pruning based on attention entropy with a hierarchical clustering of attention heads to identify and remove redundant computational pathways. Specifically, HAP first quantifies the relative importance of each attention layer using the entropy of its attention distribution. Subsequently, within each layer, attention heads are clustered based on similarity of attention patterns, enabling targeted pruning of representative heads. Experiments conducted on benchmark datasets for text classification and machine translation demonstrate that HAP can reduce the number of parameters and floating-point operations (FLOPs) by up to 40% while maintaining comparable or even improved performance relative to baseline transformer models. The proposed approach offers a promising avenue for deploying high-performing transformer models in resource-limited settings.",ai
"There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation quality.",human
"Transformer architectures have revolutionized natural language processing (NLP), demonstrating superior performance across a range of tasks. However, challenges remain in effectively adapting these models to resource-constrained settings and specialized domains. This work addresses the problem of efficient transfer learning for transformers in NLP, specifically focusing on scenarios with limited labeled data. We propose a novel method, termed ""Domain-Adaptive Masked Language Modeling with Contrastive Regularization"" (DAMLM-CR), which combines masked language modeling with a contrastive loss function. DAMLM encourages the model to learn domain-specific language patterns from unlabeled data, while contrastive regularization promotes feature distinctiveness and prevents overfitting on the target task. We evaluate DAMLM-CR on several text classification datasets with varying degrees of domain similarity and labeled data availability. Experimental results demonstrate that our approach consistently outperforms state-of-the-art transfer learning techniques, achieving significant improvements in classification accuracy, particularly in low-resource scenarios. Furthermore, ablation studies validate the contribution of both the domain-adaptive pre-training and the contrastive regularization components. These findings suggest that DAMLM-CR provides an effective and efficient method for adapting transformer models to new domains with limited supervision.",ai
"Graph Neural Networks (GNNs) have emerged as a powerful paradigm for learning representations of graph-structured data, enabling downstream tasks such as node classification and link prediction. However, existing GNN architectures often struggle with capturing long-range dependencies and handling heterogeneous graph structures effectively. This paper introduces a novel GNN framework, Heterogeneous Attentive Message Passing with Graph Transformers (HAMPT), designed to address these limitations. HAMPT integrates a heterogeneous attention mechanism that dynamically weights the importance of different node and edge types during message passing, facilitating information propagation across diverse relationships. Furthermore, HAMPT leverages Graph Transformer layers to capture long-range dependencies within the graph, enabling the model to attend to distant nodes relevant to the task. We evaluate HAMPT on several benchmark datasets for node classification and link prediction in heterogeneous graphs, demonstrating its superior performance compared to state-of-the-art GNN architectures. Specifically, HAMPT achieves significant improvements in node classification accuracy on the OGB-MAG dataset (an increase of 3.2%) and link prediction accuracy on the IMDB dataset (an increase of 4.5%), highlighting its ability to effectively model complex relationships in heterogeneous graph data. These results establish HAMPT as a promising approach for representation learning on diverse graph structures.",ai
"Optimal AP clustering and power allocation are critical in user-centric cell-free massive MIMO systems. Existing deep learning models lack flexibility to handle dynamic network configurations. Furthermore, many approaches overlook pilot contamination and suffer from high computational complexity. In this paper, we propose a lightweight transformer model that overcomes these limitations by jointly predicting AP clusters and powers solely from spatial coordinates of user devices and AP. Our model is architecture-agnostic to users load, handles both clustering and power allocation without channel estimation overhead, and eliminates pilot contamination by assigning users to AP within a pilot reuse constraint. We also incorporate a customized linear attention mechanism to capture user-AP interactions efficiently and enable linear scalability with respect to the number of users. Numerical results confirm the model's effectiveness in maximizing the minimum spectral efficiency and providing near-optimal performance while ensuring adaptability and scalability in dynamic scenarios.",human
"Over the past decade, a wide range of motion planning approaches for autonomous vehicles has been developed to handle increasingly complex traffic scenarios. However, these approaches are rarely compared on standardized benchmarks, limiting the assessment of relative strengths and weaknesses. To address this gap, we present the setup and results of the 4th CommonRoad Motion Planning Competition held in 2024, conducted using the CommonRoad benchmark suite. This annual competition provides an open-source and reproducible framework for benchmarking motion planning algorithms. The benchmark scenarios span highway and urban environments with diverse traffic participants, including passenger cars, buses, and bicycles. Planner performance is evaluated along four dimensions: efficiency, safety, comfort, and compliance with selected traffic rules. This report introduces the competition format and provides a comparison of representative high-performing planners from the 2023 and 2024 editions.",human
"Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.",human
"We investigate the challenge of representing complex relational knowledge for machine reasoning, specifically focusing on scenarios where the underlying structure is incomplete or noisy. Current knowledge representation methods often struggle to effectively capture both explicit and implicit relationships, leading to suboptimal performance in downstream tasks such as link prediction and reasoning over knowledge graphs. We propose a novel approach, Relation-Augmented Graph Embedding (RAGE), which enhances existing graph embedding techniques by explicitly incorporating relation-specific information during the embedding learning process. RAGE utilizes a multi-head attention mechanism to dynamically weigh the contributions of neighboring entities based on the specific relation being considered. This allows the model to capture subtle nuances in the relationships between entities that are often overlooked by standard graph embedding methods. We evaluate RAGE on several benchmark knowledge graph completion datasets, including WN18RR and FB15k-237. Our results demonstrate that RAGE consistently outperforms state-of-the-art baselines, achieving significant improvements in both link prediction accuracy (Hits@1 and MRR) and reasoning performance. Furthermore, we analyze the learned attention weights to provide insights into the model's ability to capture relevant relational information. These findings suggest that RAGE provides a more robust and expressive representation of relational knowledge, leading to improved performance in various downstream tasks.",ai
"Handwritten text recognition (HTR) and machine translation continue to pose significant challenges, particularly for low-resource languages like Marathi, which lack large digitized corpora and exhibit high variability in handwriting styles. The conventional approach to address this involves a two-stage pipeline: an OCR system extracts text from handwritten images, which is then translated into the target language using a machine translation model. In this work, we explore and compare the performance of traditional OCR-MT pipelines with Vision Large Language Models that aim to unify these stages and directly translate handwritten text images in a single, end-to-end step. Our motivation is grounded in the urgent need for scalable, accurate translation systems to digitize legal records such as FIRs, charge sheets, and witness statements in India's district and high courts. We evaluate both approaches on a curated dataset of handwritten Marathi legal documents, with the goal of enabling efficient legal document processing, even in low-resource environments. Our findings offer actionable insights toward building robust, edge-deployable solutions that enhance access to legal information for non-native speakers and legal professionals alike.",human
"We address the challenge of identifying causal relationships in natural language processing, specifically focusing on the problem of spurious correlations learned from observational data. While correlational models excel at prediction, they often fail to generalize to interventional settings due to their inability to distinguish correlation from causation. We propose a novel framework, Causal Language Modeling with Instrumental Variables (CLM-IV), which leverages instrumental variables derived from contextual cues to estimate direct causal effects between language features and downstream tasks. CLM-IV incorporates a two-stage least squares (2SLS) approach within a pre-trained language model architecture. In the first stage, we predict the endogenous language feature (e.g., presence of a specific word) using an instrumental variable (e.g., document source). In the second stage, we predict the outcome variable (e.g., sentiment) using the predicted value of the language feature from the first stage. Empirical evaluations on sentiment analysis and toxicity detection benchmarks demonstrate that CLM-IV consistently outperforms correlational baselines in out-of-distribution settings and under simulated interventions. Specifically, CLM-IV exhibits improved robustness against confounding biases present in the training data, achieving a relative performance gain of up to 15% compared to standard fine-tuning methods. Our findings suggest that incorporating causal inference techniques can significantly enhance the reliability and generalizability of language models.",ai
"We address the challenge of efficient exploration in sparse-reward reinforcement learning environments. Traditional methods often struggle in such scenarios due to the difficulty of discovering rewarding state-action pairs. This work introduces a novel intrinsic motivation framework, termed ""Curiosity-Driven Policy Gradients (CDPG),"" which integrates a learned state-action density model directly into the policy optimization process. Specifically, we employ a variational autoencoder (VAE) to estimate the density of encountered state-action pairs and utilize the negative log-likelihood of this density as an intrinsic reward signal. This signal encourages the agent to explore less-visited regions of the state-action space. Crucially, CDPG incorporates this intrinsic reward within the policy gradient update, allowing for direct optimization of exploration behavior alongside task-specific goals. We evaluate CDPG on a suite of challenging sparse-reward environments, demonstrating significant performance improvements compared to standard policy gradient methods and existing intrinsic motivation techniques. Our results indicate that CDPG achieves faster learning rates and higher asymptotic performance, effectively mitigating the exploration problem in sparse-reward settings by promoting directed exploration of novel state-action combinations.",ai
"Knowledge representation is a fundamental challenge in statistical machine learning, particularly when dealing with complex, structured data exhibiting relational dependencies and hierarchical organization. This paper addresses the problem of learning distributed representations for entities and relations within a knowledge graph, aiming to capture both semantic similarity and structural context. We propose a novel embedding-based approach, Hierarchical Relational Graph Convolutional Networks (HRGCN), which extends traditional Graph Convolutional Networks (GCNs) by incorporating hierarchical attention mechanisms. These mechanisms allow the model to selectively aggregate information from different levels of the graph hierarchy, effectively capturing both local relational patterns and global contextual dependencies. HRGCN learns entity and relation embeddings by optimizing a margin-based ranking loss on observed and negative triples within the knowledge graph. We evaluate our approach on several benchmark knowledge graph completion tasks, including link prediction and triple classification, using datasets such as WordNet and Freebase. Experimental results demonstrate that HRGCN consistently outperforms existing state-of-the-art methods, achieving significant improvements in mean rank and Hits@k metrics. These findings suggest that hierarchical attention is crucial for effectively capturing the rich semantic and structural information encoded within knowledge graphs, leading to more accurate and robust knowledge representation.",ai
"Physics-Informed Neural Networks (PINN) are emerging as a promising approach for quantitative parameter estimation of Magnetic Resonance Imaging (MRI). While existing deep learning methods can provide an accurate quantitative estimation of the T2 parameter, they still require large amounts of training data and lack theoretical support and a recognized gold standard. Thus, given the absence of PINN-based approaches for T2 estimation, we propose embedding the fundamental physics of MRI, the Bloch equation, in the loss of PINN, which is solely based on target scan data and does not require a pre-defined training database. Furthermore, by deriving rigorous upper bounds for both the T2 estimation error and the generalization error of the Bloch equation solution, we establish a theoretical foundation for evaluating the PINN's quantitative accuracy. Even without access to the ground truth or a gold standard, this theory enables us to estimate the error with respect to the real quantitative parameter T2. The accuracy of T2 mapping and the validity of the theoretical analysis are demonstrated on a numerical cardiac model and a water phantom, where our method exhibits excellent quantitative precision in the myocardial T2 range. Clinical applicability is confirmed in 94 acute myocardial infarction (AMI) patients, achieving low-error quantitative T2 estimation under the theoretical error bound, highlighting the robustness and potential of PINN.",human
"In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and learn to encode the meaning of the sequence into embeddings such that those embeddings can be used reliably for NLI tasks. Essentially, every word is considered against every other word in the sequence, and the transformer model is able to determine the relationships between them, entirely from scratch. However, a model that accepts explicit linguistic structures like dependency parse trees may be able to leverage prior encoded information about these relationships, without having to learn them from scratch, thus improving learning efficiency. To investigate this, we adapt Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). We compare TMN to a BERT based model on the SNLI entailment task and on the SemEval similarity task. TMN is able to achieve significantly better results with a significantly reduced memory footprint and much less training time than the BERT based model on the SNLI task, while both models struggled to preform well on the SemEval. Explicit structural representations significantly outperform sequence-based models at comparable scales, but current aggregation methods limit scalability. We propose multi-headed attention aggregation to address this limitation.",human
"Applying LLMs to predictive tasks in finance is challenging due to look-ahead bias resulting from their training on long time-series data. This precludes the backtests typically employed in finance since retraining frontier models from scratch with a specific knowledge cutoff is prohibitive. In this paper, we introduce a fast, effective, and low-cost alternative. Our method guides generation at inference time by adjusting the logits of a large base model using a pair of smaller, specialized models -- one fine-tuned on information to be forgotten and another on information to be retained. We demonstrate that our method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior methods.",human
"Near an optimal learning point of a neural network, the learning performance of gradient descent dynamics is dictated by the Hessian matrix of the loss function with respect to the network parameters. We characterize the Hessian eigenspectrum for some classes of teacher-student problems, when the teacher and student networks have matching weights, showing that the smaller eigenvalues of the Hessian determine long-time learning performance. For linear networks, we analytically establish that for large networks the spectrum asymptotically follows a convolution of a scaled chi-square distribution with a scaled Marchenko-Pastur distribution. We numerically analyse the Hessian spectrum for polynomial and other non-linear networks. Furthermore, we show that the rank of the Hessian matrix can be seen as an effective number of parameters for networks using polynomial activation functions. For a generic non-linear activation function, such as the error function, we empirically observe that the Hessian matrix is always full rank.",human
"Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions. This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance. Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.",human
"We address the problem of automatically proving theorems in first-order logic with equality, focusing on improving the efficiency of saturation-based theorem provers when dealing with large and complex problems. Many practical applications generate proof obligations with numerous redundant or irrelevant clauses, significantly hindering the performance of automated reasoning systems. We introduce a novel clause weighting heuristic, *Structural Rareness Prioritization* (SRP), which prioritizes clauses containing terms that are structurally rare within the clause set. SRP leverages term structure frequency as an indicator of potential relevance, hypothesizing that clauses with rare terms are more likely to contribute to the derivation of a refutation. We implement SRP within the Vampire theorem prover and evaluate its performance on a diverse benchmark suite, including problems from TPTP and SMT-LIB. Empirical results demonstrate that SRP significantly reduces the search space explored by Vampire, leading to a substantial increase in the number of problems solved within a given time limit compared to Vampire's default configuration and several state-of-the-art clause weighting heuristics. The results suggest that structural information, readily available during proof search, can be effectively utilized to guide clause prioritization and improve the overall efficiency of automated theorem provers.",ai
"Current approaches to AI coding agents appear to blur the lines between the Large Language Model (LLM) and the agent itself, asking the LLM to make decisions best left to deterministic processes. This leads to systems prone to stochastic failures such as gaming unit tests or hallucinating syntax. Drawing on established software engineering practices that provide deterministic frameworks for managing unpredictable processes, this paper proposes setting the control boundary such that the LLM is treated as a component of the environment environment -- preserving its creative stochasticity -- rather than the decision-making agent. A is formalized, separating workflow state (deterministic control flow) from environment state (stochastic generation). couple generation with verification as indivisible transactions, where act as sensing actions that project probabilistic outputs onto observable workflow state. The framework is validated on three code generation tasks across 13 LLMs (1.3B--15B parameters). For qualified instruction-following models, task success rates improved by up to 66 percentage points at 1.2--2.1 baseline computational cost. The results suggest that architectural constraints can substitute for parameter scale in achieving reliable code generation.",human
"Graph Neural Networks (GNNs) have become the standard for graph representation learning but remain vulnerable to structural perturbations. We propose a novel framework that integrates persistent homology features with stability regularization to enhance robustness. Building on the stability theorems of persistent homology , our method combines GIN architectures with multi-scale topological features extracted from persistence images, enforced by Hiraoka-Kusano-inspired stability constraints. Across six diverse datasets spanning biochemical, social, and collaboration networks , our approach demonstrates exceptional robustness to edge perturbations while maintaining competitive accuracy. Notably, we observe minimal performance degradation (0-4\% on most datasets) under perturbation, significantly outperforming baseline stability. Our work provides both a theoretically-grounded and empirically-validated approach to robust graph learning that aligns with recent advances in topological regularization",human
"We address the challenge of representing compositional knowledge for complex relational reasoning tasks, specifically focusing on scenarios where explicit symbolic representations are advantageous for interpretability and explainability. Existing neural-symbolic approaches often struggle to effectively integrate learned embeddings with structured knowledge bases, leading to suboptimal performance and limited generalization. We introduce a novel framework, the Differentiable Knowledge Graph Translator (DKGT), which learns to dynamically translate natural language queries and context into executable graph traversal operations on a knowledge graph. DKGT utilizes a transformer-based architecture to encode the input and generate a sequence of differentiable operations, including node selection, relation selection, and aggregation functions. These operations are applied to the knowledge graph, resulting in a predicted answer. The model is trained end-to-end using a combination of supervised learning and reinforcement learning techniques, encouraging exploration of relevant graph paths and optimization of reasoning accuracy. Empirical evaluation on benchmark datasets demonstrates that DKGT achieves significant improvements in accuracy and efficiency compared to existing state-of-the-art methods, particularly in tasks requiring multi-hop reasoning and compositional understanding. Furthermore, the differentiable nature of DKGT allows for introspection of the reasoning process, providing insights into the model's decision-making and facilitating knowledge graph error detection.",ai
"### Abstract We address the challenge of effectively representing and reasoning with hierarchical and compositional knowledge, a persistent bottleneck in achieving robust and generalizable knowledge-based systems. Existing knowledge representation formalisms often struggle to simultaneously capture both the hierarchical relationships between concepts and the compositional structure of complex objects and events. This limitation hinders the ability of systems to efficiently infer new knowledge, handle incomplete information, and adapt to evolving environments. We propose a novel hybrid knowledge representation framework that integrates Description Logic (DL) ontologies for hierarchical reasoning with a tensor-based compositional model for representing object and event structures. The DL component provides a formal semantics for defining concepts and their relationships, enabling sound inference and consistency checking. The tensor-based model allows for encoding the compositional structure of complex entities through multi-linear algebraic operations, facilitating reasoning about part-whole relationships and attribute combinations. We present a formalization of this integrated framework and demonstrate its efficacy on a benchmark knowledge base containing hierarchical and compositional data related to biological pathways. Experimental results show that our approach achieves significantly improved performance in tasks such as knowledge completion and similarity-based reasoning compared to using either DL or tensor-based models alone. The hybrid approach allows the system to leverage the strengths of both formalisms, leading to more accurate and efficient reasoning capabilities.",ai
"Recent advancements in natural language processing have been significantly driven by transformer-based architectures. However, challenges persist in efficiently handling long sequences and capturing intricate contextual relationships, particularly in tasks demanding global understanding and nuanced reasoning. This paper introduces a novel attention mechanism, termed ""Sparse Sinkhorn Attention"" (SSA), designed to mitigate these limitations. SSA leverages the Sinkhorn algorithm to induce sparsity within the attention matrix, concentrating computational resources on the most salient contextual dependencies. By approximating the optimal transport plan between query and key representations, SSA identifies and prioritizes the most relevant information, thereby reducing the quadratic complexity of standard attention mechanisms. We evaluate SSA on a suite of benchmark datasets, including long-document classification (e.g., Longformer encoder-decoder (LED)) and abstractive summarization (e.g., PubMed). Empirical results demonstrate that SSA consistently outperforms vanilla attention and existing sparse attention variants in terms of both computational efficiency and model accuracy. Specifically, we observe a 15% reduction in computational cost for similar performance on long-document classification tasks. Further, application to abstractive summarization yields a relative ROUGE-1 score improvement of 2.3 points over baseline models. The findings suggest that SSA provides a promising avenue for scaling transformer models to tackle complex NLP tasks requiring efficient processing of extended textual contexts.",ai
"Reinforcement learning (RL) agents often struggle with sparse reward environments, hindering effective exploration and learning. This work addresses the challenge of training RL agents in such scenarios by proposing a novel exploration strategy predicated on intrinsic motivation derived from unsupervised semantic change detection in the agent's observation space. Specifically, we introduce a variational autoencoder (VAE) trained on the agent's past experiences to learn a latent representation of the environment. High-dimensional state changes are then projected into this latent space, allowing for efficient computation of a change-based intrinsic reward signal proportional to the reconstruction error of consecutive latent states. This encourages the agent to actively seek out novel and informative states, thereby facilitating exploration. We evaluate our method on a suite of challenging sparse reward tasks, including Montezuma's Revenge and several Mujoco locomotion environments. Results demonstrate a significant improvement in sample efficiency and overall performance compared to traditional exploration methods, such as epsilon-greedy and random network distillation (RND). Furthermore, ablation studies confirm the importance of the VAE's learned representation in effectively capturing meaningful environmental changes. Our approach provides a robust and scalable solution for tackling exploration challenges in sparse reward RL problems.",ai
"We address the problem of training machine learning models on sensitive data while ensuring stringent privacy guarantees. Specifically, we focus on preserving differential privacy in federated learning scenarios with non-IID data distributions, which exacerbates privacy risks. Our approach introduces a novel adaptive clipping mechanism coupled with a dynamic noise allocation strategy. The clipping norm is adaptively adjusted based on the observed gradient variance within each federated round, mitigating the impact of outliers and reducing bias. Furthermore, the noise allocation strategy dynamically distributes the privacy budget across rounds, prioritizing rounds with higher gradient sensitivity. This allows for a more efficient utilization of the overall privacy budget. We empirically evaluate our method on several benchmark datasets for image classification and natural language processing, demonstrating significant improvements in model accuracy compared to existing state-of-the-art differentially private federated learning algorithms. Specifically, our method achieves up to 10% higher accuracy than comparable approaches while maintaining similar levels of privacy, as measured by -differential privacy. The results highlight the effectiveness of adaptive clipping and dynamic noise allocation in training differentially private models in heterogeneous federated settings.",ai
"We investigate the problem of identifying causal effects in the presence of unobserved confounding and selection bias, a persistent challenge in observational studies. Specifically, we consider scenarios where selection bias is induced by conditioning on a collider affected by both the treatment and the outcome, and where unobserved confounders influence both the treatment and the outcome. To address this, we propose a novel instrumental variable (IV) method leveraging proxy variables for both the unobserved confounders and the selection variable. Our method, termed Proxy-IV Selection Correction (PISC), utilizes a two-stage least squares framework adapted for this specific causal structure. First, we estimate the effect of the treatment on the outcome using the proxy variables as instruments within a structural equation model that explicitly incorporates the selection bias. Second, we correct for this bias using a derived functional form based on the estimated parameters and the proxy for the selection variable. We demonstrate the efficacy of PISC through simulations across a range of causal graphs and parameter settings. Our results indicate that PISC provides significantly less biased and more accurate estimates of the causal effect compared to standard IV approaches and methods that ignore selection bias, particularly when the proxies exhibit moderate to strong correlations with the unobserved variables. We further illustrate the practical applicability of PISC with a real-world dataset examining the effect of job training programs on employment outcomes, subject to potential selection bias in program enrollment.",ai
"Demonstrating quantum advantage in machine learning tasks requires navigating a complex landscape of proposed models and algorithms. To bring clarity to this search, we introduce a framework that connects the structure of parametrized quantum circuits to the mathematical nature of the functions they can actually learn. Within this framework, we show how fundamental properties, like circuit depth and non-Clifford gate count, directly determine whether a model's output leads to efficient classical simulation or surrogation. We argue that this analysis uncovers common pathways to dequantization that underlie many existing simulation methods. More importantly, it reveals critical distinctions between models that are fully simulatable, those whose function space is classically tractable, and those that remain robustly quantum. This perspective provides a conceptual map of this landscape, clarifying how different models relate to classical simulability and pointing to where opportunities for quantum advantage may lie.",human
"Deep learning-based object detection models play a critical role in real-world applications such as autonomous driving and security surveillance systems, yet they remain vulnerable to adversarial examples. In this work, we propose an autoencoder-based denoising defense to recover object detection performance degraded by adversarial perturbations. We conduct adversarial attacks using Perlin noise on vehicle-related images from the COCO dataset, apply a single-layer convolutional autoencoder to remove the perturbations, and evaluate detection performance using YOLOv5. Our experiments demonstrate that adversarial attacks reduce bbox mAP from 0.2890 to 0.1640, representing a 43.3% performance degradation. After applying the proposed autoencoder defense, bbox mAP improves to 0.1700 (3.7% recovery) and bbox mAP@50 increases from 0.2780 to 0.3080 (10.8% improvement). These results indicate that autoencoder-based denoising can provide partial defense against adversarial attacks without requiring model retraining.",human
"We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model's embedding space and construct an explanation of the model's prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.",human
"Cultural rights and the right to development are essential norms within the wider framework of international human rights law. However, recent technological advances in artificial intelligence (AI) and adjacent digital frontier technologies pose significant challenges to the protection and realization of these rights. This owes to the increasing influence of AI systems on the creation and depiction of cultural content, affect the use and distribution of the intellectual property of individuals and communities, and influence cultural participation and expression worldwide. In addition, the growing influence of AI thus risks exacerbating preexisting economic, social and digital divides and reinforcing inequities for marginalized communities. This dynamic challenges the existing interplay between cultural rights and the right to development, and raises questions about the integration of cultural and developmental considerations into emerging AI governance frameworks. To address these challenges, the paper examines the impact of AI on both categories of rights. Conceptually, it analyzes the epistemic and normative limitations of AI with respect to cultural and developmental assumptions embedded in algorithmic design and deployment, but also individual and structural impacts of AI on both rights. On this basis, the paper identifies gaps and tensions in existing AI governance frameworks with respect to cultural rights and the right to development. By situating cultural rights and the right to development within the broader landscape of AI and human rights, this paper contributes to the academic discourse on AI ethics, legal frameworks, and international human rights law. Finally, it outlines avenues for future research and policy development based on existing conversations in global AI governance.",human
"Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.",human
"We investigate the challenge of augmenting statistical machine learning models with automated reasoning capabilities to address complex inference problems. Existing approaches often struggle with integrating symbolic reasoning with the inherent uncertainty and inductive biases of statistical models. This work introduces a novel framework, Neural-Symbolic Reasoning via Probabilistic Logic Programs (NSR-PLP), which leverages probabilistic logic programming as an intermediate representation to facilitate bidirectional communication between a neural network and a symbolic reasoning engine. The neural network learns to map raw sensory data to probabilistic facts represented in a logical form. These facts are then used by a probabilistic logic program to perform reasoning and derive conclusions. Crucially, the reasoning process provides feedback to the neural network, allowing it to refine its representations based on the logical consequences of its outputs. We evaluate NSR-PLP on a synthetic visual question answering task and a real-world knowledge graph completion task. Our results demonstrate that NSR-PLP significantly outperforms both purely statistical and purely symbolic approaches, achieving improved accuracy and robustness. The framework exhibits superior generalization capabilities to novel scenarios by effectively combining the strengths of statistical learning and symbolic reasoning.",ai
"We address the problem of spurious correlations in neural text generation, wherein models learn to generate text based on dataset biases rather than underlying semantic relationships. This often manifests as unintended correlations between input features and output text, leading to unreliable and brittle systems. To mitigate this, we propose a novel intervention-based causal inference framework for training neural text generation models. Our method, Contrastive Masked Causal Language Modeling (CM-CaLM), explicitly models the causal relationship between masked input features and generated output. We achieve this by training the model to predict the original text, given the masked input, and contrast it with counterfactual texts generated by intervening on the masked features. Specifically, we utilize a conditional variational autoencoder to generate diverse counterfactual texts under different interventions, thereby forcing the model to learn invariant representations of the semantic content. Empirical evaluations on a diverse set of text generation tasks, including question answering and style transfer, demonstrate that CM-CaLM significantly reduces spurious correlations and improves generalization performance compared to standard training techniques and existing de-biasing methods. We observe improvements in both out-of-distribution performance and robustness to adversarial attacks designed to exploit spurious correlations. Further analysis reveals that CM-CaLM learns more semantically coherent representations, leading to more reliable and controllable text generation.",ai
"We address the challenge of effectively representing and reasoning with complex relational knowledge in dynamic environments. Current knowledge representation formalisms often struggle to maintain coherence and tractability when faced with evolving information and intricate dependencies. Our approach introduces a novel hybrid knowledge graph embedding model, termed Relational Contextualized Embedding Network (RCEN), which combines the strengths of symbolic and sub-symbolic representation. RCEN leverages a differentiable reasoning module embedded within a graph neural network architecture to propagate contextual information through the knowledge graph. This allows for adaptive weighting of relational dependencies based on both structural properties and node-specific attributes. The model is trained on dynamically updated knowledge graphs using a contrastive learning objective that encourages consistent embeddings for logically equivalent statements across different graph states. Empirical evaluation on benchmark knowledge graph completion tasks and a novel temporal reasoning dataset demonstrates that RCEN significantly outperforms existing embedding-based and symbolic reasoning methods, achieving improved accuracy and robustness to noisy updates. Furthermore, ablation studies validate the importance of the contextualized relational weighting mechanism in enhancing reasoning performance. The proposed framework offers a promising avenue for building robust and adaptable knowledge representation systems capable of handling the complexities of real-world dynamic knowledge.",ai
"**Reinforcement Learning with Curriculum-Guided Exploration for Enhanced Language Grounding** Language grounding, the process of associating linguistic expressions with perceptual experiences, remains a challenging problem in reinforcement learning (RL) due to sparse rewards and complex state spaces. Traditional RL methods often struggle to efficiently explore the environment and learn meaningful mappings between language and action sequences. This work addresses this limitation by introducing a novel curriculum-guided exploration strategy that leverages both intrinsic and extrinsic rewards to facilitate learning. Specifically, we propose a curriculum constructed based on the hierarchical complexity of linguistic commands, starting with simpler instructions and progressively increasing the difficulty. An intrinsic motivation module, driven by prediction error minimization, encourages exploration of novel states and actions within the current curriculum stage. We evaluate our approach on a simulated navigation task where an agent must follow natural language instructions to reach a target location. Experimental results demonstrate that our curriculum-guided exploration strategy significantly outperforms existing RL baselines, including those employing imitation learning and auxiliary tasks. The proposed method achieves higher success rates and faster convergence, indicating improved sample efficiency and generalization capabilities in complex language grounding scenarios. Further analysis reveals that the curriculum effectively shapes the agent's exploration, enabling it to discover more informative state-action trajectories and develop robust language understanding skills.",ai
"Traditional feature engineering approaches for molecular sequence classification suffer from sparsity issues and computational complexity, while deep learning models often underperform on tabular biological data. This paper introduces a novel topological approach that transforms molecular sequences into images by combining Chaos Game Representation (CGR) with Rips complex construction from algebraic topology. Our method maps sequence elements to 2D coordinates via CGR, computes pairwise distances, and constructs Rips complexes to capture both local structural and global topological features. We provide formal guarantees on representation uniqueness, topological stability, and information preservation. Extensive experiments on anticancer peptide datasets demonstrate superior performance over vector-based, sequence language models, and existing image-based methods, achieving 86.8\% and 94.5\% accuracy on breast and lung cancer datasets, respectively. The topological representation preserves critical sequence information while enabling effective utilization of vision-based deep learning architectures for molecular sequence analysis.",human
"The search for reliable indicators of consciousness has fragmented into competing theoretical camps (Global Workspace Theory (GWT), Integrated Information Theory (IIT), and Higher-Order Theories (HOT)), each proposing distinct neural signatures. We adopt a synthetic neuro-phenomenology approach: constructing artificial agents that embody these mechanisms to test their functional consequences through precise architectural ablations impossible in biological systems. Across three experiments, we report dissociations suggesting these theories describe complementary functional layers rather than competing accounts. In Experiment 1, a no-rewire Self-Model lesion abolishes metacognitive calibration while preserving first-order task performance, yielding a synthetic blindsight analogue consistent with HOT predictions. In Experiment 2, workspace capacity proves causally necessary for information access: a complete workspace lesion produces qualitative collapse in access-related markers, while partial reductions show graded degradation, consistent with GWT's ignition framework. In Experiment 3, we uncover a broadcast-amplification effect: GWT-style broadcasting amplifies internal noise, creating extreme fragility. The B2 agent family is robust to the same latent perturbation; this robustness persists in a Self-Model-off / workspace-read control, cautioning against attributing the effect solely to compression. We also report an explicit negative result: raw perturbational complexity (PCI-A) decreases under the workspace bottleneck, cautioning against naive transfer of IIT-adjacent proxies to engineered agents. These results suggest a hierarchical design principle: GWT provides broadcast capacity, while HOT provides quality control. We emphasize that our agents are not conscious; they are reference implementations for testing functional predictions of consciousness theories.",human
"We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top- adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top- enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.",human
"Multi-agent Large Language Model (LLM) systems face a critical bottleneck: redundant transmission of contextual information between agents consumes excessive bandwidth and computational resources. Traditional approaches discard internal semantic representations and transmit raw text, forcing receiving agents to recompute similar representations from scratch. We introduce Q-KVComm, a new protocol that enables direct transmission of compressed key-value (KV) cache representations between LLM agents. Q-KVComm combines three key innovations: (1) adaptive layer-wise quantization that allocates variable bit-widths based on sensitivity profiling, (2) hybrid information extraction that preserves critical facts across content domains, and (3) heterogeneous model calibration establishing cross-architecture communication. Extensive experiments across three diverse question-answering datasets demonstrate that Q-KVComm achieves 5-6x compression ratios while maintaining semantic fidelity, with coherence quality scores above 0.77 across all scenarios. The protocol exhibits robust performance across model sizes (1.1B-1.5B parameters) and adapts to real-world applications including conversational QA and multi-hop reasoning. Our work establishes a new paradigm for LLM agent communication, shifting from text-based to representation-based information exchange.",human
"Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.",human
"Graph Neural Networks (GNNs) have demonstrated significant potential in representation learning for graph-structured data. However, existing GNN architectures often struggle with over-smoothing, where node representations converge to similar values across the graph, hindering downstream task performance, particularly in deep GNNs. This work addresses the over-smoothing problem by introducing a novel adaptive layer aggregation strategy based on spectral graph analysis. Specifically, we derive a spectral filter bank that captures different frequency components of the graph signal. Each GNN layer learns adaptive weights to selectively aggregate representations from different layers, guided by the spectral filter bank. This allows the network to learn node representations that balance local and global information, mitigating over-smoothing. We evaluate our approach on benchmark node classification and graph classification tasks across various datasets, including citation networks and social networks. Results demonstrate that our method consistently outperforms state-of-the-art GNN architectures in terms of accuracy and robustness to over-smoothing, achieving significant improvements in performance, especially for deeper networks. Ablation studies validate the effectiveness of the adaptive layer aggregation mechanism and the learned spectral filter bank. The proposed method provides a promising avenue for designing more effective and scalable GNN architectures for complex graph data.",ai
"Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations of data structured as graphs. However, their performance is often limited by issues such as over-smoothing and the inability to effectively capture long-range dependencies within large, complex graphs. We address these limitations by introducing a novel Graph Transformer architecture incorporating learnable structural encodings and adaptive message passing mechanisms. The structural encodings, based on a spectral decomposition of the graph Laplacian, provide GNNs with explicit positional information regarding node relationships beyond immediate neighbors. Furthermore, our adaptive message passing dynamically adjusts the aggregation weights based on the relevance of neighboring nodes, mitigating the over-smoothing problem and improving the network's ability to capture long-range dependencies. We evaluate our model on a range of benchmark graph classification and node classification datasets, including ogbn-arxiv and Cora. Our experimental results demonstrate significant improvements over existing GNN architectures, achieving state-of-the-art performance on several datasets. Specifically, we observe a 5-10% improvement in classification accuracy on datasets characterized by large graph sizes and complex connectivity patterns, validating the effectiveness of our proposed structural encodings and adaptive message passing mechanisms.",ai
"Electrophysiology signals such as EEG and iEEG are central to neuroscience, brain-computer interfaces, and clinical applications, yet existing foundation models remain limited in scale despite clear evidence that scaling improves performance. We introduce DIVER-1, a family of EEG and iEEG foundation models trained on the largest and most diverse corpus to date-5.3k hours of iEEG and 54k hours of EEG (1.6M channel-hours from over 17.7k subjects)-and scaled up to 1.82B parameters. We present the first systematic scaling law analysis for this domain, showing that they follow data-constrained scaling laws: for a given amount of data and compute, smaller models trained for extended epochs consistently outperform larger models trained briefly. This behavior contrasts with prior electrophysiology foundation models that emphasized model size over training duration. To achieve strong performance, we also design architectural innovations including any-variate attention, sliding temporal conditional positional encoding, and multi-domain reconstruction. DIVER-1 iEEG and EEG models each achieve state-of-the-art performance on their respective benchmarks, establishing a concrete guidelines for efficient scaling and resource allocation in electrophysiology foundation model development.",human
"Artificial intelligence techniques have achieved strong performance in classifying Windows Portable Executable (PE) malware, but their reliability often degrades under dataset shifts, leading to misclassifications with severe security consequences. To address this, we enhance an existing LightGBM (LGBM) malware detector by integrating Neural Networks (NN), PriorNet, and Neural Network Ensembles, evaluated across three benchmark datasets: EMBER, BODMAS, and UCSB. The UCSB dataset, composed mainly of packed malware, introduces a substantial distributional shift relative to EMBER and BODMAS, making it a challenging testbed for robustness. We study uncertainty-aware decision strategies, including probability thresholding, PriorNet, ensemble-derived estimates, and Inductive Conformal Evaluation (ICE). Our main contribution is the use of ensemble-based uncertainty estimates as Non-Conformity Measures within ICE, combined with a novel threshold optimisation method. On the UCSB dataset, where the shift is most severe, the state-of-the-art probability-based ICE (SOTA) yields an incorrect acceptance rate (IA%) of 22.8%. In contrast, our method reduces this to 16% a relative reduction of about 30% while maintaining competitive correct acceptance rates (CA%). These results demonstrate that integrating ensemble-based uncertainty with conformal prediction provides a more reliable safeguard against misclassifications under extreme dataset shifts, particularly in the presence of packed malware, thereby offering practical benefits for real-world security operations.",human
"**Abstract:** Automated analysis of medical images via computer vision techniques holds significant promise for improved diagnostic accuracy and efficiency. However, existing methods often struggle with the inherent complexity and variability present in medical image data, particularly in scenarios involving subtle pathological changes. This work addresses the challenge of robust and accurate lesion detection in Magnetic Resonance Imaging (MRI) of the brain using a novel multi-scale convolutional neural network (CNN) architecture. The proposed method, termed MultiRes-CNN, incorporates residual connections within multiple parallel convolutional streams, each operating at a different spatial resolution. This allows the network to simultaneously capture both fine-grained details and global contextual information, thereby enhancing its sensitivity to subtle lesions. We evaluated MultiRes-CNN on a publicly available dataset of brain MRI scans annotated with ground truth lesion masks. Experimental results demonstrate that our approach achieves a statistically significant improvement in lesion detection performance, as measured by the Dice coefficient (mean Dice = 0.78), compared to several state-of-the-art CNN-based methods, including U-Net and V-Net (p < 0.05). Furthermore, qualitative analysis reveals improved segmentation accuracy, particularly for lesions with indistinct boundaries and complex shapes. These findings suggest that MultiRes-CNN offers a promising avenue for advancing automated medical image analysis and improving clinical workflows.",ai
"A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.",human
"We address the challenge of robust optimization under distributional uncertainty, focusing on scenarios where the underlying probability distribution governing data generation is imprecisely known. Specifically, we consider ambiguity sets defined by Wasserstein distance to a nominal distribution, which allows for capturing deviations in both the mean and covariance structure. Traditional robust optimization approaches often lead to overly conservative solutions, especially in high-dimensional settings. To mitigate this, we propose a novel sample-average approximation (SAA) scheme coupled with a data-driven ambiguity set calibration technique. Our method leverages empirical risk minimization to estimate the optimal radius of the Wasserstein ball, balancing robustness against distributional shift with sensitivity to true underlying patterns. We derive finite-sample performance guarantees, establishing convergence rates for the SAA problem and providing bounds on the out-of-sample performance of the resulting robust solution. Empirical results on synthetic and real-world datasets demonstrate that our approach achieves superior performance compared to standard robust optimization techniques, offering a more efficient trade-off between robustness and nominal performance while maintaining computational tractability. These findings highlight the effectiveness of data-driven ambiguity set calibration in enhancing the practical applicability of robust optimization methodologies.",ai
"We address the challenge of integrating heterogeneous knowledge representations to enhance statistical machine learning. Specifically, we investigate the problem of combining symbolic knowledge, expressed as logical rules, with continuous-valued embeddings learned from data. Existing approaches often treat these representations separately, leading to suboptimal performance in tasks requiring both reasoning and pattern recognition. Our method, Knowledge-Augmented Embedding Refinement (KAER), proposes an iterative framework. First, we learn initial embeddings of entities and relations from observational data using a standard neural network architecture. Subsequently, we leverage logical rules to refine these embeddings by enforcing constraints derived from the symbolic knowledge. This refinement is achieved by minimizing a novel loss function that penalizes violations of the logical constraints within the embedding space. Finally, the refined embeddings are used as features in downstream machine learning tasks. We evaluate KAER on knowledge graph completion and link prediction tasks using established benchmark datasets. Empirical results demonstrate that KAER significantly outperforms baseline methods that rely solely on either symbolic knowledge or embedding learning. We further show that the refined embeddings exhibit improved interpretability, reflecting the logical structure encoded in the knowledge base. These findings highlight the potential of KAER for effectively bridging the gap between symbolic and subsymbolic knowledge representation in statistical machine learning.",ai
"We investigate the problem of representing and reasoning with complex relational knowledge in contexts requiring high fidelity and computational efficiency. Existing knowledge representation formalisms often struggle to balance expressiveness, scalability, and inferential power, particularly when dealing with non-monotonic relationships and nuanced contextual dependencies. To address this challenge, we introduce a novel hybrid representation, termed Contextualized Relational Knowledge Graphs (CRKG), which integrates symbolic knowledge graphs with distributional semantic information derived from large-scale text corpora. CRKGs represent entities and relations as nodes and edges, respectively, while leveraging contextualized embeddings to capture semantic nuances and resolve ambiguities. A novel inference mechanism, Contextualized Path Reasoning (CPR), is developed to perform complex queries over the CRKG. CPR combines graph traversal with attention-based mechanisms to dynamically weight and filter relevant paths based on the query context. Experiments on benchmark knowledge base completion and relation prediction tasks demonstrate that CRKGs, coupled with CPR, achieve significant improvements in accuracy and recall compared to state-of-the-art methods. Furthermore, ablation studies reveal the crucial role of contextualized embeddings in disambiguating relations and improving the robustness of inference.",ai
"In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of ( and are the number of local samples and the dimension of decision variables, respectively) with -DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.",human
"Clinical notes in Electronic Health Records (EHRs) capture rich temporal information on events, clinician reasoning, and lifestyle factors often missing from structured data. Leveraging them for predictive modeling can be impactful for timely identification of chronic diseases. However, they present core natural language processing (NLP) challenges: long text, irregular event distribution, complex temporal dependencies, privacy constraints, and resource limitations. We present two complementary methods for temporally and contextually grounded risk prediction from longitudinal notes. First, we introduce HiTGNN, a hierarchical temporal graph neural network that integrates intra-note temporal event structures, inter-visit dynamics, and medical knowledge to model patient trajectories with fine-grained temporal granularity. Second, we propose ReVeAL, a lightweight, test-time framework that distills the reasoning of large language models into smaller verifier models. Applied to opportunistic screening for Type 2 Diabetes (T2D) using temporally realistic cohorts curated from private and public hospital corpora, HiTGNN achieves the highest predictive accuracy, especially for near-term risk, while preserving privacy and limiting reliance on large proprietary models. ReVeAL enhances sensitivity to true T2D cases and retains explanatory reasoning. Our ablations confirm the value of temporal structure and knowledge augmentation, and fairness analysis shows HiTGNN performs more equitably across subgroups.",human
"Transformer-based models have demonstrated remarkable success in various Natural Language Processing (NLP) tasks; however, their computational demands and the inherent limitations of fixed-length context windows pose significant challenges for handling long-range dependencies and processing lengthy documents efficiently. This work addresses the problem of scalability and long-context understanding in Transformer models. We introduce a novel architecture, the Memory-Augmented Hierarchical Transformer (MAHT), designed to leverage both local and global contextual information effectively. MAHT employs a hierarchical structure where lower-level transformers process segments of the input sequence, generating condensed representations that are subsequently aggregated and processed by higher-level transformers. A memory module, integrated at each level, allows for the storage and retrieval of relevant information from previously processed segments, facilitating long-range dependency modeling. Experiments conducted on benchmark datasets for document summarization, question answering on long documents, and long-range arena demonstrate that MAHT achieves state-of-the-art performance while exhibiting significantly reduced computational complexity compared to standard Transformer models and their variants. Specifically, MAHT achieves an average improvement of 3.5 ROUGE points on document summarization tasks and a 5% increase in accuracy on long-range question answering, confirming its efficacy in handling extended context windows. Further analyses reveal improved attention patterns, indicating a more effective utilization of long-range dependencies.",ai
"Deep learning methodologies have demonstrated remarkable potential in automated medical image analysis. However, challenges remain in addressing the inherent complexities of medical imaging data, including high dimensionality, inter-patient variability, and limited annotated datasets. This work introduces a novel convolutional neural network (CNN) architecture, termed MedViT-Fusion, specifically designed to leverage both local and global contextual information within medical images for improved diagnostic performance. MedViT-Fusion integrates a vision transformer (ViT) backbone with parallel convolutional streams to capture both fine-grained textural features and long-range dependencies critical for accurate disease characterization. A fusion module dynamically weights the representations learned by the convolutional and transformer branches based on input image characteristics. We evaluate MedViT-Fusion on two benchmark datasets: the NIH Chest X-ray dataset for pneumonia detection and the ISIC 2018 dataset for skin lesion classification. Experimental results indicate that MedViT-Fusion achieves state-of-the-art performance on both datasets, outperforming existing CNN-based and transformer-based methods. Specifically, MedViT-Fusion demonstrates an improvement of 2.3% in AUC on the NIH Chest X-ray dataset and 1.8% in accuracy on the ISIC 2018 dataset. Ablation studies further validate the efficacy of the proposed fusion mechanism. These findings suggest that MedViT-Fusion offers a robust and effective approach for medical image analysis, potentially facilitating improved clinical decision-making.",ai
"### Abstract We address the challenge of representing and reasoning with complex, evolving knowledge in dynamic environments, a critical bottleneck in achieving robust and adaptable intelligent systems. Existing knowledge representation formalisms often struggle with efficiently capturing nuanced relationships, managing uncertainty, and scaling to large, real-world datasets. This paper introduces a novel Knowledge Graph Embedding with Attentive Relation Fusion (KGE-ARF) approach. KGE-ARF utilizes a relational graph convolutional network (RGCN) to learn node embeddings that capture both structural and semantic information within the knowledge graph. Novel to our approach is an attention mechanism applied to the relation types connecting nodes, allowing the model to dynamically weight the importance of different relations during inference. This attention mechanism enables the model to focus on the most relevant relationships for specific tasks, thereby improving the accuracy of downstream reasoning tasks. We evaluate KGE-ARF on standard knowledge graph completion benchmarks, including WN18RR and FB15k-237. Experimental results demonstrate that KGE-ARF achieves state-of-the-art performance, outperforming existing embedding-based methods and symbolic approaches in link prediction tasks. Furthermore, ablation studies confirm the effectiveness of the attentive relation fusion mechanism in improving performance and model interpretability. The proposed method offers a promising avenue for building more adaptable and reliable knowledge representation systems.",ai
"We address the challenge of decentralized policy learning in cooperative multi-agent systems (MAS) under partial observability and non-stationarity. Traditional reinforcement learning algorithms often struggle in this setting due to the difficulty of credit assignment and the dynamically changing environment induced by the simultaneous learning of other agents. To mitigate these issues, we propose a novel multi-agent learning framework based on Variational Information Bottleneck (VIB) principles. Specifically, each agent learns a policy that maximizes its expected reward while simultaneously minimizing the information it shares with other agents about its internal state, conditioned on relevant communication. This induces structured exploration and encourages agents to learn policies that are robust to the actions of others. We introduce a practical implementation of this VIB-based decentralized policy, employing a deep recurrent neural network architecture to represent agent policies and a dedicated communication channel. Empirical evaluations on a suite of challenging cooperative tasks demonstrate that our approach significantly outperforms state-of-the-art multi-agent reinforcement learning algorithms in terms of both convergence speed and asymptotic performance. Furthermore, we analyze the learned communication strategies and show that agents learn to communicate only task-relevant information, leading to more efficient coordination and improved generalization capabilities. Our findings highlight the potential of information-theoretic principles for designing robust and scalable learning algorithms in complex multi-agent environments.",ai
"We address the problem of identifying causal effects in observational data when the underlying causal graph is unknown and potentially contains unobserved confounders. Existing methods often rely on strong assumptions about the functional relationships between variables or require explicit search over the space of possible causal structures, which can be computationally expensive and sensitive to model misspecification. We propose a novel approach based on a combination of instrumental variable (IV) estimation and kernel methods. Specifically, we leverage the kernel conditional independence test to identify potential instrumental variables without requiring complete causal discovery. We then employ a kernel-based IV estimator to infer the causal effect of a treatment on an outcome. This approach allows us to handle non-linear relationships and weaker instrumental variable assumptions compared to traditional linear IV methods. We provide theoretical guarantees for the consistency of our estimator under mild conditions. Empirically, we demonstrate the effectiveness of our method on both synthetic datasets and real-world benchmark datasets, showing that it achieves competitive performance in causal effect estimation compared to state-of-the-art methods, particularly in scenarios with complex, non-linear causal relationships and unobserved confounding. Our approach offers a more robust and flexible alternative for causal inference in observational studies.",ai
"Federated learning enables collaborative model training across distributed data sources but suffers from slow convergence under non-IID data conditions. Existing solutions employ algorithmic modifications treating all client updates identically, ignoring semantic validity. We introduce Semantic-Constrained Federated Aggregation (SCFA), a theoretically-grounded framework incorporating domain knowledge constraints into distributed optimization. We prove SCFA achieves convergence rate O(1/sqrt(T) + rho) where rho represents constraint violation rate, establishing the first convergence theory for constraint-based federated learning. Our analysis shows constraints reduce effective data heterogeneity by 41% and improve privacy-utility tradeoffs through hypothesis space reduction by factor theta=0.37. Under (epsilon,delta)-differential privacy with epsilon=10, constraint regularization maintains utility within 3.7% of non-private baseline versus 12.1% degradation for standard federated learning, representing 2.7x improvement. We validate our framework on manufacturing predictive maintenance using Bosch production data with 1.18 million samples and 968 sensor features, constructing knowledge graphs encoding 3,000 constraints from ISA-95 and MASON ontologies. Experiments demonstrate 22% faster convergence, 41.3% model divergence reduction, and constraint violation thresholds where rho<0.05 maintains 90% optimal performance while rho>0.18 causes catastrophic failure. Our theoretical predictions match empirical observations with R^2>0.90 across convergence, privacy, and violation-performance relationships.",human
"This work addresses the vulnerability of natural language processing (NLP) models to adversarial attacks, specifically focusing on text classification tasks. We investigate the problem of crafting robust models that maintain high performance in the presence of imperceptible, yet malicious, input perturbations. Current defense mechanisms often rely on adversarial training, a computationally expensive process that can still be circumvented by adaptive attackers. We propose a novel robust optimization framework based on distributional robust optimization (DRO) with ambiguity sets defined by word-level semantic similarity. Our approach minimizes the worst-case risk over a distribution of perturbed inputs, where perturbations are constrained based on semantic similarity metrics derived from pre-trained language models. This constraint ensures that the adversarial inputs remain semantically close to the original input, making the attack less perceptible. We evaluate our method on several benchmark text classification datasets, including sentiment analysis and topic classification tasks. Experimental results demonstrate that our DRO-based approach significantly improves the robustness of NLP models against both white-box and black-box adversarial attacks, achieving substantial gains in adversarial accuracy compared to standard adversarial training and other state-of-the-art defense strategies, while maintaining comparable clean accuracy. We further analyze the trade-off between robustness and accuracy, showing that our method achieves a more favorable balance than existing techniques.",ai
"Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs. We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.",human
"We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.",human
"We address the challenge of decision-making under uncertainty, a pervasive problem in various domains. Specifically, we investigate robust optimization models where distributional ambiguity is present, hindering the computation of optimal and reliable solutions. Traditional robust optimization techniques often lead to overly conservative solutions, sacrificing performance in nominal scenarios. We propose a novel approach combining distributionally robust optimization (DRO) with a data-driven kernel method for ambiguity set construction. This method leverages historical data to estimate the underlying probability distribution and constructs a non-parametric ambiguity set defined by kernel density estimation. We introduce a tractable reformulation of the DRO problem using conic duality, enabling efficient computation of robust solutions. Empirically, we evaluate our methodology on benchmark portfolio optimization and inventory management problems, comparing it against benchmark approaches including box uncertainty and moment-based DRO. Results demonstrate that our kernel-based DRO approach achieves a superior trade-off between robustness and performance, yielding higher expected returns and lower costs while maintaining constraint satisfaction under uncertainty. The proposed method consistently outperforms existing robust optimization techniques, particularly in situations with limited historical data, offering a practical framework for decision-making in uncertain environments.",ai
"We investigate the challenge of sample-efficient reinforcement learning in environments with sparse and delayed rewards, a common hurdle in real-world applications. Specifically, we address the exploration-exploitation dilemma in settings where effective policies are difficult to discover due to the scarcity of informative feedback. Our approach, termed Trajectory-Guided Exploration (TGE), leverages a novel combination of trajectory optimization and intrinsic motivation. TGE employs an offline dataset of potentially suboptimal trajectories, using trajectory optimization to generate candidate policies that are then evaluated in the environment. The intrinsic reward is formulated as the divergence between the learned policy and the offline trajectory distribution, incentivizing exploration towards regions of the state space previously deemed relevant. We present a theoretical analysis demonstrating that TGE enjoys improved sample complexity compared to purely exploration-based methods in certain classes of Markov Decision Processes. Empirically, we evaluate TGE on a suite of challenging benchmark environments, including sparse-reward navigation and manipulation tasks. Our results demonstrate that TGE significantly outperforms state-of-the-art reinforcement learning algorithms in terms of sample efficiency and asymptotic performance, achieving substantial improvements in cumulative reward and reducing the time required to learn effective policies. The results support the hypothesis that trajectory-guided exploration provides a robust and efficient mechanism for learning in environments with sparse rewards.",ai
"This paper investigates the problem of emergent communication protocol efficiency in decentralized multi-agent reinforcement learning (MARL) settings. Specifically, we address the challenge of agents developing sub-optimal communication strategies that hinder global task performance, particularly in environments requiring complex coordination. Our proposed method, referred to as Differentiated Reward Shaping (DRS), introduces a novel reward structure that incentivizes agents to both effectively communicate relevant information and discriminate between the utility of incoming messages. DRS incorporates a Kullback-Leibler divergence term into the reward function to encourage agents to align their message content with the perceived information need of their recipients, while simultaneously penalizing the reception of uninformative or redundant signals. We evaluate DRS on a suite of cooperative navigation and target assignment tasks characterized by partial observability and the need for decentralized decision-making. Experimental results demonstrate that DRS significantly improves the efficiency of learned communication protocols, leading to a substantial increase in task completion rates and a reduction in communication overhead compared to baseline MARL algorithms employing standard reward structures. Furthermore, analysis reveals that DRS promotes the emergence of more interpretable communication signals, facilitating a deeper understanding of the agents' learned coordination strategies. These findings suggest that DRS provides a promising approach for developing efficient and robust communication in complex multi-agent systems.",ai
"Acronym Disambiguation (AD) is a fundamental challenge in technical text processing, particularly in specialized sectors where high ambiguity complicates automated analysis. This paper addresses AD within the context of the TextMine'26 competition on French railway documentation. We present DACE (Dynamic Prompting, Retrieval Augmented Generation, Contextual Selection, and Ensemble Aggregation), a framework that enhances Large Language Models through adaptive in-context learning and external domain knowledge injection. By dynamically tailoring prompts to acronym ambiguity and aggregating ensemble predictions, DACE mitigates hallucination and effectively handles low-resource scenarios. Our approach secured the top rank in the competition with an F1 score of 0.9069.",human
"Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.",human
"Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc. We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize: L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud, where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space. Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.",human
"Memory-augmented spiking neural networks (SNNs) promise energy-efficient neuromorphic computing, yet their generalization across sensory modalities remains unexplored. We present the first comprehensive cross-modal ablation study of memory mechanisms in SNNs, evaluating Hopfield networks, Hierarchical Gated Recurrent Networks (HGRNs), and supervised contrastive learning (SCL) across visual (N-MNIST) and auditory (SHD) neuromorphic datasets. Our systematic evaluation of five architectures reveals striking modality-dependent performance patterns: Hopfield networks achieve 97.68% accuracy on visual tasks but only 76.15% on auditory tasks (21.53 point gap), revealing severe modality-specific specialization, while SCL demonstrates more balanced cross-modal performance (96.72% visual, 82.16% audio, 14.56 point gap). These findings establish that memory mechanisms exhibit task-specific benefits rather than universal applicability. Joint multi-modal training with HGRN achieves 94.41% visual and 79.37% audio accuracy (88.78% average), matching parallel HGRN performance through unified deployment. Quantitative engram analysis confirms weak cross-modal alignment (0.038 similarity), validating our parallel architecture design. Our work provides the first empirical evidence for modality-specific memory optimization in neuromorphic systems, achieving 603x energy efficiency over traditional neural networks.",human
"We investigate a novel approach to robust optimization problems characterized by uncertainty in the objective function. Traditional methods often rely on conservative uncertainty sets, leading to suboptimal solutions. This work introduces a distributionally robust optimization framework that leverages kernel mean embedding (KME) to represent the ambiguity set of the uncertain parameter. Specifically, we construct ambiguity sets based on nonparametric estimates of the distribution of the uncertain parameter using KME, thereby allowing for more flexible and data-driven representations compared to classical methods. The resulting distributionally robust optimization problem is then solved using a semi-definite programming (SDP) reformulation, ensuring tractability for a wide class of convex objective functions. We demonstrate the efficacy of our approach on benchmark portfolio optimization problems and machine learning classification tasks. Empirical results show that our KME-based distributionally robust optimization strategy consistently outperforms classical box-constrained and ellipsoidal uncertainty set approaches in terms of out-of-sample performance, achieving superior robustness against adversarial data perturbations and parameter uncertainty. Furthermore, we analyze the sensitivity of the solution to the kernel bandwidth parameter, providing practical guidelines for implementation. Our method provides a statistically motivated and computationally efficient alternative to existing robust optimization techniques, particularly in scenarios where historical data is available to inform the structure of the ambiguity set.",ai
"Accurate prediction of flow delay is essential for optimizing and managing modern communication networks. We investigate three levels of modeling for this task. First, we implement a heterogeneous GNN with attention-based message passing, establishing a strong neural baseline. Second, we propose FlowKANet in which Kolmogorov-Arnold Networks replace standard MLP layers, reducing trainable parameters while maintaining competitive predictive performance. FlowKANet integrates KAMP-Attn (Kolmogorov-Arnold Message Passing with Attention), embedding KAN operators directly into message-passing and attention computation. Finally, we distill the model into symbolic surrogate models using block-wise regression, producing closed-form equations that eliminate trainable weights while preserving graph-structured dependencies. The results show that KAN layers provide a favorable trade-off between efficiency and accuracy and that symbolic surrogates emphasize the potential for lightweight deployment and enhanced transparency.",human
"The forward problem in electrocardiology, computing body surface potentials from cardiac electrical activity, is traditionally solved using physics-based models such as the bidomain or monodomain equations. While accurate, these approaches are computationally expensive, limiting their use in real-time and large-scale clinical applications. We propose a proof-of-concept deep learning (DL) framework as an efficient surrogate for forward solvers. The model adopts a time-dependent, attention-based sequence-to-sequence architecture to predict electrocardiogram (ECG) signals from cardiac voltage propagation maps. A hybrid loss combining Huber loss with a spectral entropy term was introduced to preserve both temporal and frequency-domain fidelity. Using 2D tissue simulations incorporating healthy, fibrotic, and gap junction-remodelled conditions, the model achieved high accuracy (mean ). Ablation studies confirmed the contributions of convolutional encoders, time-aware attention, and spectral entropy loss. These findings highlight DL as a scalable, cost-effective alternative to physics-based solvers, with potential for clinical and digital twin applications.",human
"We present a novel approach to enhancing the efficiency and accuracy of automated reasoning systems when confronted with large-scale knowledge graphs containing both explicit and implicit relationships. The problem addressed is the computational complexity associated with traversing and inferring new knowledge from such graphs, often exacerbated by noise and incompleteness. Our method leverages a hybrid architecture integrating symbolic reasoning with graph neural networks (GNNs). Specifically, we employ a first-order logic reasoner to perform initial deductive steps on the explicit knowledge, generating a set of candidate inferences. These candidates are then refined by a GNN model trained to predict the validity of relationships based on graph embeddings that capture both structural and semantic context. The GNN acts as a learned heuristic, guiding the symbolic reasoner towards more promising inference paths and pruning incorrect deductions. We evaluate our approach on benchmark knowledge graph completion tasks, achieving significant improvements in Mean Reciprocal Rank (MRR) and Hits@k metrics compared to state-of-the-art symbolic and neural methods. Furthermore, we demonstrate a substantial reduction in reasoning time, attributable to the GNN's ability to filter and prioritize potential inferences, leading to more efficient exploration of the knowledge graph.",ai
"Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.",human
"We address the problem of automatically synthesizing quantified inductive invariants for verifying the safety properties of parameterized concurrent systems. Existing techniques often struggle with the complexity of quantifier instantiation and maintaining the inductive validity of generated invariants, particularly when dealing with intricate data dependencies and complex control flow. We propose a novel approach that combines counterexample-guided inductive synthesis (CEGIS) with a learned predicate abstraction refinement strategy. Our method leverages a neural network, trained on a corpus of previously verified systems, to predict a relevant set of abstract predicates for the CEGIS loop. These learned predicates guide the invariant generation process, effectively pruning the search space and improving the scalability of the verification. Furthermore, we integrate an SMT-based quantifier instantiation scheme that is specifically tailored to handle data-dependent properties, reducing the risk of spurious counterexamples. We demonstrate the effectiveness of our approach through extensive experiments on a benchmark suite of parameterized concurrent systems, including cache coherence protocols and distributed algorithms. Our results show a significant improvement in both the success rate and the runtime compared to state-of-the-art invariant generation tools, particularly on systems with complex data dependencies.",ai
"In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity, and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against tier-1 SMQs (110 queries, v28.1). Precision, recall and F1 were computed at multiple similarity-thresholds, defined either manually or using an automated method. High recall (94%)) is achieved at moderate similarity thresholds, indicative of good retrieval sensitivity. Higher thresholds filter out more terms, resulting in improved precision (up to 89%). The optimal threshold (0.70)) yielded an overall recall of (48%) and precision of (45%) across all 110 queries. Restricting to narrow-term PTs achieved slightly better performance at an increased (+0.05) similarity threshold, confirming increased relatedness of narrow versus broad terms. The automatic threshold (0.66) selection prioritizes recall (0.58) to precision (0.29). SafeTerm AMQ achieves comparable, satisfactory performance on SMQs and sanitized OCMQs. It is therefore a viable supplementary method for automated MedDRA query generation, balancing recall and precision. We recommend using suitable MedDRA PT terminology in query formulation and applying the automated threshold method to optimise recall. Increasing similarity scores allows refined, narrow terms selection.",human
"Observability into the decision making of modern AI systems may be required to safely deploy increasingly capable agents. Monitoring the chain-of-thought (CoT) of today's reasoning models has proven effective for detecting misbehavior. However, this ""monitorability"" may be fragile under different training procedures, data sources, or even continued system scaling. To measure and track monitorability, we propose three evaluation archetypes (intervention, process, and outcome-property) and a new monitorability metric, and introduce a broad evaluation suite. We demonstrate that these evaluations can catch simple model organisms trained to have obfuscated CoTs, and that CoT monitoring is more effective than action-only monitoring in practical settings. We compare the monitorability of various frontier models and find that most models are fairly, but not perfectly, monitorable. We also evaluate how monitorability scales with inference-time compute, reinforcement learning optimization, and pre-training model size. We find that longer CoTs are generally more monitorable and that RL optimization does not materially decrease monitorability even at the current frontier scale. Notably, we find that for a model at a low reasoning effort, we could instead deploy a smaller model at a higher reasoning effort (thereby matching capabilities) and obtain a higher monitorability, albeit at a higher overall inference compute cost. We further investigate agent-monitor scaling trends and find that scaling a weak monitor's test-time compute when monitoring a strong agent increases monitorability. Giving the weak monitor access to CoT not only improves monitorability, but it steepens the monitor's test-time compute to monitorability scaling trend. Finally, we show we can improve monitorability by asking models follow-up questions and giving their follow-up CoT to the monitor.",human
"We address the challenge of sample inefficiency in reinforcement learning (RL) when applied to environments with sparse rewards and complex state spaces. Specifically, we investigate the limitations of traditional on-policy methods, such as proximal policy optimization (PPO), in scenarios requiring extensive exploration to discover rewarding trajectories. To mitigate this, we propose a novel hybrid approach combining intrinsic motivation via curiosity-driven exploration with an off-policy actor-critic algorithm. Our method, which we term Curiosity-Augmented Deep Deterministic Policy Gradient (CA-DDPG), leverages a learned forward model to estimate the novelty of state transitions and incentivizes the agent to visit unexplored regions of the state space. This intrinsic reward signal is then integrated into the DDPG algorithm's reward function. We evaluate CA-DDPG on a series of benchmark environments characterized by sparse rewards and high-dimensional continuous action spaces. Our results demonstrate a significant improvement in sample efficiency compared to baseline methods, including PPO and standard DDPG. Furthermore, CA-DDPG exhibits enhanced robustness to variations in environment dynamics and reward sparsity, achieving consistently higher cumulative rewards and faster learning convergence. We provide a detailed ablation study quantifying the contribution of each component of our proposed method.",ai
"Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.",human
"We introduce compositional tensor trains (CTTs) for the approximation of multivariate functions, a class of models obtained by composing low-rank functions in the tensor-train format. This format can encode standard approximation tools, such as (sparse) polynomials, deep neural networks (DNNs) with fixed width, or tensor networks with arbitrary permutation of the inputs, or more general affine coordinate transformations, with similar complexities. This format can be viewed as a DNN with width exponential in the input dimension and structured weights matrices. Compared to DNNs, this format enables controlled compression at the layer level using efficient tensor algebra. On the optimization side, we derive a layerwise algorithm inspired by natural gradient descent, allowing to exploit efficient low-rank tensor algebra. This relies on low-rank estimations of Gram matrices, and tensor structured random sketching. Viewing the format as a discrete dynamical system, we also derive an optimization algorithm inspired by numerical methods in optimal control. Numerical experiments on regression tasks demonstrate the expressivity of the new format and the relevance of the proposed optimization algorithms. Overall, CTTs combine the expressivity of compositional models with the algorithmic efficiency of tensor algebra, offering a scalable alternative to standard deep neural networks.",human
"We address the problem of efficiently deriving non-trivial conclusions from large, complex knowledge bases represented in first-order logic. Existing automated theorem provers often struggle with such knowledge bases due to combinatorial explosion in the search space. Our approach leverages a novel combination of semantic indexing and reinforcement learning (RL) to guide the proof search. Specifically, we construct a semantic index of the knowledge base using pre-trained language models, enabling the retrieval of relevant axioms based on semantic similarity to the current goal. This index is then integrated into an RL agent trained to select appropriate inference rules and axioms to apply at each step of the proof search. The reward function is designed to encourage shorter and more efficient proofs. We evaluate our method on a suite of challenging theorem proving benchmarks, including problems from the TPTP library. Experimental results demonstrate a significant improvement in both the success rate and the average proof length compared to state-of-the-art automated theorem provers such as E and Vampire. Furthermore, our approach exhibits improved generalization capabilities, successfully proving theorems that are semantically similar to, but not directly present in, the training data. This suggests a promising direction for developing more robust and efficient automated reasoning systems.",ai
"We address the problem of decentralized policy learning in multi-agent systems characterized by partial observability and non-stationarity. Existing approaches often struggle with the curse of dimensionality and the challenges of maintaining consistent exploration across agents in complex environments. This work introduces a novel multi-agent reinforcement learning algorithm, Decentralized Variational Information Bottleneck (DVIB), which leverages a variational information bottleneck objective to encourage the learning of compact and informative representations of each agent's local observation history. Each agent individually infers a latent representation conditioned on its local experience, aiming to maximize the mutual information with its own actions while minimizing the information shared with other agents' observations. This promotes independent exploration and mitigates the issues arising from non-stationarity. We theoretically demonstrate the convergence properties of DVIB under certain regularity conditions. Empirically, we evaluate DVIB on a range of challenging cooperative and competitive multi-agent environments. Results show that DVIB outperforms several state-of-the-art decentralized policy learning algorithms in terms of both sample efficiency and asymptotic performance. Furthermore, analysis of the learned latent representations reveals that DVIB effectively captures the relevant aspects of the environment while discarding irrelevant information, leading to more robust and generalizable policies.",ai
"We address the critical challenge of protecting sensitive user data in machine learning (ML) applications, specifically within natural language processing (NLP) tasks. The increasing reliance on large language models necessitates robust privacy mechanisms to prevent leakage of private information during training and inference. This work introduces a novel differentially private fine-tuning approach for transformer-based models, combining gradient perturbation with a refined clipping strategy. We propose an adaptive clipping norm based on the layer-wise sensitivity of the model, dynamically adjusting the clipping threshold to minimize noise injection while preserving privacy guarantees. Furthermore, we leverage knowledge distillation to transfer the learned knowledge from the differentially private model to a non-private student model, enabling more efficient and accurate inference. We evaluate our method on text classification and question answering benchmarks using publicly available datasets. Empirical results demonstrate that our adaptive clipping strategy significantly improves the utility-privacy trade-off compared to standard differential privacy techniques. Specifically, we observe a 5-10% improvement in accuracy for comparable privacy budgets (ε, δ), while the distilled model maintains comparable performance to the original model with significantly reduced computational cost. These findings suggest that our approach offers a practical and effective solution for training privacy-preserving NLP models.",ai
"The emergence of open data portals necessitates more attention to protecting sensitive data before datasets get published and exchanged. While an abundance of methods for suppressing sensitive data exist, the conceptualization of sensitive data and methods to detect it, focus particularly on personal data that, if disclosed, may be harmful or violate privacy. We observe the need for refining and broadening our definitions of sensitive data, and argue that the sensitivity of data depends on its context. Based on this definition, we introduce two mechanisms for contextual sensitive data detection that consider the broader context of a dataset at hand. First, we introduce type contextualization, which first detects the semantic type of particular data values, then considers the overall context of the data values within the dataset or document. Second, we introduce domain contextualization which determines sensitivity of a given dataset in the broader context based on the retrieval of relevant rules from documents that specify data sensitivity (e.g., data topic and geographic origin). Experiments with these mechanisms, assisted by large language models (LLMs), confirm that: 1) type-contextualization significantly reduces the number of false positives for type-based sensitive data detection and reaches a recall of 94% compared to 63% with commercial tools, and 2) domain-contextualization leveraging sensitivity rule retrieval is effective for context-grounded sensitive data detection in non-standard data domains such as humanitarian datasets. Evaluation with humanitarian data experts also reveals that context-grounded LLM explanations provide useful guidance in manual data auditing processes, improving consistency. We open-source mechanisms and annotated datasets for contextual sensitive data detection at https://github.com/trl-lab/sensitive-data-detection.",human
"We investigate a novel approach to robust optimization problems characterized by uncertainty in the objective function arising from distributional ambiguity. Traditional robust optimization techniques often rely on worst-case analysis against a predefined uncertainty set, leading to overly conservative solutions, particularly when the true data distribution is unknown or imprecisely estimated. We propose a data-driven robust optimization framework utilizing a Wasserstein distributionally robust optimization (WDRO) approach. This framework minimizes the worst-case expected cost with respect to a set of probability distributions that are within a Wasserstein distance from the empirical distribution derived from observed data. To enhance computational tractability, we employ a sample average approximation (SAA) scheme coupled with a cutting-plane algorithm to solve the resulting computationally challenging minimax problem. The efficacy of the proposed methodology is demonstrated through a series of numerical experiments based on portfolio optimization and inventory management problems. Results indicate that our WDRO approach, with appropriately chosen Wasserstein radius, yields solutions that outperform both nominal optimization strategies and classical robust optimization methods in out-of-sample performance, exhibiting superior robustness against distributional shifts and noisy data. Furthermore, we explore the impact of the Wasserstein radius on the trade-off between robustness and optimality.",ai
"We investigate the challenge of decentralized learning in multi-agent systems where agents possess heterogeneous local datasets and are constrained to communicate only with their immediate neighbors. This setting precludes the use of centralized data aggregation and necessitates distributed learning strategies robust to data heterogeneity and communication limitations. We propose a novel distributed stochastic gradient descent (DSGD) algorithm, termed ""Adaptive Momentum Federated Averaging"" (AMFA), that incorporates adaptive momentum estimation and a federated averaging scheme dynamically adjusted by local data characteristics. AMFA addresses the issue of gradient staleness prevalent in asynchronous distributed learning by weighting local updates based on their estimated information content, derived from the observed variance of the local gradient. Through rigorous theoretical analysis, we establish convergence guarantees for AMFA under mild assumptions on the objective function and communication graph. Empirical evaluations on synthetic and real-world datasets demonstrate that AMFA consistently outperforms existing DSGD algorithms, particularly in scenarios characterized by significant data heterogeneity and sparse communication networks. Specifically, AMFA achieves a reduction in convergence time of up to 30% compared to standard Federated Averaging and other momentum-based DSGD variants, indicating its superior ability to navigate non-convex optimization landscapes in decentralized multi-agent systems.",ai
"We address the problem of efficiently capturing long-range dependencies in graph-structured data for node classification tasks. Traditional graph neural networks (GNNs) often struggle to propagate information effectively across distant nodes, limiting their performance on graphs with complex relational structures. To mitigate this limitation, we introduce a novel graph neural network architecture, the ""Adaptive Path Aggregation Network"" (APAN). APAN leverages a learnable path selection mechanism based on attention, enabling the model to adaptively aggregate information from relevant nodes along diverse paths, regardless of their distance. Specifically, APAN constructs a path adjacency matrix by weighting edges based on attention scores derived from node features and relational information. This matrix is then used to aggregate node representations across multiple hops. We evaluate APAN on several benchmark datasets, including OGBN-Arxiv and CoraFull, comparing its performance against established GNN baselines such as GCN, GAT, and GraphSAGE. Experimental results demonstrate that APAN significantly outperforms these baselines in terms of node classification accuracy, achieving improvements of up to 5% on datasets with sparse connectivity and long-range dependencies. Furthermore, ablation studies validate the efficacy of the adaptive path selection mechanism in capturing relevant contextual information for improved node representation learning.",ai
"Hyperspectral images with high spectral resolution provide new insights into recognizing subtle differences in similar substances. However, object detection in hyperspectral images faces significant challenges in intra- and inter-class similarity due to the spatial differences in hyperspectral inter-bands and unavoidable interferences, e.g., sensor noises and illumination. To alleviate the hyperspectral inter-bands inconsistencies and redundancy, we propose a novel network termed pectral iscrepancy and ross-odal semantic consistency learning (SDCM), which facilitates the extraction of consistent information across a wide range of hyperspectral bands while utilizing the spectral dimension to pinpoint regions of interest. Specifically, we leverage a semantic consistency learning (SCL) module that utilizes inter-band contextual cues to diminish the heterogeneity of information among bands, yielding highly coherent spectral dimension representations. On the other hand, we incorporate a spectral gated generator (SGG) into the framework that filters out the redundant data inherent in hyperspectral information based on the importance of the bands. Then, we design the spectral discrepancy aware (SDA) module to enrich the semantic representation of high-level information by extracting pixel-level spectral features. Extensive experiments on two hyperspectral datasets demonstrate that our proposed method achieves state-of-the-art performance when compared with other ones.",human
"This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.",human
"We investigate the problem of sample inefficiency in reinforcement learning (RL) when applied to complex, high-dimensional continuous control tasks with sparse rewards. Traditional RL algorithms often require prohibitively large amounts of interaction with the environment to achieve satisfactory performance. To address this limitation, we propose a novel approach combining hierarchical reinforcement learning (HRL) with off-policy correction techniques. Our method decomposes the control problem into a two-level hierarchy: a high-level manager that learns a sequence of subgoals, and a low-level worker that executes those subgoals. To improve sample efficiency, we utilize an off-policy correction algorithm, specifically a variant of Importance Sampling, to reweight experiences collected by the worker under different subgoal policies. This allows the manager to learn more effectively from the worker's experience, even when the worker's policy deviates significantly from the current subgoal. We evaluated our method on a series of challenging simulated robotic manipulation tasks, including object rearrangement and assembly. Results demonstrate a significant improvement in sample efficiency compared to standard RL algorithms and other HRL baselines. Specifically, our method achieved comparable or superior performance with up to 50% fewer environment interactions, indicating a substantial reduction in the required training time and computational resources.",ai
"Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves - speedups while retaining - of the baseline score on average. On base models, SchED yields consistent speedup gains with - performance retention, with up to under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, ), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.",human
"Graph Neural Networks (GNNs) have emerged as a powerful paradigm for learning representations on graph-structured data, exhibiting strong performance across diverse tasks. However, many existing GNN architectures struggle to effectively capture long-range dependencies between nodes, particularly in large and sparse graphs. This limitation hinders their ability to model complex relational reasoning and derive global structural insights. We introduce a novel GNN architecture, the Attentive Path Aggregation Network (APAN), designed to address this issue. APAN leverages a learned path aggregator to explicitly model relationships between nodes connected by paths of varying lengths. The aggregator dynamically weights paths based on their relevance to the target node, enabling the network to selectively propagate information from distant regions of the graph. Furthermore, APAN employs a multi-hop attention mechanism to refine node embeddings by attending over their neighborhood at multiple scales. We evaluate APAN on several benchmark graph classification and node classification datasets, including those exhibiting long-range dependencies and hierarchical structures. Experimental results demonstrate that APAN consistently outperforms state-of-the-art GNNs, achieving significant improvements in accuracy and demonstrating enhanced robustness to graph sparsity. These findings underscore the effectiveness of attentive path aggregation in capturing long-range dependencies and improving the representational capacity of GNNs.",ai
"Transformers are increasingly adopted for modeling and forecasting time-series, yet their internal mechanisms remain poorly understood from a dynamical systems perspective. In contrast to classical autoregressive and state-space models, which benefit from well-established theoretical foundations, Transformer architectures are typically treated as black boxes. This gap becomes particularly relevant as attention-based models are considered for general-purpose or zero-shot forecasting across diverse dynamical regimes. In this work, we do not propose a new forecasting model, but instead investigate the representational capabilities and limitations of single-layer Transformers when applied to dynamical data. Building on a dynamical systems perspective we interpret causal self-attention as a linear, history-dependent recurrence and analyze how it processes temporal information. Through a series of linear and nonlinear case studies, we identify distinct operational regimes. For linear systems, we show that the convexity constraint imposed by softmax attention fundamentally restricts the class of dynamics that can be represented, leading to oversmoothing in oscillatory settings. For nonlinear systems under partial observability, attention instead acts as an adaptive delay-embedding mechanism, enabling effective state reconstruction when sufficient temporal context and latent dimensionality are available. These results help bridge empirical observations with classical dynamical systems theory, providing insight into when and why Transformers succeed or fail as models of dynamical systems.",human
"Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.",human
"With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.",human
"Processing overlapping narrative documents, such as legal testimonies or historical accounts, often aims not for compression but for a unified, coherent, and chronologically sound text. Standard Multi-Document Summarization (MDS), with its focus on conciseness, fails to preserve narrative flow. This paper formally defines this challenge as a new NLP task: Narrative Consolidation, where the central objectives are chronological integrity, completeness, and the fusion of complementary details. To demonstrate the critical role of temporal structure in this task, we introduce Temporal Alignment Event Graph (TAEG), a graph structure that explicitly models chronology and event alignment. By applying a standard centrality algorithm to TAEG, our method functions as a version selection mechanism, choosing the most central representation of each event in its correct temporal position. In a study on the four Biblical Gospels, this structure-focused approach guarantees perfect temporal ordering (Kendall's Tau of 1.000) by design and dramatically improves content metrics (e.g., +357.2% in ROUGE-L F1). The success of this baseline method validates the formulation of Narrative Consolidation as a relevant task and establishes that an explicit temporal backbone is a fundamental component for its resolution.",human
"Graph Neural Networks (GNNs) have demonstrated considerable efficacy in learning representations of graph-structured data for various downstream tasks. However, many existing GNN architectures suffer from oversmoothing, limiting their ability to effectively model deep graph relationships and distinguish between nodes with increasingly similar features as the number of layers increases. This phenomenon hinders the performance of GNNs on tasks requiring the discernment of subtle differences in node representations within complex graphs. This work introduces a novel GNN architecture, termed Adaptive Residual Graph Convolutional Network (ARGCN), designed to mitigate oversmoothing. ARGCN incorporates adaptive residual connections that dynamically adjust the weight of residual pathways based on the spectral properties of the graph Laplacian. Specifically, we introduce a learnable spectral filter that attenuates high-frequency components of the node features, effectively smoothing the representation before it is added to the residual connection. This adaptive smoothing allows for controlled information propagation and prevents excessive feature homogenization. Empirical evaluations on several benchmark graph datasets, including Cora, CiteSeer, PubMed, and Amazon Computers, demonstrate that ARGCN consistently outperforms state-of-the-art GNN architectures in node classification accuracy. Furthermore, we provide a theoretical analysis demonstrating that the adaptive residual connections in ARGCN lead to improved spectral properties compared to standard residual GNNs, enabling the model to maintain distinguishable node representations even with deeper architectures. The results suggest that ARGCN represents a significant advancement in addressing the oversmoothing problem in GNNs.",ai
"We address the problem of distributionally robust optimization (DRO) for machine learning models in the presence of adversarial data perturbations. Specifically, we consider a setting where the true data distribution is unknown but assumed to lie within an ambiguity set defined by a Wasserstein distance from a nominal, empirically observed distribution. Solving the DRO problem aims to minimize the worst-case expected loss over this ambiguity set, thereby enhancing model robustness against unseen data shifts and adversarial attacks. Our method proposes a novel, computationally efficient algorithm based on stochastic gradient descent (SGD) for solving the Wasserstein DRO problem. The algorithm utilizes a mini-batch approach combined with a cutting-plane method to approximate the worst-case distribution and efficiently update the model parameters. To further improve computational scalability, we employ a Frank-Wolfe decomposition of the ambiguity set. We evaluate the performance of our proposed method on several benchmark datasets for image classification, including CIFAR-10 and MNIST, under various adversarial perturbation scenarios. Empirical results demonstrate that our method significantly outperforms existing DRO approaches and standard empirical risk minimization (ERM) in terms of both out-of-distribution generalization and robustness against adversarial attacks, while maintaining comparable computational efficiency. The proposed algorithm consistently achieves higher accuracy and lower worst-case loss, highlighting its effectiveness in mitigating the effects of data uncertainty.",ai
"Current Large Language Models (LLMs) face three major challenges: context length limitations, high inference costs, and catastrophic forgetting during continual learning. While Mixture-of-Experts (MoE) architectures mitigate some of these conflicts, their routing mechanisms typically rely on explicitly trained auxiliary classifiers. This not only increases system complexity but also often lacks interpretability when handling mixed-domain inputs. Building upon the premise that ``Compression is Intelligence,'' this paper proposes a novel architectural philosophy: Compression is Routing. We trained an 87M-parameter end-to-end Transformer Autoencoder, achieving a 64x sequence length compression (compressing 512 tokens into 8 latent vectors). Experimental results demonstrate that this compressor possesses extreme domain discriminative capability: it achieves a reconstruction accuracy of 99.47% on the in-domain (code) validation set; accuracy drops sharply to 47.76% on a semi-out-of-distribution domain (Wiki text); and further plummets to just 0.57% on a fully out-of-distribution domain (random sequences). This extreme and systematic performance discrepancy establishes the validity of reconstruction error as an Intrinsic Distribution Fingerprint. Based on this, we propose that expert modules can be automatically scheduled using reconstruction residuals directly, without the need for explicit gating networks. This mechanism offers excellent scalability. Furthermore, this architecture provides a new perspective on ``VRAM compression'' for handling ultra-long contexts. This report aims to verify the physical validity of this foundational architecture, offering a new research perspective for the next generation of scalable modular neural networks.",human
"**Privacy-Preserving Neural Network Training via Differentially Private Federated Learning with Dynamic Weight Averaging** Federated learning (FL) provides a distributed framework for training machine learning models without direct access to sensitive user data. However, FL is vulnerable to privacy attacks, including inference attacks and membership inference attacks. To mitigate these risks, we propose a novel approach combining differential privacy (DP) with federated learning and dynamic weight averaging. Our method introduces a client-level DP mechanism to each local model update prior to aggregation, parameterized by a privacy budget, *ε*. Furthermore, we introduce a dynamic weight averaging scheme that adjusts the contribution of each client's update to the global model based on the client's data distribution similarity to the global model and the calculated gradient norm. This adaptive weighting aims to improve model accuracy while mitigating the impact of noisy updates caused by DP. We theoretically analyze the privacy guarantees of our approach, proving (ε, δ)-differential privacy. Empirical evaluations on benchmark datasets demonstrate that our method achieves a significantly better trade-off between model accuracy and privacy compared to traditional DP-FL approaches with static averaging, particularly in scenarios with heterogeneous data distributions across clients. The results indicate a substantial reduction in accuracy loss for comparable privacy budgets, making our approach a promising solution for training accurate and privacy-preserving neural networks in federated environments.",ai
"With the rapid uptake of generative AI, investigating human perceptions of generated responses has become crucial. A major challenge is their `aptitude' for hallucinating and generating harmful contents. Despite major efforts for implementing guardrails, human perceptions of these mitigation strategies are largely unknown. We conducted a mixed-method experiment for evaluating the responses of a mitigation strategy across multiple-dimensions: faithfulness, fairness, harm-removal capacity, and relevance. In a within-subject study design, 57 participants assessed the responses under two conditions: harmful response plus its mitigation and solely mitigated response. Results revealed that participants' native language, AI work experience, and annotation familiarity significantly influenced evaluations. Participants showed high sensitivity to linguistic and contextual attributes, penalizing minor grammar errors while rewarding preserved semantic contexts. This contrasts with how language is often treated in the quantitative evaluation of LLMs. We also introduced new metrics for training and evaluating mitigation strategies and insights for human-AI evaluation studies.",human
"Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.",human
"We address the problem of identifying causal effects in observational data with unobserved confounding, focusing on scenarios where instrumental variables (IVs) are weak and/or exhibit heterogeneous effects across the population. Traditional IV methods often suffer from bias and instability in such settings. We propose a novel two-stage approach that leverages deep learning to enhance IV estimation. In the first stage, we train a neural network to predict the putative causal variable using candidate instruments and observed confounders. Crucially, we incorporate a regularized objective function that promotes instrument strength and orthogonality to the outcome variable within subpopulations defined by observed confounders, mitigating the impact of weak instruments and heterogeneity. In the second stage, we utilize the predicted values from the first stage as proxies for the causal variable within a two-stage least squares framework, employing debiased machine learning techniques to address overfitting. We evaluate our method on both synthetic datasets, demonstrating improved bias reduction and statistical power compared to existing IV estimators, and on a real-world healthcare dataset concerning the effect of medication on patient outcomes, where we identify plausible causal effects that are masked by conventional approaches. Our results highlight the potential of deep learning to enhance causal inference in challenging observational settings.",ai
"Large language models (LLMs) often generate hallucinations in knowledge-intensive QA due to parametric knowledge limitations. While existing methods like KG-CoT improve reliability by integrating knowledge graph (KG) paths, they suffer from rigid hop-count selection (solely question-driven) and underutilization of reasoning paths (lack of guidance). To address this, we propose RFKG-CoT: First, it replaces the rigid hop-count selector with a relation-driven adaptive hop-count selector that dynamically adjusts reasoning steps by activating KG relations (e.g., 1-hop for direct ""brother"" relations, 2-hop for indirect ""father-son"" chains), formalized via a relation mask. Second, it introduces a few-shot in-context learning path guidance mechanism with CoT (think) that constructs examples in a ""question-paths-answer"" format to enhance LLMs' ability to understand reasoning paths. Experiments on four KGQA benchmarks show RFKG-CoT improves accuracy by up to 14.7 pp (Llama2-7B on WebQSP) over KG-CoT. Ablations confirm the hop-count selector and the path prompt are complementary, jointly transforming KG evidence into more faithful answers.",human
"We present a novel approach to leveraging Graph Neural Networks (GNNs) for improved relational reasoning in complex natural language understanding tasks. The core challenge lies in effectively capturing long-range dependencies and intricate relationships between entities described in textual data. Existing methods often struggle with scalability and maintaining representational fidelity when dealing with large knowledge graphs or narratives with numerous interconnected components. Our proposed method, Graph-Augmented Attentive Relational Network (GAARN), integrates an attention mechanism directly into the GNN message-passing framework. GAARN dynamically adjusts the influence of neighboring nodes based on learned attention weights, allowing for selective aggregation of relevant information during graph propagation. Furthermore, we introduce a hierarchical graph construction strategy that aggregates word-level nodes into sentence-level and paragraph-level representations, facilitating reasoning across different granularities of text. We evaluate GAARN on several benchmark datasets for question answering and textual entailment involving complex relational reasoning. Experimental results demonstrate that GAARN consistently outperforms state-of-the-art baselines, achieving significant improvements in accuracy and demonstrating enhanced robustness to variations in graph structure. Specifically, GAARN achieves a 3.2% improvement on the OpenQA dataset, suggesting its effectiveness in complex knowledge-intensive tasks.",ai
"We address the challenge of training machine learning models that exhibit robustness against adversarial perturbations and distribution shifts, a critical requirement for reliable deployment in real-world scenarios. Traditional empirical risk minimization often fails to generalize under such conditions, leading to significant performance degradation. To mitigate this, we propose a novel robust optimization framework incorporating a distributionally robust loss function. Our approach leverages a Wasserstein ambiguity set to model uncertainty in the data distribution, minimizing the worst-case risk over this set. Specifically, we introduce a computationally tractable approximation of the Wasserstein distance using a kernel-based discrepancy measure, enabling efficient optimization via stochastic gradient descent. Furthermore, we derive theoretical generalization bounds that quantify the robustness of our method as a function of the ambiguity set size and the sample complexity. Empirical evaluations on benchmark image classification datasets, including CIFAR-10 and ImageNet, demonstrate that our proposed method achieves significant improvements in adversarial robustness and out-of-distribution generalization compared to state-of-the-art techniques, while maintaining competitive clean accuracy. The results highlight the effectiveness of our Wasserstein-based robust optimization framework in enhancing the reliability and safety of machine learning models in uncertain environments.",ai
"We address the challenge of learning compositional knowledge representations from unstructured data, specifically focusing on the problem of disentangling latent factors of variation in visual scenes. Current methods often struggle to generalize to novel compositions of known entities, exhibiting brittleness when faced with out-of-distribution data. We propose a novel approach, Compositional Disentangled Representation Learning via Neural Relation Inference (CDRL-NRI), which leverages neural relational inference to explicitly model the relationships between scene elements. CDRL-NRI learns to decompose scenes into objects and their pairwise relations, encoding these relationships into a graph structure. This graph structure, combined with object-specific latent codes, facilitates the generation of novel scenes through compositional recombination. We evaluate CDRL-NRI on a synthetic visual reasoning dataset designed to assess compositional generalization. Results demonstrate that CDRL-NRI significantly outperforms existing disentanglement and compositional representation learning methods in terms of reconstruction accuracy and compositional generalization ability. Ablation studies further reveal the importance of explicit relational modeling for achieving robust and generalizable knowledge representations. The learned relational structure provides an interpretable basis for understanding the composition of scenes, offering a step towards more robust and explainable knowledge representation.",ai
"The analysis of medical images presents a significant challenge due to the inherent complexity and variability of anatomical structures, compounded by noise and artifacts introduced during image acquisition. This work addresses the problem of automated lesion detection and segmentation in multi-modal medical imaging, specifically focusing on magnetic resonance imaging (MRI) and computed tomography (CT) scans. We propose a novel end-to-end deep learning framework leveraging a spatially-aware convolutional neural network (CNN) architecture, incorporating attention mechanisms to selectively emphasize relevant image regions and suppress irrelevant background noise. Our method integrates information from multiple MRI sequences (T1-weighted, T2-weighted, FLAIR) and CT scans through a multi-channel input layer, followed by a series of 3D convolutional blocks designed to capture volumetric features. A key innovation is the inclusion of a self-attention module after each convolutional block, enabling the network to dynamically learn and adapt to the specific characteristics of different lesions and imaging modalities. The proposed framework was evaluated on a publicly available dataset of brain tumor MRI scans (BRATS 2021) and a proprietary dataset of liver lesions from CT scans. Results demonstrate a substantial improvement in segmentation accuracy compared to state-of-the-art methods, achieving a Dice score of 0.85 ± 0.03 on the BRATS dataset and 0.78 ± 0.05 on the liver lesion dataset. These findings suggest that our method offers a robust and accurate solution for automated lesion detection and segmentation in medical imaging, potentially aiding clinicians in diagnosis and treatment planning.",ai
"We address the challenge of coordinating decentralized agents in complex, partially observable environments where agents possess heterogeneous capabilities and preferences. Existing multi-agent reinforcement learning (MARL) algorithms often struggle with scalability and convergence guarantees in such scenarios, particularly when agents must learn both communication strategies and individual policies simultaneously. We introduce a novel framework, Hierarchical Preference-Aware Communication (HPAC), which decomposes the learning problem into two hierarchical levels. The upper level employs a meta-controller to learn global coordination strategies based on an estimation of agents' collective preferences, derived from their individual reward signals and observed actions. The lower level consists of individual agent policies trained with a policy gradient method augmented with a communication channel conditioned on the meta-controller's output and local observations. HPAC facilitates efficient exploration by decoupling the learning of high-level coordination from low-level policy refinement. We evaluate HPAC on a suite of challenging multi-agent tasks, including a cooperative navigation scenario and a resource allocation game. Empirical results demonstrate that HPAC significantly outperforms state-of-the-art MARL algorithms in terms of convergence speed, final performance, and robustness to communication noise. Furthermore, analysis reveals that HPAC learns interpretable communication protocols that effectively align individual agent actions with global team objectives, leading to improved overall system performance.",ai
"**Privacy-Preserving Adversarial Training via Differentiable Privacy Amplification** The vulnerability of machine learning models to adversarial attacks necessitates robust defense mechanisms. Adversarial training (AT) is a widely recognized method, but its application in privacy-sensitive domains raises concerns due to potential information leakage. This paper addresses the problem of performing adversarial training while providing rigorous differential privacy (DP) guarantees. We introduce a novel approach that leverages the inherent randomization of AT, coupled with a differentiable Rényi differential privacy (RDP) accountant, to analyze and amplify the privacy budget. Our method, dubbed DP-AT-DiffAmp, estimates the privacy loss during training more accurately than traditional composition theorems, allowing for a tighter privacy budget accounting. We demonstrate, through extensive experiments on benchmark datasets (MNIST, CIFAR-10), that DP-AT-DiffAmp achieves significantly improved adversarial robustness compared to conventional DP-AT methods that rely on simplistic privacy composition rules. Furthermore, we show that DP-AT-DiffAmp can achieve comparable or even superior adversarial robustness to non-private AT, while providing provable DP guarantees. Our results indicate that differentiable privacy amplification offers a promising avenue for deploying robust and privacy-preserving machine learning models in real-world applications.",ai
"We address the challenge of identifying causal relationships from observational text data, a task complicated by inherent confounding and the difficulty in explicitly specifying causal mechanisms. Existing methods often rely on simplifying assumptions about the data-generating process or struggle with high-dimensional textual representations. We propose a novel framework, Text-Guided Causal Discovery (TGCD), which leverages pre-trained language models (PLMs) to extract nuanced semantic features and guide the search for causal structures. TGCD incorporates an information bottleneck principle within a structural causal model (SCM), forcing the PLM to learn compressed representations relevant to the causal relationships under investigation. Furthermore, we integrate a causal discovery algorithm based on constraint-based methods, where the PLM-derived features are used to estimate conditional independence relationships. We evaluate TGCD on benchmark causal discovery datasets augmented with text descriptions and demonstrate superior performance compared to state-of-the-art methods, particularly in scenarios with complex confounders and high-dimensional feature spaces. Our results indicate that TGCD effectively exploits textual information to improve the accuracy and robustness of causal structure learning from observational data. The proposed framework offers a promising avenue for automatically inferring causal relationships from unstructured textual sources.",ai
"We address the challenge of automating inductive theorem proving for first-order logic with equality, specifically focusing on theorems requiring complex case splits and auxiliary lemmas for successful derivation. Traditional automated theorem provers often struggle with these theorems due to the combinatorial explosion inherent in exploring possible inference steps and the difficulty in conjecturing necessary lemmas. We propose a novel approach combining connection-based proof search with a reinforcement learning (RL) framework for lemma suggestion. Our method, Connection-Based Reinforcement Lemma Learner (CBRL), iteratively extends the initial conjecture with potential lemmas generated based on observed connection patterns in failed proof attempts. The RL agent learns a policy to prioritize and select candidate lemmas for addition to the proof context, guiding the prover towards successful derivations. We evaluate CBRL on a benchmark suite of challenging inductive theorems, demonstrating a significant improvement in the success rate compared to state-of-the-art automated theorem provers and ablated versions of our approach. Specifically, CBRL solves a significantly higher proportion of theorems, particularly those requiring multiple auxiliary lemmas, showcasing its effectiveness in navigating complex proof search spaces and intelligently guiding lemma discovery. This hybrid approach offers a promising direction for enhancing the capabilities of automated theorem proving systems.",ai
"Graph Neural Networks (GNNs) have demonstrated considerable promise in learning representations for graph-structured data across diverse application domains. However, the performance of GNNs can be significantly affected by the choice of aggregation function, leading to challenges in capturing complex relational dependencies. This work introduces a novel GNN architecture, termed Adaptive Kernel Graph Neural Network (AKGNN), which addresses this limitation by employing adaptive kernel functions within the message passing framework. Specifically, AKGNN utilizes a learnable kernel that dynamically adjusts its receptive field based on the local graph structure and node features. This adaptation enables the model to selectively aggregate information from neighboring nodes, thereby mitigating the effects of over-smoothing and enhancing the discriminative power of node embeddings. We formulate the kernel function as a neural network parameterized by the node and edge attributes, allowing for flexible and expressive representations. Experimental results on several benchmark graph datasets, including node classification and graph classification tasks, demonstrate that AKGNN consistently outperforms existing GNN architectures, including GCN, GraphSAGE, and GAT. The improved performance is attributed to the adaptive nature of the kernel function, which facilitates a more nuanced and informative aggregation of neighborhood information. Furthermore, we provide ablation studies that validate the efficacy of the adaptive kernel in capturing intricate graph dependencies.",ai
"We investigate the problem of efficient exploration in sparse-reward reinforcement learning environments, where the agent receives non-zero reward signals infrequently. Such scenarios often necessitate extensive exploration, rendering traditional reinforcement learning algorithms sample-inefficient. We propose a novel intrinsic motivation framework leveraging a spectral decomposition of the state-space visitation distribution. Specifically, we construct a visitation graph based on observed transitions and employ spectral clustering to identify coherent regions representing potentially unexplored areas. The eigenvector corresponding to the second-smallest eigenvalue of the graph Laplacian is then used to generate an intrinsic reward signal, encouraging the agent to visit states with low connectivity to densely explored regions. This spectral intrinsic motivation (SIM) is combined with an off-policy actor-critic algorithm (SAC) to facilitate learning. Empirical evaluations on benchmark sparse-reward environments, including the Montezuma's Revenge and MiniGrid domains, demonstrate that SIM significantly outperforms existing exploration strategies, such as count-based exploration and random network distillation, in terms of sample efficiency and asymptotic performance. Our results suggest that spectral analysis provides a powerful tool for guiding exploration in challenging reinforcement learning settings with delayed and sparse feedback.",ai
"Learning probabilistic surrogates for partial differential equations remains challenging in data-scarce regimes: neural operators require large amounts of high-fidelity data, while generative approaches typically sacrifice resolution invariance. We formulate flow matching in an infinite-dimensional function space to learn a probabilistic transport that maps low-fidelity approximations to the manifold of high-fidelity PDE solutions via learned residual corrections. We develop a conditional neural operator architecture based on feature-wise linear modulation for flow matching vector fields directly in function space, enabling inference at arbitrary spatial resolutions without retraining. To improve stability and representational control of the induced neural ODE, we parameterize the flow vector field as a sum of a linear operator and a nonlinear operator, combining lightweight linear components with a conditioned Fourier neural operator for expressive, input-dependent dynamics. We then formulate a residual-augmented learning strategy where the flow model learns probabilistic corrections from inexpensive low-fidelity surrogates to high-fidelity solutions, rather than learning the full solution mapping from scratch. Finally, we derive tractable training objectives that extend conditional flow matching to the operator setting with input-function-dependent couplings. To demonstrate the effectiveness of our approach, we present numerical experiments on a range of PDEs, including the 1D advection and Burgers' equation, and a 2D Darcy flow problem for flow through a porous medium. We show that the proposed method can accurately learn solution operators across different resolutions and fidelities and produces uncertainty estimates that appropriately reflect model confidence, even when trained on limited high-fidelity data.",human
"Random matrix theory has played an important role in various areas of pure mathematics, mathematical physics, and machine learning. From a practical perspective of data science, input data are usually normalized prior to processing. Thus, this study investigates the statistical properties of min-max normalized eigenvalues in random matrices. Previously, the effective distribution for such normalized eigenvalues has been proposed. In this study, we apply it to evaluate a scaling law of the cumulative distribution. Furthermore, we derive the residual error that arises during matrix factorization of random matrices. We conducted numerical experiments to verify these theoretical predictions.",human
"We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.",human
"We address the challenge of coordinating decentralized agents with limited communication bandwidth in complex, partially observable environments. Existing multi-agent reinforcement learning (MARL) methods often struggle with scalability due to the exponential growth of the joint action space and the difficulty of learning effective communication protocols under bandwidth constraints. We propose a novel architecture, Communication-Efficient Attentive Policy Gradient (CEAPG), which leverages a learned attention mechanism to selectively transmit information between agents. CEAPG employs a differentiable communication channel and trains agents to attend to the most relevant messages from their neighbors, effectively prioritizing information dissemination. Furthermore, we introduce a regularization term that encourages sparse communication patterns, thereby reducing the overall bandwidth usage. We evaluate CEAPG on a set of challenging cooperative navigation and pursuit-evasion tasks. Experimental results demonstrate that CEAPG outperforms state-of-the-art MARL algorithms, achieving significantly higher team rewards and improved coordination, while maintaining substantially lower communication overhead. Our findings suggest that attentive communication and sparsity-inducing regularization offer a promising approach to scaling MARL to large-scale, bandwidth-limited multi-agent systems.",ai
"**Differentially Private Federated Learning with Improved Convergence via Variance Reduction** Federated learning (FL) enables collaborative model training without direct data sharing, but remains vulnerable to privacy breaches. Differential privacy (DP) offers a rigorous privacy guarantee by injecting noise during training. However, the necessary noise addition often degrades model convergence, particularly in heterogeneous data settings. This work addresses the challenge of accelerating convergence in differentially private FL by incorporating variance reduction techniques. We propose a novel differentially private federated learning algorithm, DP-FedVR, that leverages stochastic variance reduced gradient (SVRG) to minimize the impact of noisy gradient updates. DP-FedVR employs a client-level DP mechanism, injecting Gaussian noise calibrated to the sensitivity of the SVRG update. We provide a rigorous privacy analysis for DP-FedVR, demonstrating that it satisfies (ε, δ)-differential privacy. Empirically, we evaluate DP-FedVR on benchmark datasets for image classification and natural language processing. Results show that DP-FedVR achieves significantly improved convergence rates and higher model accuracy compared to state-of-the-art differentially private FL algorithms, especially when dealing with non-IID data distributions across clients, while provably maintaining privacy guarantees. The reduced variance leads to more stable and accurate model updates, mitigating the negative effects of noise addition.",ai
"We address the problem of identifying causal effects in observational data with unmeasured confounding when auxiliary variables, termed instrumental variables (IVs), are available. Traditional IV methods often suffer from weak instrument bias and sensitivity to instrument invalidity, particularly when dealing with high-dimensional observational settings. To mitigate these limitations, we propose a novel framework leveraging a neural network-based approach to learn representations that simultaneously estimate the causal effect and assess instrument validity. Our method, termed Representation Learning for Causal Effect Estimation with Instrument Validation (RELIV), first learns a representation of the exposure and covariates such that the instrument is strongly associated with the representation of the exposure, while simultaneously minimizing the association between the representation of the covariates and the instrument. Subsequently, we employ a debiased machine learning estimator using the learned representations to infer the causal effect. Furthermore, RELIV incorporates a statistical test for instrument validity based on conditional independence assumptions. Empirical evaluations on both synthetic and semi-synthetic datasets demonstrate that RELIV achieves superior performance compared to state-of-the-art IV methods in terms of bias reduction and accuracy of causal effect estimation, particularly in scenarios with weak or invalid instruments. Ablation studies confirm the effectiveness of the representation learning and instrument validation components in improving causal inference.",ai
"Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.",human
"Recent advances in transformer-based foundation models have made them the default choice for many tasks, but their rapidly growing size makes fitting a full model on a single GPU increasingly difficult and their computational cost prohibitive. Block low-rank (BLR) compression techniques address this challenge by learning compact representations of weight matrices. While traditional low-rank (LR) methods often incur sharp accuracy drops, BLR approaches such as Monarch and BLAST can better capture the underlying structure, thus preserving accuracy while reducing computations and memory footprints. In this work, we use roofline analysis to show that, although BLR methods achieve theoretical savings and practical speedups for single-token inference, multi-token inference often becomes memory-bound in practice, increasing latency despite compiler-level optimizations in PyTorch. To address this, we introduce custom Triton kernels with partial fusion and memory layout optimizations for both Monarch and BLAST. On memory-constrained NVIDIA GPUs such as Jetson Orin Nano and A40, our kernels deliver up to speedups and model size compression over PyTorch dense baselines using CUDA backend and compiler-level optimizations, while supporting various models including Llama-7/1B, GPT2-S, DiT-XL/2, and ViT-B. Our code is available at https://github.com/pabillam/mem-efficient-blr .",human
"Monitoring data transfer performance is a crucial task in scientific computing networks. By predicting performance early in the communication phase, potentially sluggish transfers can be identified and selectively monitored, optimizing network usage and overall performance. A key bottleneck to improving the predictive power of machine learning (ML) models in this context is the issue of class imbalance. This project focuses on addressing the class imbalance problem to enhance the accuracy of performance predictions. In this study, we analyze and compare various augmentation strategies, including traditional oversampling methods and generative techniques. Additionally, we adjust the class imbalance ratios in training datasets to evaluate their impact on model performance. While augmentation may improve performance, as the imbalance ratio increases, the performance does not significantly improve. We conclude that even the most advanced technique, such as CTGAN, does not significantly improve over simple stratified sampling.",human
"### arXiv Abstract **Deep Learning-Based Segmentation of Pulmonary Nodules in Computed Tomography Scans using a Multi-Scale Context Aggregation Network** Accurate and reliable segmentation of pulmonary nodules from computed tomography (CT) scans is crucial for early diagnosis and treatment of lung cancer. However, the high variability in nodule size, shape, and density, along with the presence of anatomical structures with similar characteristics, pose significant challenges to automated segmentation. This work proposes a novel multi-scale context aggregation network (MSCAN) for robust pulmonary nodule segmentation. The MSCAN leverages dilated convolutions with varying dilation rates to capture contextual information at multiple scales, enabling the network to effectively distinguish nodules from surrounding tissues and artifacts. Furthermore, a residual attention mechanism is incorporated to enhance the feature representation of nodules and suppress irrelevant background noise. We evaluated the proposed method on the publicly available LIDC-IDRI dataset. Experimental results demonstrate that the MSCAN achieves state-of-the-art performance, with a Dice Similarity Coefficient (DSC) of 0.84 ± 0.03 and a Sensitivity of 0.88 ± 0.04, surpassing existing methods. These findings suggest that the MSCAN offers a promising approach for automated pulmonary nodule segmentation, potentially improving the efficiency and accuracy of lung cancer screening.",ai
"We address the challenge of identifying causal effects from observational data in the presence of unobserved confounding and selection bias, a pervasive problem across numerous scientific disciplines. Existing methods often rely on strong assumptions about the functional form of relationships or the absence of certain backdoor paths. In this work, we propose a novel instrumental variable (IV) estimation framework, termed ""Residualized Instrumental Regression"" (RIV), which leverages the principle of residual orthogonality to construct valid instruments. RIV operates by first regressing the putative cause on a set of candidate instrumental variables, generating a residual. This residual is then used as an instrument in a two-stage least squares (2SLS) procedure, effectively isolating the causal effect. We demonstrate that RIV provides consistent estimates of the average treatment effect under milder conditions than standard IV methods, particularly when the instrument strength is weak or when there are non-linear confounder effects. Empirical evaluations on synthetic datasets and real-world causal inference benchmarks, including the Infant Health and Development Program (IHDP) dataset, reveal that RIV consistently outperforms several state-of-the-art causal inference techniques in terms of bias reduction and robustness to model misspecification, offering a practical and theoretically sound approach for estimating causal effects in complex observational settings.",ai
"### Abstract The effective representation of knowledge remains a central challenge in symbolic reasoning systems. Existing approaches often struggle with the trade-off between expressiveness and computational tractability, particularly when handling complex relationships and incomplete information. This paper introduces a novel knowledge representation framework, termed Context-Aware Semantic Networks (CASNs), designed to address these limitations. CASNs extend traditional semantic networks by incorporating explicit contextual information within each node and edge. Contexts are formally represented as attributed graphs, allowing for the specification of conditions under which relationships hold and facilitating context-specific reasoning. We present a formal semantics for CASNs, define a tractable inference procedure based on graph homomorphism with context-aware constraints, and prove its soundness and completeness within a well-defined fragment of first-order logic. Empirical evaluation on benchmark knowledge graph completion tasks demonstrates that CASNs significantly improve accuracy compared to state-of-the-art embedding-based methods, especially when dealing with sparse and ambiguous relationships. Furthermore, the explicit representation of context enables explanation generation, providing insight into the reasoning process. The results suggest that CASNs offer a promising direction for developing more robust and interpretable knowledge representation systems.",ai
"The transition of Large Language Models (LLMs) from stochastic chat interfaces to reliable software components necessitates a fundamental re-engineering of interaction paradigms. Current methodologies, predominantly heuristic-based ""prompt engineering,"" fail to provide the deterministic guarantees required for mission-critical applications. We introduce the Meta-Prompting Protocol, a rigorous theoretical framework that formalizes the orchestration of LLMs as a programmable, self-optimizing system. Central to this protocol is the Adversarial Trinity, a tripartite topology comprising a Generator (P), an Auditor (A), and an Optimizer (O). By treating natural language instructions as differentiable variables within a semantic computation graph and utilizing textual critiques as gradients, this architecture mitigates hallucination and prevents model collapse. We demonstrate the theoretical viability of this approach using declarative programming paradigms (DSPy) and automatic textual differentiation (TextGrad), establishing a foundation for ""Observable Software Engineering"" in the era of probabilistic computing.",human
"Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.",human
"While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.",human
"We address the challenge of constructing structured knowledge representations from unstructured textual data, a critical bottleneck in automated reasoning and downstream machine learning tasks. Existing approaches often struggle with scalability and maintaining semantic coherence across diverse and noisy sources. This work introduces a novel graph-based method, Knowledge Refinement via Iterative Embedding Alignment (KR-IEA), which leverages iterative updates of node and relation embeddings within a knowledge graph. The algorithm initializes node embeddings using a pre-trained language model and then refines both node and relation embeddings through a series of alignment steps. These alignment steps are guided by a combination of topological graph structure and textual context, minimizing a contrastive loss function designed to enforce consistency between linked entities and their corresponding textual descriptions. Empirically, KR-IEA demonstrates superior performance in link prediction and entity typing tasks on benchmark datasets such as WordNet and DBpedia, outperforming state-of-the-art methods by a significant margin in terms of Mean Reciprocal Rank (MRR) and Hits@K metrics. Ablation studies confirm the importance of both the iterative refinement process and the integrated use of graph structure and textual context in achieving optimal performance. The resulting knowledge graphs exhibit improved semantic coherence and facilitate more effective knowledge-based reasoning.",ai
"We present a novel approach for automated detection and segmentation of pulmonary nodules in computed tomography (CT) scans, addressing the critical need for improved early diagnosis of lung cancer. Manual analysis of CT scans is time-consuming and prone to inter-observer variability, hindering efficient screening programs. Our method leverages a deep convolutional neural network (CNN) architecture, specifically a three-dimensional U-Net variant, modified with attention mechanisms to focus on regions of interest and enhance nodule feature extraction. The network is trained end-to-end on a large, publicly available dataset of annotated CT scans, incorporating techniques such as data augmentation and transfer learning from pre-trained models on natural images to mitigate overfitting. Evaluation was performed on a held-out test set, achieving a sensitivity of 92.7% at a false positive rate of 4 per scan. Furthermore, we demonstrate a Dice similarity coefficient of 83.2% for nodule segmentation, indicating accurate delineation of nodule boundaries. These results suggest that our method offers a significant improvement over existing techniques and has the potential to substantially enhance the efficiency and accuracy of lung cancer screening.",ai
"Recent advancements in Natural Language Processing (NLP) have been significantly driven by the Transformer architecture. However, challenges remain in effectively capturing long-range dependencies and efficiently processing extensive text sequences. This work investigates a novel adaptation of the Transformer architecture, termed the ""Adaptive Sparse Transformer"" (AST), designed to mitigate these limitations. AST incorporates a dynamic sparsity mechanism within the attention layers, adaptively selecting the most relevant tokens for attention calculation based on contextual embeddings and learned gating functions. This approach reduces the computational complexity from quadratic to approximately linear with respect to sequence length. We evaluate AST on a suite of benchmark NLP tasks, including long document classification (Longformer dataset), question answering (TriviaQA), and neural machine translation (WMT'14 English-German). Experimental results demonstrate that AST achieves comparable or superior performance to existing Transformer variants, while exhibiting a substantial reduction in computational cost, particularly on tasks involving long sequences. Specifically, on the Longformer dataset, AST achieves a 3.2% improvement in F1-score with a 40% reduction in training time compared to the standard Transformer. These findings suggest that AST offers a promising direction for developing more efficient and scalable Transformer-based models for NLP.",ai
"### Abstract This paper addresses the challenge of representing and reasoning with incomplete and inconsistent knowledge in dynamic environments. Current knowledge representation formalisms often struggle with gracefully handling contradictions or adapting to evolving information landscapes, leading to brittle and unreliable inference. We introduce a novel framework, **Dynamic Evidential Knowledge Graphs (DEKG)**, which integrates evidential reasoning with graph-based knowledge representation. DEKG represents knowledge as a graph where nodes represent entities and edges represent relationships, each annotated with an evidential mass function quantifying belief, disbelief, and uncertainty. To manage inconsistencies, we employ Dempster-Shafer theory to fuse conflicting evidence based on source reliability, allowing for the principled resolution of contradictory information. Furthermore, DEKG incorporates a temporal dimension, enabling the tracking of knowledge evolution and the proactive revision of beliefs in response to new observations. We evaluate DEKG on a series of benchmark knowledge base completion tasks and a simulated real-world scenario involving dynamic news feeds and conflicting reports. Results demonstrate that DEKG outperforms traditional knowledge graph embedding techniques and existing evidential reasoning approaches in terms of accuracy, robustness to noise, and adaptability to changing information landscapes, showing a significant improvement in handling incomplete and contradictory information.",ai
"We address the problem of scalable and efficient learning on large-scale graphs with complex structural dependencies, where traditional graph neural networks (GNNs) often suffer from computational bottlenecks due to message passing across the entire graph. To mitigate this, we propose a novel subgraph-based GNN architecture, termed SubG-GNN, which operates on adaptively sampled subgraphs. SubG-GNN employs a multi-stage process: (1) a graph coarsening module that identifies structurally important nodes based on spectral clustering and generates representative seed nodes; (2) a subgraph sampling strategy that expands these seed nodes into localized subgraphs using random walk with restart; and (3) a GNN-based learning module that aggregates information within each subgraph and subsequently combines subgraph-level representations for node classification or graph classification tasks. Furthermore, we introduce a contrastive learning objective to encourage the subgraphs to capture diverse and complementary information about the global graph structure. Empirical evaluations on several benchmark datasets, including OGBN-Products, OGBN-Arxiv, and Reddit, demonstrate that SubG-GNN achieves competitive or superior performance compared to state-of-the-art GNN models while significantly reducing computational costs and memory consumption. Specifically, we observe a speedup of up to 5x during training and a reduction in memory usage of up to 3x without sacrificing accuracy. These results highlight the effectiveness of subgraph-based learning for addressing scalability challenges in large-scale graph representation learning.",ai
"The source detection problem arises when an epidemic process unfolds over a contact network, and the objective is to identify its point of origin, i.e., the source node. Research on this problem began with the seminal work of Shah and Zaman in 2010, who formally defined it and introduced the notion of rumor centrality. With the emergence of Graph Neural Networks (GNNs), several studies have proposed GNN-based approaches to source detection. However, some of these works lack methodological clarity and/or are hard to reproduce. As a result, it remains unclear (to us, at least) whether GNNs truly outperform more traditional source detection methods across comparable settings. In this paper, we first review existing GNN-based methods for source detection, clearly outlining the specific settings each addresses and the models they employ. Building on this research, we propose a principled GNN architecture tailored to the source detection task. We also systematically investigate key questions surrounding this problem. Most importantly, we aim to provide a definitive assessment of how GNNs perform relative to other source detection methods. Our experiments show that GNNs substantially outperform all other methods we test across a variety of network types. Although we initially set out to challenge the notion of GNNs as a solution to source detection, our results instead demonstrate their remarkable effectiveness for this task. We discuss possible reasons for this strong performance. To ensure full reproducibility, we release all code and data on GitHub. Finally, we argue that epidemic source detection should serve as a benchmark task for evaluating GNN architectures.",human
"We address the critical challenge of training machine learning models on sensitive data while preserving individual privacy. Specifically, we investigate privacy-preserving training of sequence-to-sequence models, a class of architectures widely used in natural language processing tasks. Traditional approaches, such as differential privacy (DP), often introduce significant performance degradation when applied to complex models. To mitigate this, we propose a novel two-stage training framework incorporating a differentially private teacher model and a student model trained via knowledge distillation. The teacher model is trained with DP-SGD and subsequently used to generate synthetic, privacy-preserving labels for the student. We introduce a tailored loss function for the student, combining standard cross-entropy with a consistency regularization term to encourage alignment with the teacher's predictions while minimizing direct exposure to the original sensitive data. Empirical evaluation on benchmark NLP datasets demonstrates that our approach achieves substantial privacy guarantees, quantified by epsilon and delta values, while maintaining significantly higher utility compared to directly training a differentially private sequence-to-sequence model. Ablation studies validate the effectiveness of each component of the proposed framework. The results suggest that our approach offers a practical and effective solution for privacy-preserving training of sequence-to-sequence models, balancing privacy and utility in sensitive NLP applications.",ai
"We investigate the problem of coordinating decentralized agents with limited communication bandwidth in complex, partially observable environments. Existing methods often rely on centralized training or require agents to learn explicit communication protocols, which can be computationally expensive and struggle to generalize to unseen scenarios. We propose a novel approach, the Differentiable Inter-Agent Coordination Network (DIACN), which integrates implicit communication within a differentiable attention mechanism. Each agent learns to attend to the states and actions of other agents, modulating its behavior based on the perceived relevance of its teammates. This attention mechanism is optimized end-to-end during training, allowing agents to implicitly discover effective coordination strategies without explicit message passing. We evaluated DIACN on a series of challenging cooperative navigation and resource allocation tasks. Empirical results demonstrate that DIACN significantly outperforms state-of-the-art decentralized multi-agent reinforcement learning algorithms, particularly in scenarios with restricted communication and noisy observations. Furthermore, analysis of the learned attention weights reveals that DIACN agents develop interpretable coordination patterns, dynamically adjusting their focus on different teammates based on the task demands.",ai
"Copper nanoparticles (Cu NPs) have a broad applicability, yet their synthesis is sensitive to subtle changes in reaction parameters. This sensitivity, combined with the time- and resource-intensive nature of experimental optimization, poses a major challenge in achieving reproducible and size-controlled synthesis. While Machine Learning (ML) shows promise in materials research, its application is often limited by scarcity of large high-quality experimental data sets. This study explores ML to predict the size of Cu NPs from microwave-assisted polyol synthesis using a small data set of 25 in-house performed syntheses. Latin Hypercube Sampling is used to efficiently cover the parameter space while creating the experimental data set. Ensemble regression models, built with the AMADEUS framework, successfully predict particle sizes with high accuracy (), outperforming classical statistical approaches (). Overall, this study highlights that, for lab-scale synthesis optimization, high-quality small datasets combined with classical, interpretable ML models outperform traditional statistical methods and are fully sufficient for quantitative synthesis prediction. This approach provides a sustainable and experimentally realistic pathway toward data-driven inorganic synthesis design.",human
"The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.",human
"Drama script continuation requires models to maintain character consistency, advance plot coherently, and preserve dramatic structurecapabilities that existing benchmarks fail to evaluate comprehensively. We present DramaBench, the first large-scale benchmark for evaluating drama script continuation across six independent dimensions: Format Standards, Narrative Efficiency, Character Consistency, Emotional Depth, Logic Consistency, and Conflict Handling. Our framework combines rulebased analysis with LLM-based labeling and statistical metrics, ensuring objective and reproducible evaluation. We conduct comprehensive evaluation of 8 state-of-the-art language models on 1,103 scripts (8,824 evaluations total), with rigorous statistical significance testing (252 pairwise comparisons, 65.9% significant) and human validation (188 scripts, substantial agreement on 3/5 dimensions). Our ablation studies confirm all six dimensions capture independent quality aspects (mean | r | = 0.020). DramaBench provides actionable, dimensionspecific feedback for model improvement and establishes a rigorous standard for creative writing evaluation.",human
"The rise of large language models (LLMs) has introduced a new type of programming: natural language programming. By writing prompts that direct LLMs to perform natural language processing, code generation, reasoning, etc., users are writing code in natural language -- natural language code -- for the LLM to execute. An emerging area of research enables interoperability between natural language code and formal languages such as Python. We present a novel programming abstraction, shared program state, that removes the manual work required to enable interoperability between natural language code and program state. With shared program state, programmers can write natural code that directly writes program variables, computes with program objects, and implements control flow in the program. We present a schema for specifying natural function interfaces that extend programming systems to support natural code and leverage this schema to specify shared program state as a natural function interface. We implement shared program state in the Nightjar programming system. Nightjar enables programmers to write Python programs that contain natural code that shares the Python program state. We show that Nightjar programs achieve comparable or higher task accuracy than manually written implementations (+4-19%), while decreasing the lines of code by 39.6% on average. The tradeoff to using Nightjar is that it may incur runtime overhead (0.4-4.3x runtime of manual implementations).",human
"We address the problem of spurious correlations in natural language processing (NLP) tasks, which often lead to poor generalization performance on out-of-distribution (OOD) data. Specifically, we focus on mitigating confounding bias in text classification, where latent confounders influence both the input text and the target label. To this end, we propose a novel causal inference framework leveraging backdoor adjustment for unbiased estimation of treatment effects. Our approach, termed Backdoor Adjustment for Text Classification (BATC), explicitly models the causal relationships between text features, latent confounders, and the classification outcome using a structural causal model (SCM). BATC then utilizes learned representations of observed text features to estimate and control for the influence of latent confounders. This is achieved by training a neural network to predict the treatment effect (the classification outcome) given the text features, while simultaneously minimizing the dependence between the learned representations and estimated confounders. Experiments conducted on benchmark datasets demonstrate that BATC significantly outperforms existing methods in terms of OOD generalization performance, as measured by accuracy and F1-score. Moreover, ablation studies confirm the efficacy of the backdoor adjustment strategy in removing spurious correlations, leading to more robust and reliable classification models.",ai
"Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.",human
"We address the problem of training machine learning models on sensitive datasets while guaranteeing rigorous privacy protection. Current differentially private (DP) training methods often suffer from significant utility degradation, particularly in high-dimensional feature spaces and complex model architectures. This paper introduces a novel approach based on functional mechanism composition coupled with adaptive gradient clipping tailored to the sensitivity of individual data points. Specifically, we leverage a Rényi differential privacy (RDP) framework to track the privacy budget consumption of each iteration and dynamically adjust the clipping norm based on the observed distribution of gradient norms. Furthermore, we employ a functional mechanism that directly perturbs the parameters of the model, rather than the gradients themselves, leading to improved privacy-utility trade-offs. Empirical evaluation on benchmark datasets, including CIFAR-10 and MNIST, demonstrates that our method achieves state-of-the-art performance compared to existing DP-SGD variants, offering a substantial improvement in accuracy under comparable privacy budgets. Ablation studies validate the effectiveness of adaptive gradient clipping and the benefits of functional mechanism composition in enhancing privacy and model utility. The proposed approach offers a practical and effective solution for training privacy-preserving machine learning models in real-world applications.",ai
"Diffusion models excel at generating high-quality, diverse samples, yet they risk memorizing training data when overfit to the training objective. We analyze the distinctions between memorization and generalization in diffusion models through the lens of representation learning. By investigating a two-layer ReLU denoising autoencoder (DAE), we prove that (i) memorization corresponds to the model storing raw training samples in the learned weights for encoding and decoding, yielding localized ""spiky"" representations, whereas (ii) generalization arises when the model captures local data statistics, producing ""balanced"" representations. Furthermore, we validate these theoretical findings on real-world unconditional and text-to-image diffusion models, demonstrating that the same representation structures emerge in deep generative models with significant practical implications. Building on these insights, we propose a representation-based method for detecting memorization and a training-free editing technique that allows precise control via representation steering. Together, our results highlight that learning good representations is central to novel and meaningful generative modeling.",human
"We address the challenge of automated pulmonary nodule detection in computed tomography (CT) scans, a critical task for early lung cancer diagnosis. Manual review of CT volumes is time-consuming and prone to inter-observer variability, necessitating robust and efficient automated solutions. This work introduces a novel deep learning framework leveraging a multi-scale, attention-guided convolutional neural network (CNN) for enhanced nodule characterization. The architecture incorporates a 3D CNN backbone for volumetric feature extraction, followed by a series of attention modules designed to selectively emphasize relevant regions associated with potential nodules at varying scales. These attention-modulated features are then aggregated and fed into a classification head to predict the presence and location of pulmonary nodules. We evaluate our proposed method on the publicly available Lung Image Database Consortium image collection (LIDC-IDRI). The experimental results demonstrate superior performance compared to state-of-the-art methods, achieving a competitive free-response operating characteristic (FROC) score of 0.88 at 4 false positives per scan. Further analysis reveals the attention mechanism effectively highlights subtle nodule characteristics, contributing to improved sensitivity and reduced false positive rates, thus showcasing the potential of our approach for clinical deployment.",ai
"While end-to-end (E2E) automatic speech recognition (ASR) models excel at general transcription, they struggle to recognize rare or unseen named entities (e.g., contact names, locations), which are critical for downstream applications like virtual assistants. In this paper, we propose a contextual biasing method for attention based encoder decoder (AED) models using a list of candidate named entities. Instead of predicting only the next token, we simultaneously predict multiple future tokens, enabling the model to ""peek into the future"" and score potential candidate entities in the entity list. Moreover, our approach leverages the multi-token prediction logits directly without requiring additional entity encoders or cross-attention layers, significantly reducing architectural complexity. Experiments on Librispeech demonstrate that our approach achieves up to 50.34% relative improvement in named entity word error rate compared to the baseline AED model.",human
"Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word privée, meaning ""private."" PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.",human
"Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.",human
"We present a novel methodology for the automated detection and segmentation of pulmonary nodules in computed tomography (CT) scans, addressing the critical need for improved early diagnosis of lung cancer. Manual analysis of CT images is time-consuming and susceptible to inter-observer variability, hindering efficient screening programs. Our approach leverages a three-dimensional convolutional neural network (3D-CNN) architecture incorporating a novel attention mechanism to focus on regions of interest while suppressing irrelevant background information. Specifically, we introduce a spatial attention module integrated into the 3D-CNN that adaptively weights feature maps based on their relevance to nodule presence. The network is trained end-to-end on a large, publicly available dataset of chest CT scans containing both benign and malignant nodules. We evaluate the performance of our method using standard metrics including sensitivity, specificity, and F1-score, comparing against established nodule detection algorithms. Experimental results demonstrate a significant improvement in nodule detection accuracy, achieving a sensitivity of 92.3% at a false positive rate of 0.25 per scan, surpassing existing state-of-the-art techniques. Furthermore, the segmentation accuracy, measured using the Dice similarity coefficient, reaches 84.7%, indicating precise delineation of nodule boundaries. This enhanced performance has the potential to significantly improve the efficiency and effectiveness of lung cancer screening programs.",ai
"Privacy-preserving machine learning (PPML) addresses the critical challenge of training models on sensitive data without compromising individual privacy. A central problem in PPML is the inherent trade-off between model utility and privacy guarantees. Differential privacy (DP) offers a rigorous mathematical framework for quantifying privacy loss, but achieving high accuracy with strong DP guarantees remains a significant hurdle. This work introduces a novel approach, Differentially Private Federated Averaging with Calibrated Noise Allocation (DP-FedAvg-CNA), which optimizes noise addition in federated learning. DP-FedAvg-CNA dynamically calibrates the amount of noise added to each client's update based on the client's data heterogeneity and contribution to the global model. We leverage a Bayesian framework to estimate these quantities without compromising privacy. Empirical evaluation on benchmark datasets demonstrates that DP-FedAvg-CNA achieves significantly improved model accuracy compared to standard DP-FedAvg and other DP-optimized federated learning algorithms under comparable privacy budgets. Specifically, we observe a relative accuracy improvement of up to 15% while maintaining a rigorous ε-δ differential privacy guarantee. These results highlight the potential of calibrated noise allocation for enhancing the utility-privacy trade-off in PPML.",ai
"We investigate the application of Transformer architectures to several benchmark natural language processing tasks, focusing on sequence-to-sequence transduction and text classification. Despite their initial success in machine translation, the computational demands associated with quadratic self-attention complexity pose challenges for deployment on long input sequences and in resource-constrained environments. This work explores two strategies for mitigating these limitations: sparse attention mechanisms and knowledge distillation. We evaluate variants of the Transformer incorporating sparse attention patterns that reduce the computational cost to linear or sub-quadratic complexity in sequence length, specifically examining the Longformer and Reformer models. Furthermore, we employ knowledge distillation to transfer the knowledge of a large, pre-trained Transformer model to smaller, more efficient student models. Empirical results on the GLUE benchmark and WMT’14 English-German translation task demonstrate that sparse attention enables effective processing of longer sequences while maintaining competitive performance. Knowledge distillation yields smaller models with reduced inference latency and minimal performance degradation, offering a viable pathway for deploying Transformer-based models on edge devices. The findings highlight the potential of these techniques for improving the efficiency and scalability of Transformer models for a wider range of NLP applications.",ai
"Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%",human
"We address the problem of distributional robustness in neural text classification, specifically focusing on adversarial examples generated via semantic perturbations. Current training regimes often fail to generalize to unseen adversarial attacks due to overfitting to specific perturbation strategies. We propose a novel Robust Optimization via Contextual Augmentation (ROCA) method, which utilizes a contextual masked language model to generate diverse and plausible semantic paraphrases of the input text during training. ROCA iteratively identifies the most challenging augmentations within a mini-batch and optimizes the model against these hard examples. This dynamically adjusts the training distribution to encompass a wider range of potential adversarial perturbations, thereby enhancing robustness. We evaluate ROCA on multiple benchmark text classification datasets (e.g., SST-2, AG News) against various state-of-the-art adversarial attack methods including TextFooler, BAE, and PWWS. Experimental results demonstrate that ROCA significantly improves the model's adversarial robustness compared to standard adversarial training and other data augmentation techniques. Specifically, ROCA achieves an average increase of 5-10% in classification accuracy under strong adversarial attacks, while maintaining comparable performance on clean data. Furthermore, analysis reveals that ROCA encourages the model to learn more robust semantic representations, leading to better generalization across different attack strategies.",ai
"We address the problem of training machine learning models on sensitive user data while guaranteeing differential privacy (DP). Existing approaches often compromise utility for strong privacy guarantees, particularly in complex tasks such as natural language processing. This paper introduces a novel framework, Differentially Private Federated Averaging with Adaptive Clipping (DP-FedAvg-AC), which enhances the utility of federated learning under DP constraints. DP-FedAvg-AC employs a client-side gradient clipping mechanism where the clipping norm is dynamically adjusted based on the observed gradient distribution within each round. This adaptive clipping strategy reduces the noise added for privacy preservation by avoiding overly aggressive clipping, thereby preserving valuable signal in the gradients. Furthermore, we incorporate a privacy accounting method that leverages Rényi differential privacy to provide tighter privacy bounds compared to traditional moment accountant methods. We evaluate DP-FedAvg-AC on a variety of NLP tasks, including sentiment classification and text generation, using benchmark datasets. Experimental results demonstrate that DP-FedAvg-AC achieves significantly improved utility compared to state-of-the-art DP-FedAvg baselines, while maintaining comparable or stronger privacy guarantees. Specifically, we observe an average improvement of 5-10% in downstream task accuracy across different privacy budgets (ε), indicating a substantial improvement in the trade-off between privacy and utility.",ai
"We address the challenge of automated detection and segmentation of subtle pulmonary nodules in chest CT scans, a critical task for early lung cancer diagnosis. Manual analysis of these scans is time-consuming and prone to inter-observer variability, highlighting the need for robust and automated solutions. This work proposes a novel deep learning architecture, termed Multi-Scale Attentive Segmentation Network (MSASN), designed to leverage contextual information at varying resolutions and enhance feature representation through attention mechanisms. MSASN incorporates a 3D convolutional encoder-decoder structure with skip connections to preserve fine-grained details. Crucially, multi-scale feature maps extracted from the encoder are fed into attention modules that selectively highlight relevant nodule-specific features while suppressing background noise. We evaluated MSASN on the publicly available LUNA16 dataset, achieving a competitive detection rate of 91.2% at 8 false positives per scan, outperforming several state-of-the-art methods. Furthermore, the segmentation performance, measured by Dice Similarity Coefficient (DSC), reached 82.5%, demonstrating the network's ability to accurately delineate nodule boundaries. The results suggest that MSASN provides a promising approach for automated pulmonary nodule analysis, potentially improving diagnostic accuracy and efficiency in clinical practice.",ai
"Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANELÉ, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANELÉ matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems.",human
"Autonomous multi-agent systems are fundamentally fragile: they struggle to solve the Hayekian Information problem (eliciting dispersed private knowledge) and the Hurwiczian Incentive problem (aligning local actions with global objectives), making coordination computationally intractable. I introduce Mechanism-Based Intelligence (MBI), a paradigm that reconceptualizes intelligence as emergent from the coordination of multiple ""brains"", rather than a single one. At its core, the Differentiable Price Mechanism (DPM) computes the exact loss gradient $(N)$) with the number of agents, bypassing the combinatorial complexity of Dec-POMDPs and is empirically 50x faster than Model-Free Reinforcement Learning. By structurally aligning agent self-interest with collective objectives, it provides a provably efficient, auditable and generalizable approach to coordinated, trustworthy and scalable multi-agent intelligence grounded in economic principles.",human
"We investigate the problem of constructing structured knowledge representations from unstructured textual data, specifically focusing on the extraction and organization of relational facts. Current methods often struggle with scalability and robustness in the face of noisy and ambiguous text, particularly when dealing with long-range dependencies and nuanced semantic relationships. To address these limitations, we propose a novel approach combining distributional semantics with graph-based knowledge consolidation. Our method leverages pre-trained language models to encode textual entities and relations into high-dimensional embeddings. These embeddings are then used to construct a knowledge graph, where nodes represent entities and edges represent relationships. A key innovation is the introduction of a graph neural network (GNN) architecture designed to iteratively refine entity and relation embeddings based on local and global graph structure. We evaluate our method on benchmark knowledge graph completion and relation extraction tasks, demonstrating significant improvements in accuracy and efficiency compared to existing state-of-the-art techniques. Furthermore, we show that our approach exhibits enhanced robustness to noisy data and improved generalization across different domains. The resulting knowledge graphs exhibit superior structural coherence and semantic consistency, facilitating more effective knowledge retrieval and reasoning.",ai
"Despite the state-of-the-art performance of Large Language Models (LLMs) achieved on many tasks, their massive scale often leads to high computational and environmental costs, limiting their accessibility. Parameter-efficient fine-tuning (PEFT) methods address this challenge by reducing the number of trainable parameters while maintaining strong downstream performance. Despite the increased development in PEFT methods, current evaluations remain limited (in terms of evaluated models and datasets) and difficult to reproduce. To bridge this gap, we introduce PEFT-Bench, a unified end-to-end benchmark for evaluating diverse PEFT methods on autoregressive LLMs. We demonstrate its usage across 27 NLP datasets and 6 PEFT methods. To account for different PEFT training and inference factors, we also introduce the PEFT Soft Score Penalties (PSCP) metric, which takes trainable parameters, inference speed, and training memory usage into account.",human
"Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.",human
"We address the challenge of identifying causal relationships from observational text data, a persistent problem in computational linguistics due to inherent confounds and the absence of controlled interventions. Current methodologies often rely on statistical associations or heuristic-based pattern matching, which are susceptible to spurious correlations and limited in capturing nuanced causal mechanisms. We propose a novel framework leveraging causal discovery algorithms, specifically a modified version of the PC algorithm, integrated with contextualized language models. This approach, termed Contextualized PC (CPC), incorporates embeddings from pre-trained language models to represent semantic relationships between textual variables, thereby enriching the conditional independence tests crucial for causal structure learning. We evaluate CPC on a benchmark dataset of causal narratives, demonstrating its superior performance compared to baseline methods, including standard PC and Granger causality tests. CPC achieves a substantial improvement in Structural Hamming Distance (SHD), reducing error by 15-20% while simultaneously increasing precision in identifying true causal edges. Furthermore, we conduct ablation studies demonstrating the effectiveness of contextual embeddings in mitigating the impact of confounding variables present in the data. These findings highlight the potential of CPC for more reliable causal inference from unstructured textual sources.",ai
"### Abstract Knowledge representation is a fundamental challenge in creating intelligent systems capable of reasoning and problem-solving. A significant problem lies in representing complex, hierarchical, and context-dependent knowledge in a manner that facilitates efficient inference and adaptation. Existing approaches, such as ontologies and semantic networks, often struggle with scalability and the inherent ambiguity of natural language. This paper introduces a novel knowledge representation framework, termed Contextualized Relational Knowledge Graphs (CRKG), designed to address these limitations. CRKGs extend traditional knowledge graphs by incorporating contextual embedding spaces associated with each entity and relation. These spaces capture the semantic nuances and dependencies arising from the context in which the knowledge is expressed. A graph embedding technique, leveraging a transformer-based architecture, is used to construct these contextual embeddings. We evaluate CRKG on a benchmark question answering dataset requiring complex reasoning over hierarchical knowledge. Results demonstrate a significant improvement in accuracy compared to state-of-the-art knowledge graph embedding methods and ontology-based systems. Specifically, CRKG achieves a 15% increase in F1-score, demonstrating its ability to effectively capture and leverage contextual information for enhanced reasoning performance. This work suggests that context-aware knowledge representation offers a promising avenue for building more robust and adaptable intelligent systems.",ai
"Inferring causal relationships from observational text data is a challenging problem due to the presence of confounding variables and selection biases inherent in natural language. This paper addresses the task of identifying causal effects between events described in textual narratives, focusing on estimating the Average Treatment Effect (ATE) despite the lack of explicit intervention data. We propose a novel framework, Contrastive Causal Language Modeling (CCLM), which leverages pre-trained language models to generate counterfactual narratives by manipulating key causal variables. CCLM employs a contrastive learning objective, training the language model to distinguish between factual and counterfactual narratives conditioned on specific causal interventions. This allows for the estimation of the ATE by comparing model outputs across different intervention scenarios. We evaluate CCLM on a benchmark dataset of causal relations extracted from story corpora. Our results demonstrate that CCLM significantly outperforms existing methods for causal effect estimation based on correlation analysis and traditional observational causal inference techniques. Ablation studies confirm the importance of both the contrastive learning objective and the use of pre-trained language models in accurately capturing causal dependencies within textual narratives. Specifically, CCLM achieves a 15% improvement in ATE estimation accuracy compared to the strongest baseline, demonstrating its ability to mitigate the impact of confounding and provide more reliable causal inferences from observational text.",ai
"--- We address the challenge of representing complex, multi-relational knowledge graphs (KGs) in a manner that balances expressivity with computational tractability for downstream reasoning tasks. Existing methods often struggle to capture higher-order relationships or incur substantial computational overhead due to their complexity. We propose a novel tensor decomposition-based approach, incorporating a hierarchical regularization scheme, to learn distributed representations of KG entities and relations. This method, termed Hierarchical Regularized Tensor Decomposition (HRTD), decomposes the KG adjacency tensor into a set of core tensors and factor matrices, while simultaneously imposing L1 and L2 regularization at different levels of the hierarchy. The L1 regularization promotes sparsity in the factor matrices, facilitating feature selection and reducing model size. The L2 regularization stabilizes training and prevents overfitting, particularly crucial for large and noisy KGs. We evaluate HRTD on several benchmark KG completion datasets, including FB15k-237 and WN18RR. Experimental results demonstrate that HRTD achieves competitive performance in link prediction accuracy compared to state-of-the-art embedding-based methods, while exhibiting significantly improved efficiency in terms of parameter count and training time. Furthermore, we analyze the learned embeddings and show that the hierarchical regularization effectively captures meaningful structural information within the KG, leading to more interpretable and robust representations.",ai
"While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical approach for effectively adapting LMs to a diverse range of downstream tasks. However, existing PEFT methods face two primary challenges: (1) High resource cost. Although PEFT methods significantly reduce resource demands compared to full fine-tuning, it still requires substantial time and memory, making it impractical in resource-constrained environments. (2) Parameter dependency. PEFT methods heavily rely on updating a subset of parameters associated with LMs to incorporate task-specific knowledge. Yet, due to increasing competition in the LMs landscape, many companies have adopted closed-source policies for their leading models, offering access only via Application Programming Interface (APIs). Whereas, the expense is often cost-prohibitive and difficult to sustain, as the fine-tuning process of LMs is extremely slow. Even if small models perform far worse than LMs in general, they can achieve superior results on particular distributions while requiring only minimal resources. Motivated by this insight, we propose Easy Adaptation (EA), which designs Specific Small Models (SSMs) to complement the underfitted data distribution for LMs. Extensive experiments show that EA matches the performance of PEFT on diverse tasks without accessing LM parameters, and requires only minimal resources.",human
"Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by ""Context Explosion"", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.",human
"We investigate the problem of emergent communication protocols in decentralized multi-agent reinforcement learning (MARL) environments characterized by partial observability and sparse rewards. Specifically, we address the challenge of agents learning to coordinate effectively without explicit communication channels in scenarios requiring complex temporal credit assignment. We introduce a novel differentiable communication architecture, the Temporally Coordinated Recurrent Inference Network (TCRIN), which integrates recurrent neural networks with an attentional mechanism designed to dynamically prioritize and aggregate information from neighboring agents across multiple timesteps. This allows agents to infer latent communication signals from observed actions and rewards of their peers, effectively mitigating the vanishing gradient problem associated with long-range dependencies. We evaluate TCRIN in challenging cooperative navigation and resource allocation tasks, demonstrating significant improvements in team performance and communication efficiency compared to state-of-the-art MARL algorithms, including independent learners and methods employing explicit communication schemes. Our findings show that TCRIN facilitates the emergence of more robust and scalable communication protocols, enabling agents to adapt to dynamic environments and achieve superior coordination with minimal communication overhead. Moreover, analysis of the learned attention weights reveals the development of interpretable communication strategies that are tailored to the specific demands of each task.",ai
"We address the challenge of distributional robustness in neural machine translation (NMT) when faced with noisy or adversarial input. Specifically, we investigate a novel robust optimization framework that incorporates a data augmentation strategy guided by gradient-based adversarial examples. Our approach, termed ""Adversarially-Augmented Robust Optimization (AARO)"", minimizes the worst-case expected loss over a carefully constructed uncertainty set, where the uncertainty set is defined by perturbations to the input embeddings generated via projected gradient descent. Unlike prior work which relies on fixed perturbation magnitudes, AARO adaptively adjusts the perturbation size based on the vulnerability of the model to specific input tokens, thereby focusing on the most impactful adversarial attacks. We evaluate AARO on standard NMT benchmarks, including WMT'14 English-German and English-French. Experimental results demonstrate that AARO significantly improves the robustness of NMT models against both white-box and black-box adversarial attacks, leading to a relative BLEU score improvement of up to 3.5% compared to standard training and other robust optimization baselines under adversarial settings. Furthermore, AARO exhibits improved generalization performance on naturally occurring noisy datasets, suggesting that the proposed method effectively mitigates overfitting to spurious correlations in the training data.",ai
"We address the challenge of constructing robust and interpretable knowledge representations from sparse and heterogeneous data sources, a critical problem in domains characterized by incomplete observations and diverse feature modalities. Existing approaches often struggle to effectively integrate information across varying levels of abstraction and exhibit limited capacity for reasoning under uncertainty. We propose a novel framework, Hierarchical Relational Embedding (HRE), which combines relational graph convolutional networks with variational autoencoders to learn a hierarchical embedding space that captures both explicit relational structure and implicit feature dependencies. HRE iteratively refines entity embeddings by aggregating information from neighboring entities and simultaneously infers latent hierarchical relationships through a variational objective. This allows for the representation of knowledge at multiple levels of granularity, facilitating both fine-grained entity classification and coarse-grained relationship prediction. Empirical evaluations on benchmark knowledge graph completion and entity classification datasets demonstrate that HRE significantly outperforms state-of-the-art methods, particularly in scenarios with noisy and incomplete data. Furthermore, visualization and analysis of the learned hierarchical embedding space reveal that HRE effectively captures meaningful semantic relationships between entities, enabling improved interpretability and facilitating downstream reasoning tasks.",ai
"Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of ""concept"" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents ""concepts"" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.",human
"We investigate the problem of decentralized learning in multi-agent systems characterized by non-stationary environments and limited communication bandwidth. Traditional reinforcement learning approaches often falter in such scenarios due to the curse of dimensionality and the challenge of maintaining consistent state representations across agents. This work introduces a novel algorithm, Federated Value Decomposition with Communication Compression (FVD-CC), which combines value decomposition techniques with federated learning principles and leverages stochastic quantization for efficient communication. Specifically, FVD-CC decomposes the global value function into agent-specific value functions and employs a federated averaging scheme to aggregate local policy updates. To mitigate communication bottlenecks, we implement a stochastic quantization method that drastically reduces the size of transmitted gradients. Empirical evaluation on a suite of challenging cooperative navigation tasks demonstrates that FVD-CC outperforms state-of-the-art decentralized learning algorithms, achieving superior coordination and convergence rates. Furthermore, we show that FVD-CC maintains comparable performance even with significantly reduced communication bandwidth, making it a viable solution for resource-constrained multi-agent systems. The results highlight the effectiveness of combining value decomposition, federated learning, and communication compression for addressing the challenges of decentralized learning in complex, dynamic environments.",ai
"Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.",human
"Graph Neural Networks (GNNs) have emerged as a powerful paradigm for learning representations of graph-structured data, demonstrating notable success in various domains. However, training GNNs effectively, particularly on large graphs or with complex architectures, remains a significant challenge due to issues such as over-smoothing, vanishing gradients, and high computational costs. This paper addresses the problem of improving GNN training efficiency and performance through a novel adaptive layer aggregation technique. Our method, Adaptive Graph Aggregation (AGA), dynamically adjusts the receptive field of each node during training based on the observed gradient flow and node similarity. AGA utilizes a learnable aggregation weight assigned to each layer’s output, allowing the network to prioritize information from relevant neighbors and distant nodes while mitigating the impact of noisy or redundant information. Empirically, we evaluate AGA on several benchmark graph datasets for node classification and graph classification tasks. Our results demonstrate that AGA consistently outperforms existing GNN models, including Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Graph Isomorphism Networks (GINs), achieving significant improvements in accuracy and convergence speed. Furthermore, we provide ablation studies that validate the effectiveness of the adaptive aggregation mechanism and its contribution to mitigating over-smoothing. AGA provides a robust and efficient approach to training GNNs, enhancing their applicability to real-world graph-based problems.",ai
"Differential privacy (DP) provides a rigorous mathematical framework for protecting the privacy of individuals in statistical analyses. However, applying DP in machine learning (ML) often entails a significant trade-off between privacy guarantees and model utility, particularly in high-dimensional settings. This paper addresses the problem of training accurate and differentially private ML models for tabular data, focusing on scenarios where feature interactions are crucial for predictive performance. We introduce a novel algorithm, Differentially Private Feature-Aware Aggregation (DP-FAA), which leverages a hierarchical aggregation strategy based on feature similarity to reduce the sensitivity of the learning process. DP-FAA iteratively constructs groups of features with high mutual information, applies differentially private aggregation within these groups, and then learns a model on the aggregated representation. This approach allows us to effectively capture feature interactions while minimizing the privacy cost associated with high-dimensional data. We theoretically analyze the privacy guarantees of DP-FAA and empirically evaluate its performance on several benchmark datasets. Our results demonstrate that DP-FAA consistently outperforms existing state-of-the-art differentially private ML algorithms, achieving significantly higher utility for comparable privacy budgets. Furthermore, we provide an ablation study to illustrate the effectiveness of the feature-aware aggregation strategy.",ai
"Machine learning is profoundly reshaping molecular and materials modeling; however, given the vast scale of chemical space (10^30-10^60), it remains an open scientific question whether models can achieve convergent learning across this space. We introduce a Dual-Axis Representation-Complete Convergent Learning (RCCL) strategy, enabled by a molecular representation that integrates graph convolutional network (GCN) encoding of local valence environments, grounded in modern valence bond theory, together with no-bridge graph (NBG) encoding of ring/cage topologies, providing a quantitative measure of chemical-space coverage. This framework formalizes representation completeness, establishing a principled basis for constructing datasets that support convergent learning for large models. Guided by this RCCL framework, we develop the FD25 dataset, systematically covering 13,302 local valence units and 165,726 ring/cage topologies, achieving near-complete combinatorial coverage of organic molecules with H/C/N/O/F elements. Graph neural networks trained on FD25 exhibit representation-complete convergent learning and strong out-of-distribution generalization, with an overall prediction error of approximately 1.0 kcal/mol MAE across external benchmarks. Our results establish a quantitative link between molecular representation, structural completeness, and model generalization, providing a foundation for interpretable, transferable, and data-efficient molecular intelligence.",human
"Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, Tool comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on Tool, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.",human
"Multi-view learning (MVL) is an emerging field in machine learning that focuses on improving generalization performance by leveraging complementary information from multiple perspectives or views. Various multi-view support vector machine (MvSVM) approaches have been developed, demonstrating significant success. Moreover, these models face challenges in effectively capturing decision boundaries in high-dimensional spaces using the kernel trick. They are also prone to errors and struggle with view inconsistencies, which are common in multi-view datasets. In this work, we introduce the multiview twin restricted kernel machine (TMvRKM), a novel model that integrates the strengths of kernel machines with the multiview framework, addressing key computational and generalization challenges associated with traditional kernel-based approaches. Unlike traditional methods that rely on solving large quadratic programming problems (QPPs), the proposed TMvRKM efficiently determines an optimal separating hyperplane through a regularized least squares approach, enhancing both computational efficiency and classification performance. The primal objective of TMvRKM includes a coupling term designed to balance errors across multiple views effectively. By integrating early and late fusion strategies, TMvRKM leverages the collective information from all views during training while remaining flexible to variations specific to individual views. The proposed TMvRKM model is rigorously tested on UCI, KEEL, and AwA benchmark datasets. Both experimental results and statistical analyses consistently highlight its exceptional generalization performance, outperforming baseline models in every scenario.",human
"--- We address the challenge of automated segmentation of lung nodules in Computed Tomography (CT) scans, a critical task for early diagnosis and treatment planning of lung cancer. Manual segmentation is time-consuming and subject to inter-observer variability, motivating the development of robust and efficient automated solutions. Our method introduces a novel deep learning architecture, termed Context-Aware Convolutional Transformer Network (CACTNet), that integrates convolutional neural networks (CNNs) for local feature extraction with Transformer encoders for capturing long-range contextual dependencies within the CT volume. Specifically, we employ a multi-scale CNN backbone to extract hierarchical feature maps, which are then fed into a Transformer encoder to model relationships between different regions of the lung. To further enhance segmentation accuracy, we incorporate a spatial attention mechanism that selectively attends to relevant image regions based on contextual information. We evaluate CACTNet on the publicly available LIDC-IDRI dataset, comparing its performance against state-of-the-art segmentation algorithms. Experimental results demonstrate that CACTNet achieves a Dice Similarity Coefficient (DSC) of 0.87 and an Average Symmetric Surface Distance (ASSD) of 1.2 mm, outperforming existing methods by a significant margin. These findings suggest that CACTNet offers a promising approach for accurate and reliable lung nodule segmentation, potentially improving the efficiency and effectiveness of lung cancer screening programs.",ai
"Spatio-temporal graph neural networks (ST-GNNs) have achieved notable success in structured domains such as road traffic and public transportation, where spatial entities can be naturally represented as fixed nodes. In contrast, many real-world systems including maritime traffic lack such fixed anchors, making the construction of spatio-temporal graphs a fundamental challenge. Anomaly detection in these non-grid environments is particularly difficult due to the absence of canonical reference points, the sparsity and irregularity of trajectories, and the fact that anomalies may manifest at multiple granularities. In this work, we introduce a novel benchmark dataset for anomaly detection in the maritime domain, extending the Open Maritime Traffic Analysis Dataset (OMTAD) into a benchmark tailored for graph-based anomaly detection. Our dataset enables systematic evaluation across three different granularities: node-level, edge-level, and graph-level anomalies. We plan to employ two specialized LLM-based agents: and to construct richer interaction contexts and generate semantically meaningful anomalies. We expect this benchmark to promote reproducibility and to foster methodological advances in anomaly detection for non-grid spatio-temporal systems.",human
"The rapid development of generative artificial intelligence (AI) and large language models (LLMs), and the availability of services that make them accessible, have led the general public to begin incorporating them into everyday life. The extended reality (XR) community has also sought to integrate LLMs, particularly in the form of conversational agents, to enhance user experience and task efficiency. When interacting with such conversational agents, users may easily disclose sensitive information due to the naturalistic flow of the conversations, and combining such conversational data with fine-grained sensor data may lead to novel privacy issues. To address these issues, a user-centric understanding of technology acceptance and concerns is essential. Therefore, to this end, we conducted a large-scale crowdsourcing study with 1036 participants, examining user decision-making processes regarding LLM-powered conversational agents in XR, across factors of XR setting type, speech interaction type, and data processing location. We found that while users generally accept these technologies, they express concerns related to security, privacy, social implications, and trust. Our results suggest that familiarity plays a crucial role, as daily generative AI use is associated with greater acceptance. In contrast, previous ownership of XR devices is linked to less acceptance, possibly due to existing familiarity with the settings. We also found that men report higher acceptance with fewer concerns than women. Regarding data type sensitivity, location data elicited the most significant concern, while body temperature and virtual object states were considered least sensitive. Overall, our study highlights the importance of practitioners effectively communicating their measures to users, who may remain distrustful. We conclude with implications and recommendations for LLM-powered XR.",human
"Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.",human
"Recent advances in natural language processing (NLP) have been significantly driven by the transformer architecture, particularly in tasks involving long-range dependencies. However, efficiently modeling these dependencies in extensive corpora remains a computational challenge. This paper investigates a novel optimization strategy for transformer-based models designed to reduce computational complexity while preserving, and in some cases enhancing, performance across a range of NLP benchmarks. We introduce a sparse attention mechanism, termed ""Selective Attention with Learned Strides"" (SALS), which strategically attends to a subset of input tokens based on learned importance weights and dynamically adjusted stride lengths. SALS reduces the quadratic complexity of standard attention to sub-quadratic. We evaluate SALS on several established NLP tasks, including machine translation (English-German), text summarization, and sentiment analysis. Results demonstrate that SALS achieves comparable or improved performance compared to full attention transformers and other sparsity-inducing methods, while requiring significantly fewer computational resources, measured in terms of FLOPs and wall-clock time. Further analysis reveals that the learned strides adapt to the structure of the input sequence, focusing attention on salient features while effectively ignoring less relevant context. The proposed SALS mechanism provides a computationally efficient and performance-competitive alternative to standard attention in transformer architectures for NLP.",ai
"We address the problem of training machine learning models on sensitive data while preserving individual privacy. Specifically, we focus on learning language models under differential privacy (DP), where the goal is to minimize the impact of any single data point on the resulting model. Previous approaches often suffer from significant utility degradation, particularly for large vocabulary sizes and complex model architectures. Our method, DP-AdaMix, introduces a novel differentially private adaptation strategy for pre-trained language models. We leverage the inherent knowledge captured in pre-trained models and adapt only a small, carefully selected subset of parameters to a downstream task under DP constraints. This is achieved through a combination of parameter selection based on singular value decomposition (SVD) to identify low-rank structures suitable for adaptation, and adaptive noise injection guided by a privacy accounting mechanism. Experiments on text classification and sequence generation tasks demonstrate that DP-AdaMix significantly outperforms existing DP training techniques. Specifically, we observe an average improvement of 5-10% in downstream task accuracy compared to naive DP fine-tuning and comparable methods utilizing gradient clipping, while maintaining strong privacy guarantees. Moreover, our method demonstrates improved scalability, enabling the training of differentially private language models with larger vocabularies and model sizes than previously feasible, pushing the boundaries of practical privacy-preserving natural language processing.",ai
"Reliable uncertainty estimation is crucial for machine learning models, especially in safety-critical domains. While exact Bayesian inference offers a principled approach, it is often computationally infeasible for deep neural networks. Monte Carlo dropout (MCD) was proposed as an efficient approximation to Bayesian inference in deep learning by applying neuron dropout at inference time . Hence, the method generates multiple sub-models yielding a distribution of predictions to estimate uncertainty. We empirically investigate its ability to capture true uncertainty and compare to Gaussian Processes (GP) and Bayesian Neural Networks (BNN). We find that MCD struggles to accurately reflect the underlying true uncertainty, particularly failing to capture increased uncertainty in extrapolation and interpolation regions as observed in Bayesian models. The findings suggest that uncertainty estimates from MCD, as implemented and evaluated in these experiments, is not as reliable as those from traditional Bayesian approaches for capturing epistemic and aleatoric uncertainty.",human
"We investigate the challenge of coordinating communication strategies within heterogeneous multi-agent systems operating in partially observable environments. Prior work often relies on centralized training or assumes homogeneity in agent capabilities and communication protocols, limiting scalability and adaptability to realistic scenarios. This paper introduces a novel decentralized learning framework, termed Differentiable Inter-Agent Negotiation (DIAN), that enables agents with diverse observation spaces and action sets to autonomously learn effective communication protocols. DIAN employs a differentiable negotiation module that allows agents to exchange probabilistic beliefs about the environment and their intended actions. This module is trained end-to-end with a reinforcement learning objective, fostering the emergence of communication strategies tailored to individual agent roles and environmental dynamics. We evaluate DIAN on a challenging cooperative navigation task and a resource allocation scenario. Our results demonstrate that DIAN significantly outperforms baseline methods, including independent learners and communication strategies based on pre-defined protocols. Furthermore, we analyze the emergent communication patterns, revealing that DIAN facilitates the development of efficient and context-aware communication strategies that improve overall system performance and robustness to environmental uncertainty. The learned negotiation module exhibits strong generalization capabilities to unseen environmental configurations and agent compositions.",ai
"**Abstract:** Estimating causal effects from observational data remains a significant challenge due to the presence of confounding variables. Traditional methods, such as propensity score matching and inverse probability weighting, often rely on strong assumptions about the functional form of the treatment and outcome models, which, if misspecified, can lead to biased estimates. This paper addresses the problem of robust causal effect estimation in the presence of potential model misspecification. We introduce a novel approach combining representation learning with targeted maximum likelihood estimation (TMLE). Specifically, we employ a neural network to learn a balanced representation of the observed covariates, effectively reducing confounding bias by approximating a sufficient adjustment set. This learned representation is then incorporated into the TMLE framework, allowing for doubly robust estimation of the average treatment effect (ATE). Our method provides consistent ATE estimates if either the treatment or outcome model is correctly specified, even when both are misspecified in the original covariate space. We demonstrate the efficacy of our approach through simulations on synthetic datasets, exhibiting superior performance in terms of bias and mean squared error compared to standard methods under various model misspecification scenarios. Furthermore, we apply our method to a real-world healthcare dataset, showcasing its practical utility in inferring the causal effect of a medical intervention.",ai
"In this work, we revisit dictionary-based sparse regression, in particular, Sequential Threshold Least Squares (STLS), and propose a score-guided library selection to provide practical guidance for data-driven modeling, with emphasis on SINDy-type algorithms. STLS is an algorithm to solve the sparse least-squares problem, which relies on splitting to efficiently solve the least-squares portion while handling the sparse term via proximal methods. It produces coefficient vectors whose components depend on both the projected reconstruction errors, here referred to as the scores, and the mutual coherence of dictionary terms. The first contribution of this work is a theoretical analysis of the score and dictionary-selection strategy. This could be understood in both the original and weak SINDy regime. Second, numerical experiments on ordinary and partial differential equations highlight the effectiveness of score-based screening, improving both accuracy and interpretability in dynamical system identification. These results suggest that integrating score-guided methods to refine the dictionary more accurately may help SINDy users in some cases to enhance their robustness for data-driven discovery of governing equations.",human
"We investigate the challenge of emergent communication in cooperative multi-agent systems operating in partially observable environments. Specifically, we address the issue of semantic alignment between agents developing their own communication protocols, which can lead to brittle performance and difficulty in understanding learned strategies. Our approach, termed Differentiable Inter-Agent Alignment (DIA), introduces a novel regularisation term during training that encourages agents to develop similar internal representations for equivalent messages. DIA leverages a differentiable approximation of the earth mover's distance to quantify the similarity between the learned message embeddings across agents. This regularisation term is incorporated into a standard reinforcement learning objective function. We evaluate DIA in a collaborative navigation task where agents must coordinate to reach target locations in a grid-world with limited visibility. Empirical results demonstrate that DIA significantly improves inter-agent communication alignment, leading to more robust and generalisable cooperation strategies. Furthermore, we observe a statistically significant improvement in task completion rate compared to baseline methods without explicit alignment. Analysis of the learned communication protocols reveals a more interpretable and predictable relationship between message content and agent behavior.",ai
"We address the challenge of coordinating collaborative multi-agent systems in environments with sparse rewards and delayed feedback. Traditional reinforcement learning techniques often struggle in such settings due to the difficulty in assigning credit across agents and temporally distant actions. To mitigate this issue, we propose a novel approach, Cooperative Trajectory Optimization with Learned Communication (CTOLC), which combines trajectory optimization with a learned communication channel. CTOLC leverages trajectory optimization to generate tentative, coordinated plans for the team, enabling efficient exploration of the state-action space. Simultaneously, agents learn to communicate relevant state information through a differentiable communication module, facilitating improved coordination and credit assignment. The communication module is trained end-to-end alongside the trajectory optimization process, ensuring that agents exchange information that is pertinent to collaborative planning. We evaluate CTOLC on a suite of challenging multi-agent environments characterized by sparse rewards and long planning horizons. Results demonstrate that CTOLC significantly outperforms existing state-of-the-art multi-agent reinforcement learning algorithms, achieving higher cumulative rewards and improved coordination strategies. Furthermore, analysis of the learned communication channel reveals that agents learn to selectively transmit information relevant to predicting future joint trajectories, leading to more robust and efficient collaborative decision-making.",ai
"Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.",human
"Reliable reinforcement learning (RL) for diffusion large language models (dLLMs) requires both accurate advantage estimation and precise estimation of prediction probabilities. Existing RL methods for dLLMs fall short in both aspects: they rely on coarse or unverifiable reward signals, and they estimate prediction probabilities without accounting for the bias relative to the true, unbiased expected prediction probability that properly integrates over all possible decoding orders. To mitigate these issues, we propose -TreeRPO, a reliable RL framework for dLLMs that leverages tree-structured rollouts and bottom-up advantage computation based on verifiable outcome rewards to provide fine-grained and verifiable step-wise reward signals. When estimating the conditional transition probability from a parent node to a child node, we theoretically analyze the estimation error between the unbiased expected prediction probability and the estimate obtained via a single forward pass, and find that higher prediction confidence leads to lower estimation error. Guided by this analysis, we introduce a time-scheduled self-distillation loss during training that enhances prediction confidence in later training stages, thereby enabling more accurate probability estimation and improved convergence. Experiments show that -TreeRPO outperforms existing baselines and achieves significant gains on multiple reasoning benchmarks, including +86.2 on Sudoku, +51.6 on Countdown, +4.5 on GSM8K, and +5.3 on Math500. Ablation studies and computational cost analyses further demonstrate the effectiveness and practicality of our design choices.",human
"This work addresses the challenge of identifying and estimating causal effects in settings with unobserved confounding and non-linear relationships. Specifically, we consider scenarios where traditional instrumental variable (IV) methods are rendered ineffective due to weak instrument strength or violations of the exclusion restriction. We propose a novel approach, termed Contrastive Representation Learning for Causal Effect Estimation (CRLCee), that leverages contrastive learning to learn latent representations of observed variables such that causal and confounding effects are disentangled within the latent space. This is achieved by training an encoder to maximize agreement between representations of units under different interventions while minimizing agreement between representations of units with similar observed covariates but differing intervention status. We demonstrate that CRLCee enables consistent estimation of the average treatment effect (ATE) under weaker assumptions than standard IV approaches. Empirically, we evaluate CRLCee on both synthetic datasets and real-world benchmark datasets, including the Infant Health and Development Program (IHDP) dataset. Our results demonstrate that CRLCee outperforms existing methods, including traditional IV estimators and state-of-the-art machine learning approaches for causal inference, particularly in the presence of strong unobserved confounding and complex non-linear relationships. Furthermore, we provide theoretical guarantees for the identification and estimation of the ATE under specific conditions on the data generating process and the learned representations.",ai
"This paper addresses the challenge of decision-making under uncertainty within the framework of robust optimization, specifically focusing on problems where the uncertainty set is defined by moment-based ambiguity sets. Traditional robust optimization approaches often lead to overly conservative solutions, sacrificing performance in nominal scenarios. We propose a novel method leveraging a data-driven approach to refine the ambiguity set, incorporating empirical observations to better represent the true underlying distribution. Our approach employs a scenario generation technique based on kernel density estimation, coupled with a tractable approximation of the robust counterpart. This allows for adaptive adjustment of the ambiguity set's size based on the available data, striking a balance between robustness and optimality. We demonstrate the effectiveness of our methodology through numerical experiments on a multi-stage inventory management problem. Compared to classical box and ellipsoidal uncertainty sets, our data-driven robust optimization method achieves significant improvements in expected cost while maintaining a high level of robustness against adversarial scenarios. The results demonstrate that our approach effectively reduces conservatism without compromising performance, providing a practical and efficient solution for robust decision-making under distributional uncertainty.",ai
"This paper presents a marketing analytics framework that operationalizes subscription pricing as a dynamic, guardrailed decision system, uniting multivariate demand forecasting, segment-level price elasticity, and churn propensity to optimize revenue, margin, and retention. The approach blends seasonal time-series models with tree-based learners, runs Monte Carlo scenario tests to map risk envelopes, and solves a constrained optimization that enforces business guardrails on customer experience, margin floors, and allowable churn. Validated across heterogeneous SaaS portfolios, the method consistently outperforms static tiers and uniform uplifts by reallocating price moves toward segments with higher willingness-to-pay while protecting price-sensitive cohorts. The system is designed for real-time recalibration via modular APIs and includes model explainability for governance and compliance. Managerially, the framework functions as a strategy playbook that clarifies when to shift from flat to dynamic pricing, how to align pricing with CLV and MRR targets, and how to embed ethical guardrails, enabling durable growth without eroding customer trust.",human
"Performance prediction for OpenMP workloads on heterogeneous embedded SoCs is challenging due to complex interactions between task DAG structure, control-flow irregularity, cache and branch behavior, and thermal dynamics; classical heuristics struggle under workload irregularity, tabular regressors discard structural information, and model-free RL risks overheating resource-constrained devices. We introduce GraphPerf-RT, the first surrogate that unifies task DAG topology, CFG-derived code semantics, and runtime context (per-core DVFS, thermal state, utilization) in a heterogeneous graph representation with typed edges encoding precedence, placement, and contention. Multi-task evidential heads predict makespan, energy, cache and branch misses, and utilization with calibrated uncertainty (Normal-Inverse-Gamma), enabling risk-aware scheduling that filters low-confidence rollouts. We validate GraphPerf-RT on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi), achieving R^2 > 0.95 with well-calibrated uncertainty (ECE < 0.05). To demonstrate end-to-end scheduling utility, we integrate the surrogate with four RL methods on Jetson TX2: single-agent model-free (SAMFRL), single-agent model-based (SAMBRL), multi-agent model-free (MAMFRL-D3QN), and multi-agent model-based (MAMBRL-D3QN). Experiments across 5 seeds (200 episodes each) show that MAMBRL-D3QN with GraphPerf-RT as the world model achieves 66% makespan reduction (0.97 +/- 0.35s) and 82% energy reduction (0.006 +/- 0.005J) compared to model-free baselines, demonstrating that accurate, uncertainty-aware surrogates enable effective model-based planning on thermally constrained embedded systems.",human
"**Abstract:** This paper addresses the challenge of scaling symbolic reasoning to high-dimensional datasets where explicit knowledge representations are intractable. We introduce a novel method, Differentiable Inductive Reasoning (DIR), which leverages a neural-symbolic architecture to learn latent symbolic representations and perform inductive reasoning in a differentiable manner. DIR consists of two primary components: a neural encoder that maps input data to a set of probabilistic facts expressed in a first-order logic, and a differentiable inference engine that performs probabilistic resolution over these facts to derive new conclusions. We parameterize the inference engine using recurrent neural networks, allowing for iterative reasoning steps guided by a learned attention mechanism. Critically, the entire system is trained end-to-end using a loss function that encourages logical consistency and accurate prediction of target variables. We evaluate DIR on a series of synthetic and real-world relational reasoning tasks, demonstrating superior performance compared to both purely neural and purely symbolic baselines, particularly in scenarios with noisy or incomplete information. The results show that DIR effectively combines the strengths of neural networks (handling high-dimensional data) and symbolic reasoning (providing explainability and generalization), offering a promising approach for automated reasoning in complex domains.",ai
"Ultra-processed foods are increasingly linked to health issues like obesity, cardiovascular disease, type 2 diabetes, and mental health disorders due to poor nutritional quality. This first-of-its-kind study at such a scale uses machine learning to classify food processing levels (NOVA) based on the Open Food Facts dataset of over 900,000 products. Models including LightGBM, Random Forest, and CatBoost were trained on nutrient concentration data. LightGBM performed best, achieving 80-85% accuracy across different nutrient panels and effectively distinguishing minimally from ultra-processed foods. Exploratory analysis revealed strong associations between higher NOVA classes and lower Nutri-Scores, indicating poorer nutritional quality. Products in NOVA 3 and 4 also had higher carbon footprints and lower Eco-Scores, suggesting greater environmental impact. Allergen analysis identified gluten and milk as common in ultra-processed items, posing risks to sensitive individuals. Categories like Cakes and Snacks were dominant in higher NOVA classes, which also had more additives, highlighting the role of ingredient modification. This study, leveraging the largest dataset of NOVA-labeled products, emphasizes the health, environmental, and allergenic implications of food processing and showcases machine learning's value in scalable classification. A user-friendly web tool is available for NOVA prediction using nutrient data: https://cosylab.iiitd.edu.in/foodlabel/.",human
"Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.",human
"We investigate the efficacy of Transformer-based architectures for a range of natural language processing (NLP) tasks, focusing on improving performance in low-resource scenarios and enhancing model interpretability. The prevalent reliance on large, pre-trained models, while achieving state-of-the-art results, often necessitates substantial computational resources and limits accessibility for researchers with limited infrastructure. Furthermore, the black-box nature of these models hinders understanding of their decision-making processes. Our approach explores two primary avenues: (1) fine-tuning smaller, task-specific Transformers optimized for efficiency and (2) incorporating attention-based visualization techniques to provide insights into the model's internal representations. Specifically, we examine the effectiveness of knowledge distillation from larger models to train smaller, more manageable architectures. Additionally, we propose a novel attention aggregation method to highlight the most salient input features contributing to model predictions. Experimental results demonstrate that distilled Transformers achieve comparable performance to their larger counterparts with significantly reduced computational overhead. The attention aggregation technique successfully identifies relevant phrases and keywords, offering a degree of interpretability previously unattainable with standard Transformer models. Quantitative evaluations on benchmark datasets, including GLUE and SQuAD, confirm these findings, showcasing a promising trade-off between performance, efficiency, and interpretability in Transformer-based NLP.",ai
"Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.",human
"We address the problem of efficiently representing and reasoning with complex relational knowledge, specifically focusing on scenarios involving hierarchical classifications and nuanced property inheritance. Traditional knowledge representation schemes often struggle with maintaining consistency and tractability as the scale and intricacy of the knowledge base increase. We propose a novel approach leveraging a hybrid knowledge graph architecture integrating expressive description logic (DL) axioms with distributed entity embeddings. This architecture allows for both precise logical inference and efficient similarity-based reasoning. The DL component provides a formal foundation for defining class hierarchies and property constraints, while the embeddings capture semantic relationships and enable approximate query answering. We introduce a training methodology that jointly optimizes the DL component and the embedding space through a combination of logical reasoning tasks and link prediction objectives. Empirical evaluation on benchmark knowledge graphs demonstrates that our approach achieves significantly improved performance in terms of accuracy and scalability compared to state-of-the-art knowledge representation techniques. Furthermore, we show that our hybrid architecture allows for more effective handling of incomplete and noisy data, paving the way for building more robust and practical knowledge-based systems.",ai
"Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and ""surprising"" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.",human
"It has been demonstrated both theoretically and empirically that the ReLU MLP tends to extrapolate linearly for an out-of-distribution evaluation point. The machine learning literature provides ample analysis with respect to the mechanisms to which linearity is induced. However, the analysis of extrapolation at the origin under the NTK regime remains a more unexplored special case. In particular, the infinite-dimensional feature map induced by the neural tangent kernel is not translationally invariant. This means that the study of an out-of-distribution evaluation point very far from the origin is not equivalent to the evaluation of a point very near the origin. And since the feature map is rotation invariant, these two special cases may represent the most canonically extreme bounds of ReLU NTK extrapolation. Ultimately, it is this loose recognition of the two special cases of extrapolation that motivate the discovery of quadratic extrapolation for an evaluation close to the origin.",human
"While Joint-Embedding Predictive Architecture (JEPA) has emerged as a powerful architecture for learning rich latent representations, it fundamentally lacks generative abilities. Meanwhile, latent space reasoning attempts for Transformer models like COCONUT do improve performance, but they ultimately rely on token-by-token generation, which still accumulates compounding error and relies on context information to gain reasoning insights. To address these limitations, we propose JEPA-Reasoner, a novel JEPA model enhanced with generative ability that reasons in latent space. We augment it with a separate action-taker model, Talker, to produce human-readable sentences. Our approach demonstrates that decoupling latent space reasoning and token generation enables JEPA-Reasoner to produce mixed latent vectors that might lay the foundation for multi-threaded reasoning, while performing autoregressive generation with superior robustness to compounding error.",human
"Federated learning across multi-cloud environments faces critical challenges, including non-IID data distributions, malicious participant detection, and substantial cross-cloud communication costs (egress fees). Existing Byzantine-robust methods focus primarily on model accuracy while overlooking the economic implications of data transfer across cloud providers. This paper presents Cost-TrustFL, a hierarchical federated learning framework that jointly optimizes model performance and communication costs while providing robust defense against poisoning attacks. We propose a gradient-based approximate Shapley value computation method that reduces the complexity from exponential to linear, enabling lightweight reputation evaluation. Our cost-aware aggregation strategy prioritizes intra-cloud communication to minimize expensive cross-cloud data transfers. Experiments on CIFAR-10 and FEMNIST datasets demonstrate that Cost-TrustFL achieves 86.7% accuracy under 30% malicious clients while reducing communication costs by 32% compared to baseline methods. The framework maintains stable performance across varying non-IID degrees and attack intensities, making it practical for real-world multi-cloud deployments.",human
"We address the problem of robust causal effect estimation in observational studies where unobserved confounding is suspected and instrumental variables are weak or invalid. Traditional instrumental variable (IV) methods often suffer from substantial bias under these conditions. We propose a novel approach, Causal Anchor Regression (CAR), which leverages a learned set of *anchor variables* that are proxies for the unobserved confounders. CAR first employs a variational autoencoder (VAE) with carefully designed structural constraints to identify variables that are statistically associated with both the treatment and the outcome, while remaining conditionally independent given the observed covariates and a latent representation of the unobserved confounders. These identified anchor variables are then incorporated as controls in a regression model to estimate the average treatment effect (ATE). Theoretical analysis demonstrates that CAR can consistently estimate the ATE under specific identifiability conditions related to the strength of the anchor variables as proxies for the unobserved confounders. Empirically, we evaluate CAR on synthetic datasets and a real-world healthcare dataset concerning the effect of a specific medication on patient outcomes. Results indicate that CAR outperforms existing methods, including standard IV estimators and sensitivity analysis techniques, especially when the instruments are weak or invalid and the degree of unobserved confounding is high. The observed gains in robustness and accuracy suggest CAR provides a promising alternative for causal inference in complex observational settings.",ai
"Drought is a complex natural hazard that affects ecological and human systems, often resulting in substantial environmental and economic losses. Recent increases in drought severity, frequency, and duration underscore the need for effective monitoring and mitigation strategies. Predicting drought impacts rather than drought conditions alone offers opportunities to support early warning systems and proactive decision-making. This study applies machine learning techniques to link drought indices with historical drought impact records (2005:2024) to generate short-term impact forecasts. By addressing key conceptual and data-driven challenges regarding temporal scale and impact quantification, the study aims to improve the predictability of drought impacts at actionable lead times. The Drought Severity and Coverage Index (DSCI) and the Evaporative Stress Index (ESI) were combined with impact data from the Drought Impact Reporter (DIR) to model and forecast weekly drought impacts. Results indicate that Fire and Relief impacts were predicted with the highest accuracy, followed by Agriculture and Water, while forecasts for Plants and Society impacts showed greater variability. County and state level forecasts for New Mexico were produced using an eXtreme Gradient Boosting (XGBoost) model that incorporated both DSCI and ESI. The model successfully generated forecasts up to eight weeks in advance using the preceding eight weeks of data for most impact categories. This work supports the development of an Ecological Drought Information Communication System (EcoDri) for New Mexico and demonstrates the potential for broader application in similar drought-prone regions. The findings can aid stakeholders, land managers, and decision-makers in developing and implementing more effective drought mitigation and adaptation strategies.",human
"Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.",human
"Data attribution aims to explain model predictions by estimating how they would change if certain training points were removed, and is used in a wide range of applications, from interpretability and credit assignment to unlearning and privacy. Even in the relatively simple case of linear regressions, existing mathematical analyses of leading data attribution methods such as Influence Functions (IF) and single Newton Step (NS) remain limited in two key ways. First, they rely on global strong convexity assumptions which are often not satisfied in practice. Second, the resulting bounds scale very poorly with the number of parameters () and the number of samples removed (). As a result, these analyses are not tight enough to answer fundamental questions such as ""what is the asymptotic scaling of the errors of each method?"" or ""which of these methods is more accurate for a given dataset?"" In this paper, we introduce a new analysis of the NS and IF data attribution methods for convex learning problems. To the best of our knowledge, this is the first analysis of these questions that does not assume global strong convexity and also the first explanation of [KATL19] and [RH25a]'s observation that NS data attribution is often more accurate than IF. We prove that for sufficiently well-behaved logistic regression, our bounds are asymptotically tight up to poly-logarithmic factors, yielding scaling laws for the errors in the average-case sample removals. \[ _{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T - \hatθ_T^{}\|_2 \bigr] = \widetildeΘ\!\left({n^2}\right), \qquad _{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T^{} - \hatθ_T^{}\|_2 \bigr] = \widetildeΘ\!\left( }{n^2} \right). \]",human
"Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce , a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.",human
"Adsorption energy is a key descriptor of catalytic reactivity. It is fundamentally defined as the difference between the relaxed total energy of the adsorbate-surface system and that of an appropriate reference state; therefore, the accuracy of relaxed-energy prediction directly determines the reliability of machine-learning-driven catalyst screening. E(3)-equivariant graph neural networks (GNNs) can natively operate on three-dimensional atomic coordinates under periodic boundary conditions and have demonstrated strong performance on such tasks. In contrast, language-model-based approaches, while enabling human-readable textual descriptions and reducing reliance on explicit graph -- thereby broadening applicability -- remain insufficient in both adsorption-configuration energy prediction accuracy and in distinguishing ``the same system with different configurations,'' even with graph-assisted pretraining in the style of GAP-CATBERTa. To this end, we propose QE-Catalytic, a multimodal framework that deeply couples a large language model (wen) with an E(3)-equivariant graph Transformer (quiformer-V2), enabling unified support for adsorption-configuration property prediction and inverse design on complex catalytic surfaces. During prediction, QE-Catalytic jointly leverages three-dimensional structures and structured configuration text, and injects ``3D geometric information'' into the language channel via graph-text alignment, allowing it to function as a high-performance text-based predictor when precise coordinates are unavailable, while also autoregressively generating CIF files for target-energy-driven structure design and information completion. On OC20, QE-Catalytic reduces the MAE of relaxed adsorption energy from 0.713~eV to 0.486~eV, and consistently outperforms baseline models such as CatBERTa and GAP-CATBERTa across multiple evaluation protocols.",human
"We investigate the application of transformer architectures to the problem of long-range dependency modeling in natural language processing. Traditional recurrent neural networks (RNNs), while historically dominant, struggle to capture relationships between distant words due to vanishing gradients and inherent sequential processing limitations. We propose a novel adaptation of the standard transformer encoder, incorporating a learned, sparse attention mechanism that reduces computational complexity and memory requirements. This sparsity pattern is derived from a graph-based representation of the input text, explicitly encoding syntactic and semantic relationships. We evaluate our approach on a diverse set of benchmark tasks, including document summarization, question answering, and long document classification, comparing against state-of-the-art transformer variants and RNN-based models. Our results demonstrate significant improvements in performance, particularly on tasks requiring reasoning over extended contexts. Specifically, we observe a 5-8% increase in ROUGE scores for summarization and a 3-5% increase in accuracy for long document classification compared to dense attention transformers, while simultaneously reducing computational cost by 20-30%. These findings suggest that incorporating structured inductive biases into transformer architectures can enhance their ability to model long-range dependencies and improve their efficiency for large-scale NLP tasks.",ai
"Clustering is a fundamental unsupervised learning task for uncovering patterns in data. While Gaussian Blurring Mean Shift (GBMS) has proven effective for identifying arbitrarily shaped clusters in Euclidean space, it struggles with datasets exhibiting hierarchical or tree-like structures. In this work, we introduce HypeGBMS, a novel extension of GBMS to hyperbolic space. Our method replaces Euclidean computations with hyperbolic distances and employs Möbius-weighted means to ensure that all updates remain consistent with the geometry of the space. HypeGBMS effectively captures latent hierarchies while retaining the density-seeking behavior of GBMS. We provide theoretical insights into convergence and computational complexity, along with empirical results that demonstrate improved clustering quality in hierarchical datasets. This work bridges classical mean-shift clustering and hyperbolic representation learning, offering a principled approach to density-based clustering in curved spaces. Extensive experimental evaluations on real-world datasets demonstrate that HypeGBMS significantly outperforms conventional mean-shift clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.",human
"We address the problem of robust causal effect estimation in the presence of unobserved confounding and measurement error. Traditional instrumental variable (IV) methods often falter under violations of the exclusion restriction or in the presence of weak instruments. Furthermore, observational studies relying solely on covariate adjustment are susceptible to bias from unmeasured confounders. To mitigate these limitations, we propose a novel method, Causal Latent Factor Adjustment with Neural Instruments (CLFANI), which leverages a neural network architecture to simultaneously learn latent confounders and construct strong, valid instruments. CLFANI models the observed data as a function of observed covariates, unobserved confounders captured by a latent factor model, and instrument variables generated by a neural network trained to predict treatment assignment. Critically, the neural network instruments are encouraged to be independent of the latent confounders through an adversarial training objective. Empirical evaluation on synthetic datasets and real-world benchmark datasets demonstrates that CLFANI achieves superior performance compared to state-of-the-art methods, including traditional IV estimators, methods based on deconfounding autoencoders, and doubly robust estimators, particularly in scenarios with high-dimensional confounding and weak or invalid instruments. These results suggest that CLFANI provides a more robust and reliable approach to causal inference in challenging observational settings.",ai
"We investigate the problem of decentralized policy learning in cooperative multi-agent systems where agents possess limited communication bandwidth and heterogeneous action spaces. Existing methods often rely on centralized training with decentralized execution (CTDE) paradigms, necessitating complete state information during training, a condition frequently violated in real-world applications. We propose a novel framework, Decentralized Value Decomposition with Communication Compression (DVD-CC), which leverages value decomposition techniques to approximate the global reward function with locally tractable agent-specific reward functions. Crucially, DVD-CC incorporates a learnable communication bottleneck realized via vector quantization to reduce the communication overhead between agents during both training and execution. This allows for efficient message passing of relevant information while adhering to bandwidth constraints. Empirically, we evaluate DVD-CC on a suite of challenging cooperative tasks with varying levels of communication limitations and action space heterogeneity. Results demonstrate that DVD-CC significantly outperforms existing decentralized learning algorithms, achieving higher cumulative rewards and faster convergence rates. Furthermore, we analyze the trade-off between communication bandwidth and performance, revealing that DVD-CC can maintain near-optimal performance even under severe communication restrictions. These findings highlight the effectiveness of DVD-CC for learning decentralized policies in complex multi-agent environments.",ai
"We address the challenge of coordinating decentralized agents in complex, partially observable environments, specifically focusing on scenarios where communication bandwidth is limited and agents possess heterogeneous reward functions. Traditional reinforcement learning algorithms struggle in such settings due to the curse of dimensionality in joint action spaces and the difficulty of learning effective communication protocols. We propose a novel approach, Decentralized Variational Information Bottleneck (DVIB), which incorporates a variational information bottleneck to constrain the information transmitted between agents, thereby encouraging the learning of concise and relevant communication signals. Furthermore, DVIB leverages a personalized experience replay buffer for each agent, allowing for the effective learning of individualized policies tailored to their specific reward structures and observations. We evaluate DVIB on a suite of challenging cooperative navigation and resource allocation tasks within a multi-agent grid world environment. Our results demonstrate that DVIB significantly outperforms existing state-of-the-art methods, achieving higher levels of cooperation and efficiency while adhering to strict communication constraints. Specifically, DVIB exhibits improved sample efficiency and robustness to variations in communication bandwidth and agent heterogeneity, suggesting its potential for application in real-world multi-agent systems with limited communication resources.",ai
"Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.",human
"We investigate the problem of sparse reward environments in reinforcement learning, specifically focusing on the challenge of efficient exploration in natural language generation tasks. Existing methods often struggle to discover meaningful solutions due to the vast search space and the delayed nature of feedback signals. We propose a hierarchical reinforcement learning framework that incorporates intrinsic motivation via a learned skill discovery mechanism. This mechanism decomposes the overall task into a set of sub-goals, each corresponding to a distinct skill. A high-level manager learns to select appropriate skill sequences, while a low-level policy executes the selected skills to generate corresponding text fragments. Crucially, the skills are trained using an information-theoretic objective that encourages diversity and coverage of the state space. We evaluate our approach on a synthetic text generation task and a document summarization benchmark, demonstrating significant improvements in sample efficiency and final performance compared to standard reinforcement learning algorithms and imitation learning baselines. Our results indicate that hierarchical decomposition and intrinsic motivation can effectively mitigate the sparse reward problem and facilitate the learning of complex language generation policies. The learned skills also exhibit interpretability, providing insights into the underlying structure of the task.",ai
"We address the problem of long-range dependency modeling in graph-structured data for tasks requiring global contextual understanding, such as molecular property prediction and social network influence maximization. Traditional Graph Neural Networks (GNNs) often struggle to effectively propagate information across distant nodes due to issues like over-smoothing and vanishing gradients. We introduce a novel GNN architecture, the Attentive Global Contextualizer (AGC), which explicitly incorporates a global context vector representing the aggregated state of the entire graph. This vector is iteratively refined through an attention mechanism that selectively attends to individual node embeddings, allowing the model to capture intricate relationships between distant parts of the graph. The AGC is further enhanced with a residual connection to mitigate over-smoothing. We evaluate the proposed model on benchmark datasets for molecular property prediction (QM9, ZINC) and social network influence maximization (Facebook Ego-Nets). Experimental results demonstrate that the AGC significantly outperforms existing GNN models, achieving state-of-the-art performance on several key metrics, including mean absolute error (MAE) for regression tasks and AUC for classification tasks. Ablation studies confirm the importance of both the attention mechanism and the residual connection in achieving these improvements, highlighting the effectiveness of the AGC in capturing and leveraging global contextual information in graph-structured data.",ai
"While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.",human
"Test-time policy optimization enables large language models (LLMs) to adapt to distribution shifts by leveraging feedback from self-generated rollouts. However, existing methods rely on fixed-budget majority voting to estimate rewards, incurring substantial computational redundancy. We propose Optimal Rollout Allocation for Test-time Policy Optimization (OptPO), a principled framework that adaptively allocates inference budgets. By formulating the voting process as a Bayesian sequential probability ratio test, OptPO dynamically halts sampling once the posterior confidence in a consensus answer exceeds a specified threshold. Crucially, it utilizes the retained rollouts for on-policy updates, seamlessly integrating with algorithms like PPO or GRPO without requiring ground-truth labels. Across diverse reasoning benchmarks, OptPO significantly reduces rollout overhead compared to fixed-sample baselines while preserving or improving accuracy. By unifying statistically optimal stopping with test-time learning, OptPO offers a computationally efficient paradigm for test-time adaptation. The source code will be open upon acceptance at https://open-upon-acceptance.",human
"We address the problem of distributional robustness in natural language processing models, specifically concerning vulnerability to adversarial examples generated through semantic perturbations. Such perturbations, while imperceptible to humans, can significantly degrade model performance. We propose a novel robust optimization framework, Semantic Perturbation Robust Optimization (SPRO), which explicitly incorporates semantic similarity constraints during adversarial example generation within the training loop. SPRO utilizes a bi-level optimization approach. The inner loop generates adversarial examples by optimizing for maximum model loss subject to constraints ensuring semantic similarity to the original input, measured using a combination of contextualized word embeddings and syntactic dependency parse overlap. The outer loop minimizes the expected risk over the augmented training set, effectively forcing the model to learn features invariant to semantically plausible perturbations. Empirical evaluations on sentiment classification and textual entailment tasks demonstrate that SPRO significantly improves robustness compared to standard training and existing adversarial training methods. Specifically, SPRO achieves a relative improvement of 15-25% in adversarial accuracy against strong semantic perturbation attacks, while maintaining competitive performance on clean data. Ablation studies validate the efficacy of the proposed semantic similarity constraints in generating realistic and effective adversarial examples. The results highlight the importance of considering semantic plausibility when designing robust NLP models.",ai
"Deep learning methodologies have demonstrated promise in automated analysis of medical images, but often suffer from limited generalizability due to dataset bias and variations in image acquisition protocols. This work addresses the problem of robust image segmentation in medical imaging by introducing a novel multi-scale convolutional neural network architecture incorporating attention mechanisms and adversarial training. The proposed network, named Atrium-GAN, utilizes a U-Net-based generator with multi-scale feature extraction to capture both global contextual information and fine-grained details within the image. Spatial and channel-wise attention modules are integrated to enhance the network's focus on clinically relevant regions. An adversarial training framework is implemented using a discriminator network that distinguishes between real and segmented images, encouraging the generator to produce more realistic and accurate segmentations. The Atrium-GAN was evaluated on three publicly available datasets of varying modalities: chest X-rays for lung segmentation, brain MRIs for tumor segmentation, and retinal fundus images for optic disc segmentation. Experimental results demonstrate that the Atrium-GAN achieves statistically significant improvements in segmentation accuracy, measured by Dice score and IoU, compared to state-of-the-art methods. Furthermore, the adversarial training enhances the robustness of the model to variations in image contrast and noise, leading to improved performance on unseen datasets.",ai
"We address the problem of sample inefficiency in model-free reinforcement learning (RL) for continuous control tasks with sparse rewards. Existing approaches often require extensive interaction with the environment to learn effective policies, particularly when the reward signal is infrequent. To mitigate this, we propose a novel method, Reward-Augmented Trajectory Optimization (RATO), which combines trajectory optimization with a learned reward shaping function. RATO leverages a Gaussian Mixture Model (GMM) to model successful state trajectories, obtained through a limited number of environment interactions and a sparse reward signal. This GMM is then used to generate pseudo-rewards that guide the trajectory optimizer towards promising regions of the state space. Furthermore, we incorporate a trust-region constraint to ensure that the optimized trajectories remain within the learned distribution, improving stability and convergence. We evaluate RATO on a suite of challenging continuous control benchmarks with sparse rewards, including the Ant-v4 and HalfCheetah-v4 environments. Our experimental results demonstrate that RATO significantly outperforms state-of-the-art model-free RL algorithms, achieving comparable or superior performance with substantially fewer environment interactions. Specifically, RATO achieves a sample efficiency gain of up to 5x compared to baselines, demonstrating its effectiveness in learning optimal policies under sparse reward conditions.",ai
"The explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge. First, existing benchmarks often focus narrowly on retrieval while neglecting high-level planning and reasoning. Second, existing benchmarks favor general domains over the scientific domains that are the core application for DR agents. To address these gaps, we introduce Dr.Mi-Bench, a Modular-integrated benchmark for scientific DR agents. Grounded in academic literature, our benchmark uses a human-annotated dataset of 200 instances across 10 scientific domains, including both research and review papers. Besides, we also propose a Modular-integrated Evaluation Paradigm for DR Agents (Dr.Mi-Eval), a novel modular-integrated evaluation paradigm, which leverages the rich structure of academic papers to assess the core competencies of planning, retrieval, and reasoning through two complementary modes: an end-to-end evaluation for DR agents and an isolated evaluation for foundational LLMs as potential backbones. Experimental results reveal a fragmented performance landscape: agents exhibit specialized strengths but share critical weaknesses, most notably in performing the multi-source retrieval required for review-style tasks and performing consistently across diverse scientific fields. Moreover, improving high-level planning capability is the crucial factor for unlocking the reasoning potential of foundational LLMs as backbones. By exposing these actionable failure modes, Dr.Mi-Bench provides a diagnostic tool to guide the development of more reliable academic research assistants.",human
"We investigate the problem of sample inefficiency in deep reinforcement learning (DRL) when applied to complex, high-dimensional environments. Existing DRL algorithms often require extensive interaction with the environment to achieve satisfactory performance, hindering their applicability in real-world scenarios where data acquisition is costly or time-consuming. To address this challenge, we propose a novel hierarchical reinforcement learning framework incorporating a learned skill library and a meta-controller. The skill library comprises a set of pre-trained, reusable behaviors acquired through unsupervised exploration and offline reinforcement learning. The meta-controller learns to select and sequence these skills to achieve long-horizon goals, effectively decomposing the original complex task into a sequence of simpler sub-tasks. We evaluate our approach on a suite of challenging continuous control tasks, including simulated robotics manipulation and locomotion. Experimental results demonstrate that our method significantly improves sample efficiency compared to state-of-the-art DRL baselines, achieving comparable or superior performance with an order of magnitude fewer environment interactions. Furthermore, we demonstrate the transferability of the learned skill library to new, unseen tasks within the same environment, showcasing the adaptability and generalization capabilities of our framework. These findings suggest that hierarchical reinforcement learning with pre-trained skill libraries provides a promising direction for developing more practical and efficient DRL agents.",ai
"This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.",human
"We address the challenge of achieving coherent and contextually appropriate dialogue in multi-agent systems operating in complex, dynamic environments. Existing reinforcement learning approaches often struggle with generating consistent personas and long-term strategic communication, leading to suboptimal coordination and task completion. We propose a novel framework, Contextualized Persona-Consistent Communication (CPCC), which integrates a hierarchical reinforcement learning architecture with a persona memory module and a context encoder. The hierarchical architecture decouples strategic decision-making from tactical action selection, enabling agents to plan over extended horizons. The persona memory module allows agents to maintain a consistent and adaptable representation of their identity, facilitating predictable interaction. The context encoder captures the dynamic state of the environment and the communication history, enabling agents to adapt their dialogue to the evolving situation. We evaluate CPCC in a collaborative resource gathering environment. Experimental results demonstrate that CPCC significantly outperforms baseline methods in terms of task completion rate, communication efficiency, and persona consistency, achieving a 32% improvement in overall performance. Further analysis reveals that CPCC agents exhibit more strategic communication patterns and are better able to adapt their dialogue to both their own persona and the environmental context.",ai
"Weakly-Supervised Camouflaged Object Detection (WSCOD) aims to locate and segment objects that are visually concealed within their surrounding scenes, relying solely on sparse supervision such as scribble annotations. Despite recent progress, existing WSCOD methods still lag far behind fully supervised ones due to two major limitations: (1) the pseudo masks generated by general-purpose segmentation models (e.g., SAM) and filtered via rules are often unreliable, as these models lack the task-specific semantic understanding required for effective pseudo labeling in COD; and (2) the neglect of inherent annotation bias in scribbles, which hinders the model from capturing the global structure of camouflaged objects. To overcome these challenges, we propose ETOR, a two-stage WSCOD framework consisting of Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing. In the first stage, we introduce an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to enhance the capability of SAM for COD, improving the interpretability and precision of pseudo masks. In the second stage, we design FADeNet, which progressively fuses multi-level frequency-aware features to balance global semantic understanding with local detail modeling, while dynamically reweighting supervision strength across regions to alleviate scribble bias. By jointly exploiting the supervision signals from both the pseudo masks and scribble semantics, ETOR significantly narrows the gap between weakly and fully supervised COD, achieving state-of-the-art performance on multiple benchmarks.",human
"Language models are vulnerable to short adversarial suffixes that can reliably alter predictions. Previous works usually find such suffixes with gradient search or rule-based methods, but these are brittle and often tied to a single task or model. In this paper, a reinforcement learning framework is used where the suffix is treated as a policy and trained with Proximal Policy Optimization against a frozen model as a reward oracle. Rewards are shaped using calibrated cross-entropy, removing label bias and aggregating across surface forms to improve transferability. The proposed method is evaluated on five diverse NLP benchmark datasets, covering sentiment, natural language inference, paraphrase, and commonsense reasoning, using three distinct language models: Qwen2-1.5B Instruct, TinyLlama-1.1B Chat, and Phi-1.5. Results show that RL-trained suffixes consistently degrade accuracy and transfer more effectively across tasks and models than previous adversarial triggers of similar genres.",human
"Graph Neural Networks (GNNs) have demonstrated efficacy in learning representations of graph-structured data, facilitating tasks such as node classification and link prediction. However, the performance of GNNs is often contingent upon the quality of the input graph, which may be incomplete, noisy, or exhibit homophily-heterophily imbalances. This work addresses the problem of enhancing GNN robustness by incorporating graph structure learning within the network's training process. We propose a novel framework, Graph Structure Learner with Adaptive Homophily (GSL-AH), which jointly optimizes node embeddings and graph structure. GSL-AH dynamically adjusts edge weights based on both node feature similarity and a learned homophily coefficient specific to each node. This allows the network to adaptively prioritize connections between nodes with similar features while accounting for the inherent degree of homophily within the graph neighborhood. We evaluate GSL-AH on a diverse set of benchmark datasets exhibiting varying levels of homophily. Empirical results demonstrate that GSL-AH consistently outperforms existing GNN models and graph structure learning methods, particularly in scenarios where the initial graph structure is suboptimal. Ablation studies further validate the effectiveness of the adaptive homophily mechanism in improving GNN performance and robustness across different graph types. Our findings suggest that adaptively learning graph structure with consideration for local homophily enhances the representational power of GNNs.",ai
"We address the challenge of inferring causal relationships from observational data in high-dimensional settings, where the number of potential confounders significantly exceeds the sample size. Traditional causal discovery algorithms often falter under such conditions, leading to spurious causal links and unreliable causal effect estimations. To mitigate this issue, we propose a novel two-stage framework, termed ""Sparse Structure Agnostic Modeling for Causal Inference"" (SSAMCI). First, SSAMCI employs a penalized regression technique incorporating stability selection to identify a sparse set of potentially relevant confounders for each variable of interest. This stage focuses on robustness against model misspecification and variable selection uncertainty. Second, we apply constraint-based causal discovery algorithms, specifically PC algorithm with modified independence tests tailored to handle high-dimensional data, leveraging the reduced confounder sets identified in the first stage. This allows for a more reliable estimation of the underlying causal graph. Empirical evaluations on synthetic datasets, generated from known causal structures with varying levels of complexity and noise, demonstrate that SSAMCI significantly outperforms existing methods in terms of structural Hamming distance (SHD) and false discovery rate (FDR) for causal edge identification. Furthermore, experiments on real-world datasets suggest that SSAMCI produces more accurate estimates of causal effects compared to baseline approaches. These findings highlight the efficacy of SSAMCI in uncovering causal relationships in complex observational data.",ai
"We address the challenge of sample-efficient reinforcement learning in environments with sparse and delayed rewards, a common characteristic of real-world robotic control and game-playing scenarios. Conventional reinforcement learning algorithms often struggle in such environments due to the difficulty of exploring the state space and attributing credit to effective actions. To mitigate these issues, we introduce a novel hierarchical reinforcement learning framework, termed ""Goal-Conditioned Exploration with Intrinsic Motivation (GCE-IM)."" This framework comprises two distinct levels: a high-level goal-setting policy and a low-level action policy. The goal-setting policy learns to propose intermediate goals in state space based on an intrinsic motivation signal derived from prediction error, encouraging exploration towards unexplored regions. The action policy, conditioned on these intermediate goals, learns to navigate towards them using standard reinforcement learning techniques. We evaluate GCE-IM on a suite of challenging benchmark environments characterized by sparse rewards and long horizons. Our results demonstrate that GCE-IM significantly outperforms state-of-the-art baseline algorithms, including proximal policy optimization (PPO) and hindsight experience replay (HER), in terms of both learning speed and asymptotic performance. We further provide ablation studies demonstrating the efficacy of each component of the GCE-IM framework, highlighting the synergistic benefits of goal-conditioned exploration and intrinsic motivation. The proposed method offers a promising approach for addressing the sample efficiency bottleneck in reinforcement learning for complex, sparse-reward environments.",ai
"Safeguard models help large language models (LLMs) detect and block harmful content, but most evaluations remain English-centric and overlook linguistic and cultural diversity. Existing multilingual safety benchmarks often rely on machine-translated English data, which fails to capture nuances in low-resource languages. Southeast Asian (SEA) languages are underrepresented despite the region's linguistic diversity and unique safety concerns, from culturally sensitive political speech to region-specific misinformation. Addressing these gaps requires benchmarks that are natively authored to reflect local norms and harm scenarios. We introduce SEA-SafeguardBench, the first human-verified safety benchmark for SEA, covering eight languages, 21,640 samples, across three subsets: general, in-the-wild, and content generation. The experimental results from our benchmark demonstrate that even state-of-the-art LLMs and guardrails are challenged by SEA cultural and harm scenarios and underperform when compared to English texts.",human
"Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.",human
"We address the challenge of training machine learning models on sensitive data while ensuring strong privacy guarantees. Existing approaches often suffer from either significant utility degradation or rely on strong assumptions about the data distribution. This work introduces a novel differentially private federated learning framework, termed ""PrivFedAvg-SCA,"" which leverages secure computation aggregation (SCA) coupled with carefully calibrated noise injection during both local model updates and global aggregation. Specifically, local model gradients are privatized using a tailored sensitivity analysis that accounts for the non-i.i.d nature of federated data, minimizing unnecessary noise amplification. These privatized gradients are then securely aggregated using a multi-party computation protocol, preventing individual server access to raw updates. To further mitigate utility loss, we employ a dynamic clipping strategy that adapts to the statistical properties of the local gradients. We empirically evaluate PrivFedAvg-SCA on benchmark image classification and natural language processing datasets, demonstrating significant improvements in model accuracy compared to standard differentially private federated learning algorithms, particularly in highly heterogeneous data settings. Results indicate that PrivFedAvg-SCA achieves state-of-the-art privacy-utility trade-offs, enabling the training of accurate models with provable differential privacy guarantees.",ai
"Recent advancements in Natural Language Processing (NLP) have been largely driven by the Transformer architecture and its variants. However, scaling Transformers to handle increasingly complex tasks and longer sequences remains a significant computational challenge. This paper investigates the efficacy of a novel sparse attention mechanism, termed ""Context-Aware Gating Attention"" (CAGA), designed to mitigate the quadratic complexity bottleneck associated with standard Transformer attention. CAGA employs a learnable gating function conditioned on the contextual information of both query and key embeddings to selectively attend to the most relevant elements in the input sequence. This approach promotes efficient resource allocation by dynamically pruning irrelevant attention connections, thereby reducing computational cost and memory footprint. We evaluate CAGA on a suite of benchmark NLP tasks, including machine translation (WMT'14 English-German), text summarization (CNN/DailyMail), and long-range dependency modeling (Long Range Arena). Experimental results demonstrate that CAGA achieves comparable or superior performance to full attention Transformers while exhibiting a substantial reduction in computational cost, particularly for longer sequence lengths. Specifically, CAGA attains a 15% reduction in training time and a 20% reduction in memory usage on the Long Range Arena benchmark, without sacrificing accuracy. These findings suggest that CAGA offers a promising avenue for scaling Transformers to handle more demanding NLP applications.",ai
"While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.",human
"We address the critical challenge of training machine learning models on sensitive datasets while preserving individual privacy. Specifically, we investigate the problem of differentially private stochastic gradient descent (DP-SGD) in the context of non-convex optimization landscapes common in deep learning. Traditional DP-SGD methods often suffer from significant utility degradation, particularly at high privacy budgets or with large model complexities. Our work proposes a novel adaptive noise calibration strategy for DP-SGD, dynamically adjusting the noise scale based on the observed gradient variance during training. This approach leverages an online estimation of the per-layer gradient norms to refine the noise injection process, mitigating the over-noising inherent in fixed-noise DP-SGD variants. We theoretically analyze the privacy guarantees of our adaptive noise calibration strategy, providing rigorous bounds on the overall privacy loss via Rényi differential privacy. Empirically, we demonstrate the efficacy of our method on benchmark image classification datasets (CIFAR-10, CIFAR-100) and language modeling tasks, showing significant improvements in model accuracy compared to state-of-the-art DP-SGD baselines, especially when operating under strict privacy constraints. Furthermore, our adaptive approach reduces the sensitivity to hyperparameter tuning related to the noise scale, simplifying the practical application of DP-SGD.",ai
"Simulating complex unsteady physical phenomena relies on detailed mathematical models, simulated for instance by using the Finite Element Method (FEM). However, these models often exhibit discrepancies from the reality due to unmodeled effects or simplifying assumptions. We refer to this gap as the ignorance model. While purely data-driven approaches attempt to learn full system behavior, they require large amounts of high-quality data across the entire spatial and temporal domain. In real-world scenarios, such information is unavailable, making full data-driven modeling unreliable. To overcome this limitation, we model of the ignorance component using a hybrid twin approach, instead of simulating phenomena from scratch. Since physics-based models approximate the overall behavior of the phenomena, the remaining ignorance is typically lower in complexity than the full physical response, therefore, it can be learned with significantly fewer data. A key difficulty, however, is that spatial measurements are sparse, also obtaining data measuring the same phenomenon for different spatial configurations is challenging in practice. Our contribution is to overcome this limitation by using Graph Neural Networks (GNNs) to represent the ignorance model. GNNs learn the spatial pattern of the missing physics even when the number of measurement locations is limited. This allows us to enrich the physics-based model with data-driven corrections without requiring dense spatial, temporal and parametric data. To showcase the performance of the proposed method, we evaluate this GNN-based hybrid twin on nonlinear heat transfer problems across different meshes, geometries, and load positions. Results show that the GNN successfully captures the ignorance and generalizes corrections across spatial configurations, improving simulation accuracy and interpretability, while minimizing data requirements.",human
"Transformer-based architectures have become ubiquitous in natural language processing, demonstrating state-of-the-art performance across a diverse range of tasks. However, a persistent challenge remains in efficiently modeling long-range dependencies within extensive textual sequences. The computational complexity of the self-attention mechanism, scaling quadratically with sequence length, limits the practical application of vanilla transformers to documents of significant size. This work investigates a novel sparse attention mechanism integrated within a transformer framework to mitigate this computational bottleneck. Our approach, termed ""Contextualized Sparse Transformer"" (CST), employs a learnable gating function that dynamically selects a subset of context tokens for each query token, based on a contextual understanding of the input sequence. We evaluate CST on several benchmark datasets encompassing long-document summarization, question answering, and text classification. Experimental results demonstrate that CST achieves comparable or superior performance to full attention transformers on these tasks, while significantly reducing computational cost, particularly for longer sequences. Specifically, CST exhibits a 2x speedup in training and inference compared to a standard transformer on documents exceeding 2048 tokens, while maintaining comparable accuracy on the Long Range Arena (LRA) benchmark. These findings suggest that CST offers a viable alternative for processing long sequences, facilitating the application of transformers to a broader range of real-world NLP problems.",ai
"This paper addresses the challenges of low scheduling efficiency, unbalanced resource allocation, and poor adaptability in ETL (Extract-Transform-Load) processes under heterogeneous data environments by proposing an intelligent scheduling optimization framework based on deep Q-learning. The framework formalizes the ETL scheduling process as a Markov Decision Process and enables adaptive decision-making by a reinforcement learning agent in high-dimensional state spaces to dynamically optimize task allocation and resource scheduling. The model consists of a state representation module, a feature embedding network, a Q-value estimator, and a reward evaluation mechanism, which collectively consider task dependencies, node load states, and data flow characteristics to derive the optimal scheduling strategy in complex environments. A multi-objective reward function is designed to balance key performance indicators such as average scheduling delay, task completion rate, throughput, and resource utilization. Sensitivity experiments further verify the model's robustness under changes in hyperparameters, environmental dynamics, and data scale. Experimental results show that the proposed deep Q-learning scheduling framework significantly reduces scheduling delay, improves system throughput, and enhances execution stability under multi-source heterogeneous task conditions, demonstrating the strong potential of reinforcement learning in complex data scheduling and resource management, and providing an efficient and scalable optimization strategy for intelligent data pipeline construction.",human
"Graph Neural Networks (GNNs) have demonstrated remarkable success in various node- and graph-level prediction tasks. However, many existing GNN architectures struggle to effectively capture long-range dependencies and complex relational structures within large, sparse graphs. This limitation often hinders their performance in applications requiring reasoning over extended graph distances. To address this challenge, we introduce a novel Graph Transformer architecture, named ""Long-Horizon Relational Encoder (LHRE),"" that incorporates a multi-hop attention mechanism integrated with a learned structural encoding. LHRE leverages a sparse attention matrix, dynamically constructed based on shortest path distances between nodes, to mitigate the computational complexity typically associated with global attention. Furthermore, our structural encoding explicitly encodes node positional information and graph connectivity patterns. We evaluate LHRE on several benchmark datasets, including both synthetic and real-world graph learning tasks such as node classification, link prediction, and graph classification. Our results demonstrate that LHRE significantly outperforms existing GNN models and standard Graph Transformers, achieving state-of-the-art performance in capturing long-range relationships and improving prediction accuracy, particularly on graphs with complex structural properties. Ablation studies confirm the importance of both the multi-hop attention and the learned structural encoding components of LHRE.",ai
"Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.",human
"Machine-learning surrogate models have shown promise in accelerating aerodynamic design, yet progress toward generalizable predictors for three-dimensional wings has been limited by the scarcity and restricted diversity of existing datasets. Here, we present SuperWing, a comprehensive open dataset of transonic swept-wing aerodynamics comprising 4,239 parameterized wing geometries and 28,856 Reynolds-averaged Navier-Stokes flow field solutions. The wing shapes in the dataset are generated using a simplified yet expressive geometry parameterization that incorporates spanwise variations in airfoil shape, twist, and dihedral, allowing for an enhanced diversity without relying on perturbations of a baseline wing. All shapes are simulated under a broad range of Mach numbers and angles of attack covering the typical flight envelope. To demonstrate the dataset's utility, we benchmark two state-of-the-art Transformers that accurately predict surface flow and achieve a 2.5 drag-count error on held-out samples. Models pretrained on SuperWing further exhibit strong zero-shot generalization to complex benchmark wings such as DLR-F6 and NASA CRM, underscoring the dataset's diversity and potential for practical usage.",human
"Existing feature engineering methods based on large language models (LLMs) have not yet been applied to multi-label learning tasks. They lack the ability to model complex label dependencies and are not specifically adapted to the characteristics of multi-label tasks. To address the above issues, we propose Feature Engineering Automation for Multi-Label Learning (FEAML), an automated feature engineering method for multi-label classification which leverages the code generation capabilities of LLMs. By utilizing metadata and label co-occurrence matrices, LLMs are guided to understand the relationships between data features and task objectives, based on which high-quality features are generated. The newly generated features are evaluated in terms of model accuracy to assess their effectiveness, while Pearson correlation coefficients are used to detect redundancy. FEAML further incorporates the evaluation results as feedback to drive LLMs to continuously optimize code generation in subsequent iterations. By integrating LLMs with a feedback mechanism, FEAML realizes an efficient, interpretable and self-improving feature engineering paradigm. Empirical results on various multi-label datasets demonstrate that our FEAML outperforms other feature engineering methods.",human
"Medical Entity Recognition (MedER) is an essential NLP task for extracting meaningful entities from the medical corpus. Nowadays, MedER-based research outcomes can remarkably contribute to the development of automated systems in the medical sector, ultimately enhancing patient care and outcomes. While extensive research has been conducted on MedER in English, low-resource languages like Bangla remain underexplored. Our work aims to bridge this gap. For Bangla medical entity recognition, this study first examined a number of transformer models, including BERT, DistilBERT, ELECTRA, and RoBERTa. We also propose a novel Multi-BERT Ensemble approach that outperformed all baseline models with the highest accuracy of 89.58%. Notably, it provides an 11.80% accuracy improvement over the single-layer BERT model, demonstrating its effectiveness for this task. A major challenge in MedER for low-resource languages is the lack of annotated datasets. To address this issue, we developed a high-quality dataset tailored for the Bangla MedER task. The dataset was used to evaluate the effectiveness of our model through multiple performance metrics, demonstrating its robustness and applicability. Our findings highlight the potential of Multi-BERT Ensemble models in improving MedER for Bangla and set the foundation for further advancements in low-resource medical NLP.",human
"Approximate nearest neighbor search (ANN) is a common way to retrieve relevant search results, especially now in the context of large language models and retrieval augmented generation. One of the most widely used algorithms for ANN is based on constructing a multi-layer graph over the dataset, called the Hierarchical Navigable Small World (HNSW). While this algorithm supports insertion of new data, it does not support deletion of existing data. Moreover, deletion algorithms described by prior work come at the cost of increased query latency, decreased recall, or prolonged deletion time. In this paper, we propose a new theoretical framework for graph-based ANN based on random walks. We then utilize this framework to analyze a randomized deletion approach that preserves hitting time statistics compared to the graph before deleting the point. We then turn this theoretical framework into a deterministic deletion algorithm, and show that it provides better tradeoff between query latency, recall, deletion time, and memory usage through an extensive collection of experiments.",human
"When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty.",human
"We address the challenge of training machine learning models on sensitive datasets while minimizing the risk of privacy breaches. Existing approaches, such as differential privacy (DP), often necessitate substantial trade-offs between privacy guarantees and model utility, particularly in complex, high-dimensional settings. This paper introduces a novel privacy-preserving learning framework, Federated Averaging with Client-Specific Noise Injection (FA-CSNI), designed to mitigate these trade-offs. FA-CSNI leverages the principles of federated learning to distribute the training process across multiple clients, each possessing a local dataset. Critically, instead of applying a global noise perturbation, FA-CSNI injects noise that is specifically tailored to each client's local gradients, determined by a differential privacy budget allocated according to the client’s data sensitivity. We develop a rigorous theoretical analysis demonstrating that FA-CSNI provides ε-differential privacy under composition. Empirical evaluations on benchmark datasets (MNIST, CIFAR-10) show that FA-CSNI achieves significantly improved model accuracy compared to standard federated averaging with global DP noise injection, especially at stringent privacy levels (ε ≤ 1). Furthermore, FA-CSNI exhibits robustness to variations in client data heterogeneity and demonstrates superior performance compared to existing client-level DP methods.",ai
"Natural language understanding inherently depends on integrating multiple complementary perspectives spanning from surface syntax to deep semantics and world knowledge. However, current Aspect-Based Sentiment Analysis (ABSA) systems typically exploit isolated linguistic views, thereby overlooking the intricate interplay between structural representations that humans naturally leverage. We propose CMV-Fuse, a Cross-Modal View fusion framework that emulates human language processing by systematically combining multiple linguistic perspectives. Our approach systematically orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge integration. Through hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, CMV-Fuse captures both fine-grained structural patterns and broad contextual understanding. A novel structure aware multi-view contrastive learning mechanism ensures consistency across complementary representations while maintaining computational efficiency. Extensive experiments demonstrate substantial improvements over strong baselines on standard benchmarks, with analysis revealing how each linguistic view contributes to more robust sentiment analysis.",human
"Interactive reinforcement learning (IRL) has shown promise in enabling autonomous agents and robots to learn complex behaviours from human teachers, yet the dynamics of teacher selection remain poorly understood. This paper reveals an unexpected phenomenon in IRL: when given a choice between teachers with different reward structures, learning agents overwhelmingly prefer conservative, low-reward teachers (93.16% selection rate) over those offering 20x higher rewards. Through 1,250 experimental runs in navigation tasks with multiple expert teachers, we discovered: (1) Conservative bias dominates teacher selection: agents systematically choose the lowest-reward teacher, prioritising consistency over optimality; (2) Critical performance thresholds exist at teacher availability rho >= 0.6 and accuracy omega >= 0.6, below which the framework fails catastrophically; (3) The framework achieves 159% improvement over baseline Q-learning under concept drift. These findings challenge fundamental assumptions about optimal teaching in RL and suggest potential implications for human-robot collaboration, where human preferences for safety and consistency may align with the observed agent selection behaviour, potentially informing training paradigms for safety-critical robotic applications.",human
"Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.",human
"## Decentralized Task Allocation in Dynamic Environments with Adaptive Communication Topologies The problem of efficiently allocating tasks among agents in dynamic multi-agent systems, particularly when agents possess limited communication ranges and task availability fluctuates, remains a significant challenge. Static communication topologies often restrict adaptability to changing environmental conditions, hindering overall system performance. We propose a novel decentralized task allocation algorithm, termed Adaptive Communication Topology Task Allocation (ACTTA), which integrates a dynamic communication topology adjustment mechanism with a consensus-based task allocation protocol. ACTTA leverages a reinforcement learning approach to enable each agent to independently learn the optimal communication neighbors based on local observations of task availability and agent workload. The learned communication strategies facilitate faster convergence to consensus on task allocation, particularly in scenarios with spatiotemporal task dependencies. We evaluate ACTTA through simulations conducted in a dynamically changing warehouse environment, comparing its performance against established methods employing fixed communication topologies and centralized task allocation. Results demonstrate that ACTTA achieves significant improvements in overall task completion rate, reduction in agent idle time, and enhanced robustness to communication failures. Specifically, ACTTA exhibits a 15-20% increase in task completion compared to static topology baselines and maintains performance under communication noise levels where centralized approaches fail. These findings indicate that adaptive communication topologies significantly enhance the efficiency and resilience of decentralized multi-agent task allocation systems.",ai
"Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead. Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.",human
"We address the challenge of accurate and efficient automated segmentation of pulmonary nodules in computed tomography (CT) scans, a task critical for early lung cancer diagnosis. Manual segmentation is time-consuming and prone to inter-observer variability. Current deep learning approaches often require substantial computational resources and large, labeled datasets, limiting their applicability in resource-constrained environments. We propose a novel multi-resolution residual U-Net architecture incorporating spatial attention mechanisms and trained using a semi-supervised learning framework. Specifically, we leverage a smaller set of fully annotated CT volumes in conjunction with a larger set of unlabeled volumes, employing a consistency loss that enforces agreement between predictions generated under different perturbations applied to the unlabeled data. The spatial attention modules dynamically re-weight feature maps, enhancing the network's focus on nodule regions and suppressing irrelevant background noise. Experimental results on the publicly available LIDC-IDRI dataset demonstrate that our approach achieves competitive segmentation performance, measured by Dice score (0.82 ± 0.03) and volumetric overlap error (15.7 ± 4.1%), compared to fully supervised models trained with significantly more labeled data. Furthermore, our semi-supervised training paradigm reduces the annotation burden without compromising accuracy, offering a practical solution for medical image analysis where labeled data acquisition is expensive and time-consuming.",ai
"Graph Neural Networks (GNNs) have demonstrated efficacy in learning representations for graph-structured data across diverse domains. However, existing GNN architectures often struggle with long-range dependencies and over-smoothing, limiting their ability to effectively process complex graph structures. This work addresses these limitations by introducing a novel graph neural network architecture, Long Range Graph Transformer (LRGT), which incorporates a learned attention mechanism inspired by Transformer models to capture long-range relationships between nodes. LRGT employs a graph-aware attention module that considers both node features and structural information during the attention calculation, enabling the model to selectively attend to relevant nodes regardless of their proximity in the graph. Furthermore, we introduce a multi-scale aggregation strategy that combines information from different layers of the network, mitigating the over-smoothing issue. We evaluate LRGT on a suite of benchmark graph classification and node classification datasets, including those with known long-range dependencies. Empirical results demonstrate that LRGT consistently outperforms state-of-the-art GNN models, achieving significant improvements in accuracy and F1-score. Ablation studies confirm the importance of both the graph-aware attention and the multi-scale aggregation components in achieving these performance gains. These findings highlight the potential of incorporating Transformer-inspired attention mechanisms into GNN architectures for improved graph representation learning.",ai
"Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\% [95\% CI 98.2--100], specificity 83.1\% [95\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\% [95\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.",human
"We investigate the problem of robust optimization in environments characterized by uncertainty in the objective function parameters, specifically addressing scenarios where traditional robust counterparts lead to overly conservative solutions. We propose a novel methodology, the Distributionally Robust Optimization with Kernel Embedding (DROKE), which leverages kernel embedding techniques to model the ambiguity set encompassing the uncertain parameters. DROKE employs a data-driven approach, utilizing observed parameter samples to construct a non-parametric representation of the underlying probability distribution via kernel density estimation. This distribution is then incorporated into a robust optimization framework, allowing for probabilistic guarantees on constraint satisfaction. The resulting optimization problem is formulated as a semi-definite program (SDP), enabling efficient solution through established solvers. We evaluate the performance of DROKE on benchmark portfolio optimization and power system dispatch problems, comparing it against classical box uncertainty and budget uncertainty approaches. Results demonstrate that DROKE achieves a significant improvement in out-of-sample performance, yielding solutions that are less conservative and exhibit superior robustness to unseen parameter variations. Specifically, DROKE attains a 15-20% reduction in cost compared to traditional robust counterparts while maintaining comparable levels of constraint satisfaction under various uncertainty realizations. The efficacy of DROKE is further validated through sensitivity analysis, examining the impact of kernel parameters and sample size on the robustness and optimality of the solutions.",ai
"We address the challenge of robust causal effect estimation in observational studies when facing unobserved confounding and selection bias. Existing methods often rely on strong assumptions regarding the functional form of the causal relationships or the absence of specific types of biases. We propose a novel approach, leveraging a variational autoencoder (VAE) framework integrated with instrumental variable (IV) techniques, to simultaneously learn latent confounders and estimate causal effects. Our method, dubbed ""Latent Causal VAE with Instrumental Variables (LCV-IV),"" learns a low-dimensional representation of the observed variables, explicitly modelling the influence of unobserved confounders within the latent space. The IV component is incorporated as a regularization term during training, encouraging the learned latent representation to be conditionally independent of the treatment variable given the instrument. We evaluate LCV-IV on both synthetic datasets with known ground truth and real-world observational datasets. Our experiments demonstrate that LCV-IV consistently outperforms existing methods in terms of bias reduction and accuracy of causal effect estimation, particularly in scenarios with strong confounding and selection bias. Furthermore, we provide empirical evidence suggesting that LCV-IV is more robust to violations of the instrumental variable assumptions compared to traditional IV-based estimators.",ai
"Graph Neural Networks (GNNs) have demonstrated significant efficacy in learning representations for graph-structured data across diverse domains. However, many existing GNN architectures struggle to effectively capture long-range dependencies and complex relational patterns inherent in large, heterogeneous graphs. This limitation often stems from the inherent neighborhood aggregation paradigm, which can lead to over-smoothing and difficulty in propagating information across distant nodes. This work introduces a novel GNN architecture, the Relation-Aware Transformer Graph Network (RATGN), designed to mitigate these limitations. RATGN leverages a transformer-based attention mechanism to directly model relationships between all node pairs in the graph, circumventing the need for iterative message passing. Furthermore, it incorporates relation-specific embeddings to encode the diverse edge types present in heterogeneous graphs, enabling the model to differentiate between various forms of interaction. Empirical evaluation on several benchmark graph classification and node classification datasets, including OGBN-Arxiv and ACM, demonstrates that RATGN achieves state-of-the-art performance. Specifically, RATGN improves upon existing GNNs by an average of 5% in classification accuracy on these datasets, highlighting its ability to effectively capture long-range dependencies and complex relational information for enhanced graph representation learning.",ai
"Neural Architecture Search is a powerful approach for automating model design, but existing methods struggle to accurately optimize for real hardware performance, often relying on proxy metrics such as bit operations. We present Surrogate Neural Architecture Codesign Package (SNAC-Pack), an integrated framework that automates the discovery and optimization of neural networks focusing on FPGA deployment. SNAC-Pack combines Neural Architecture Codesign's multi-stage search capabilities with the Resource Utilization and Latency Estimator, enabling multi-objective optimization across accuracy, FPGA resource utilization, and latency without requiring time-intensive synthesis for each candidate model. We demonstrate SNAC-Pack on a high energy physics jet classification task, achieving 63.84% accuracy with resource estimation. When synthesized on a Xilinx Virtex UltraScale+ VU13P FPGA, the SNAC-Pack model matches baseline accuracy while maintaining comparable resource utilization to models optimized using traditional BOPs metrics. This work demonstrates the potential of hardware-aware neural architecture search for resource-constrained deployments and provides an open-source framework for automating the design of efficient FPGA-accelerated models.",human
"We address the challenge of efficiently reasoning with large-scale knowledge graphs containing complex relational information. Existing reasoners often struggle to scale to graphs with millions of entities and relations, exhibiting performance bottlenecks due to exhaustive search or reliance on specific graph structures. This paper introduces a novel approach, *Differential Property Propagation (DPP)*, which leverages a combination of approximate reasoning and targeted refinement to improve reasoning efficiency without sacrificing accuracy. DPP utilizes a differentiable message passing framework to propagate property embeddings across the graph, providing a coarse-grained initial inference. Subsequently, a selective refinement module, guided by uncertainty estimates derived from the propagation process, focuses computational resources on areas of the graph where the initial inference is least confident. This allows for a more precise and locally constrained exploration of the knowledge graph. Experiments conducted on benchmark knowledge graphs (FB15k-237, WN18RR) demonstrate that DPP achieves comparable or superior accuracy to state-of-the-art rule-based and embedding-based reasoners, while exhibiting significant improvements in inference speed, particularly on complex reasoning tasks requiring multiple inference steps. We further analyze the impact of different uncertainty metrics on the refinement module's effectiveness, providing insights into the optimal configuration of DPP for various knowledge graph characteristics.",ai
"We address the problem of sample inefficiency in reinforcement learning (RL) when applied to complex, high-dimensional environments with sparse rewards. Traditional RL algorithms often require a prohibitively large number of interactions with the environment to learn effective policies. To mitigate this limitation, we propose a novel approach combining a hierarchical reinforcement learning framework with a goal-conditioned imitation learning component. The hierarchical structure decomposes the task into a set of sub-goals, enabling exploration to be guided by a higher-level policy. The goal-conditioned imitation learning module leverages a small set of expert demonstrations to initialize the lower-level policy, facilitating faster learning and improved exploration. We evaluate our proposed method on a challenging robotic manipulation task within a simulated environment. Results demonstrate a significant improvement in sample efficiency compared to baseline algorithms such as Proximal Policy Optimization (PPO) and Hindsight Experience Replay (HER). Specifically, our approach achieves a 30% increase in success rate with a comparable number of environment interactions and demonstrates enhanced robustness to changes in the environment dynamics. Furthermore, we analyze the learned hierarchical policy and demonstrate that the sub-goals identified by the higher-level policy effectively decompose the task into manageable sub-problems.",ai
"**Abstract:** We address the problem of decentralized task allocation in dynamic multi-agent systems operating under communication constraints and partial observability. Traditional approaches often rely on centralized coordination or require agents to maintain complete knowledge of the global state, which is impractical in many real-world scenarios. We propose a novel distributed algorithm, Decentralized Task Allocation via Belief Propagation and Consensus (DTABPC), that combines belief propagation for local inference with a consensus mechanism to achieve global agreement on task assignments. DTABPC allows each agent to maintain a belief distribution over possible task allocations based on its local observations and messages received from neighboring agents. These beliefs are iteratively updated using belief propagation, while a consensus protocol encourages agents to converge towards a mutually consistent allocation. We evaluate DTABPC in a simulated foraging environment with varying degrees of communication range and environmental complexity. Our results demonstrate that DTABPC outperforms existing decentralized task allocation algorithms, including auction-based methods and reinforcement learning approaches, in terms of overall task completion rate and agent efficiency. Specifically, DTABPC exhibits improved robustness to communication failures and demonstrates superior scalability as the number of agents and tasks increases. Furthermore, we observe that the consensus mechanism effectively mitigates the effects of partial observability, leading to more coordinated and efficient task execution.",ai
"Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.",human
"We investigate the problem of distributionally robust optimization (DRO) for machine learning models trained on datasets susceptible to adversarial corruption. Standard empirical risk minimization (ERM) exhibits vulnerability to even small deviations between the training and deployment distributions. To mitigate this, we propose a novel DRO framework based on Wasserstein ambiguity sets centered around the empirical distribution. Our method leverages a stochastic proximal point algorithm to efficiently solve the resulting min-max optimization problem, offering a computationally tractable alternative to existing DRO approaches. The key innovation lies in a dynamic sampling strategy that adapts to the uncertainty in the estimated worst-case distribution, thereby accelerating convergence. Empirical evaluations on image classification tasks with adversarial label noise and feature corruption demonstrate that our approach significantly enhances robustness compared to ERM and other state-of-the-art DRO methods. Specifically, our method achieves a relative improvement of up to 15% in classification accuracy under various levels of data contamination, while maintaining comparable performance on clean data. Furthermore, we provide theoretical guarantees on the convergence rate of our algorithm and its generalization performance.",ai
"The evaluation of Retrieval-Augmented Generation (RAG) systems typically examines retrieval quality and generation parameters like temperature in isolation, overlooking their interaction. This work presents a systematic investigation of how text perturbations (simulating noisy retrieval) interact with temperature settings across multiple LLM runs. We propose a comprehensive RAG Perturbation-Temperature Analysis Framework that subjects retrieved documents to three distinct perturbation types across varying temperature settings. Through extensive experiments on HotpotQA with both open-source and proprietary LLMs, we demonstrate that performance degradation follows distinct patterns: high-temperature settings consistently amplify vulnerability to perturbations, while certain perturbation types exhibit non-linear sensitivity across the temperature range. Our work yields three key contributions: (1) a diagnostic benchmark for assessing RAG robustness, (2) an analytical framework for quantifying perturbation-temperature interactions, and (3) practical guidelines for model selection and parameter tuning under noisy retrieval conditions.",human
"We address the challenge of training machine learning models on decentralized data while preserving individual data privacy. Traditional federated learning (FL) methodologies, while distributed, are susceptible to privacy breaches through gradient leakage. We propose a novel Differentially Private Federated Learning (DP-FL) algorithm leveraging a combination of secure aggregation and client-level differential privacy mechanisms. Our approach incorporates a calibrated noise addition strategy tailored to the sensitivity of local model updates, ensuring rigorous privacy guarantees without compromising model utility. Further, we introduce a dynamic clipping norm adaptation technique to minimize the impact of noise on model performance. We rigorously analyze the privacy guarantees of our algorithm using Rényi Differential Privacy (RDP) accounting, providing tight bounds on the overall privacy loss. Empirical evaluation on benchmark datasets demonstrates that our DP-FL algorithm achieves a significant improvement in model accuracy compared to existing DP-FL methods for comparable privacy budgets (ε, δ). Specifically, we observe a 5-10% increase in accuracy on the MNIST and CIFAR-10 datasets for ε values ranging from 2 to 8. This validates the effectiveness of our noise calibration and dynamic clipping strategies in mitigating the performance degradation typically associated with differential privacy.",ai
"Recent advancements in large language models (LLMs) have spurred interest in their potential as autonomous agents capable of complex reasoning and collaboration within multi-agent systems. However, effectively coordinating LLM-based agents remains a challenge, particularly in scenarios requiring nuanced communication and adaptability to changing environmental conditions. This paper addresses the problem of optimizing collaborative task completion in multi-agent systems where agents are instantiated as LLMs with limited communication bandwidth. We propose a novel method, Collaborative Planning with Iterative Refinement (CPIR), which integrates hierarchical planning, distributed execution, and iterative feedback mechanisms. CPIR enables agents to collaboratively decompose tasks into sub-goals, communicate intermediate plans, and dynamically adjust their strategies based on observations of other agents' actions and environmental state. We evaluate CPIR on a simulated resource allocation task and a collaborative problem-solving environment. Our experimental results demonstrate that CPIR significantly outperforms baseline methods, including direct instruction and simple turn-taking strategies, achieving a 25% increase in task completion rate and a 15% reduction in communication overhead. These findings suggest that CPIR offers a promising approach for enabling robust and efficient collaboration in multi-agent systems leveraging LLMs.",ai
"A central goal of interpretability is to recover representations of causally relevant concepts from the activations of neural networks. The quality of these concept representations is typically evaluated in isolation, and under implicit independence assumptions that may not hold in practice. Thus, it is unclear whether common featurization methods - including sparse autoencoders (SAEs) and sparse probes - recover disentangled representations of these concepts. This study proposes a multi-concept evaluation setting where we control the correlations between textual concepts, such as sentiment, domain, and tense, and analyze performance under increasing correlations between them. We first evaluate the extent to which featurizers can learn disentangled representations of each concept under increasing correlational strengths. We observe a one-to-many relationship from concepts to features: features correspond to no more than one concept, but concepts are distributed across many features. Then, we perform steering experiments, measuring whether each concept is independently manipulable. Even when trained on uniform distributions of concepts, SAE features generally affect many concepts when steered, indicating that they are neither selective nor independent; nonetheless, features affect disjoint subspaces. These results suggest that correlational metrics for measuring disentanglement are generally not sufficient for establishing independence when steering, and that affecting disjoint subspaces is not sufficient for concept selectivity. These results underscore the importance of compositional evaluations in interpretability research.",human
"We address the problem of robust causal effect estimation in the presence of unobserved confounding and selection bias, a pervasive challenge in observational studies. Existing methods often rely on strong assumptions about the functional form of the causal relationships or the distribution of confounders, which are frequently violated in real-world scenarios. To mitigate these limitations, we propose a novel framework leveraging adversarial learning within a variational autoencoder (VAE) architecture. Our approach, termed Adversarial Variational Causal Inference (AVCI), learns a latent representation that disentangles the treatment effect from confounding factors by minimizing the mutual information between the latent representation and the treatment assignment, conditioned on observed covariates. Furthermore, we incorporate an adversarial discriminator to ensure that the latent representation is balanced across different treatment groups, thereby addressing selection bias. We evaluate AVCI on both synthetic datasets with varying degrees of confounding and selection bias, and real-world benchmark datasets for causal effect estimation. Our results demonstrate that AVCI consistently outperforms state-of-the-art methods in terms of estimation accuracy, particularly in settings with strong unobserved confounding and complex non-linear relationships. The proposed method offers a more robust and reliable approach for causal inference in observational data.",ai
"We address the problem of robust causal effect estimation in observational studies with unmeasured confounding. Existing methods often rely on strong assumptions regarding the functional form of the treatment assignment mechanism or the outcome model, or require access to instrumental variables which are often unavailable or weakly associated with the treatment. We propose a novel approach leveraging a learned representation of confounders based on Kernel Stein Discrepancy (KSD) minimization. Specifically, we minimize the KSD between the conditional distributions of observed covariates given the treatment and the outcome, respectively, aiming to learn a shared representation that captures the relevant confounding information. This learned representation is then incorporated into propensity score weighting or outcome regression models to estimate the average treatment effect (ATE). We theoretically demonstrate that, under certain smoothness conditions, minimizing the KSD leads to a consistent estimate of the confounder representation. Empirically, we evaluate our method on both synthetic and semi-synthetic datasets, demonstrating significant improvements in ATE estimation compared to state-of-the-art methods under various degrees of unmeasured confounding. Our results highlight the efficacy of KSD-based representation learning for robust causal inference in challenging observational settings.",ai
"We address the challenge of training machine learning models on sensitive data while providing rigorous privacy guarantees. A central problem lies in balancing model utility with the level of privacy protection, particularly when dealing with complex model architectures and high-dimensional data. We propose a novel privacy-preserving framework, Differentially Private Federated Averaging with Client-Specific Clipping (DP-FedAvg-CSC), designed to improve both the privacy guarantees and the performance of federated learning. DP-FedAvg-CSC introduces a client-specific adaptive clipping mechanism that leverages local data characteristics to minimize gradient norm inflation and reduce the noise required to achieve differential privacy. Furthermore, we incorporate a dynamic noise calibration strategy, adjusting the noise scale based on the observed gradient distribution across clients. We evaluate our approach on several benchmark datasets for image classification and natural language processing. Empirical results demonstrate that DP-FedAvg-CSC significantly outperforms existing state-of-the-art methods in terms of both accuracy and convergence rate under comparable privacy budgets. Specifically, we observed up to a 15% improvement in classification accuracy compared to standard differentially private federated averaging, while provably maintaining user-level differential privacy. Our findings highlight the effectiveness of client-specific adaptation and dynamic noise calibration in enhancing privacy-preserving machine learning.",ai
"Implicit Neural Representations (INRs) often converge slowly and struggle to recover high-frequency details due to spectral bias. While prior work links this behavior to the Neural Tangent Kernel (NTK), how specific architectural choices affect NTK conditioning remains unclear. We show that many INR mechanisms can be understood through their impact on a small set of pairwise similarity factors and scaling terms that jointly determine NTK eigenvalue variance. For standard coordinate MLPs, limited input-feature interactions induce large eigenvalue dispersion and poor conditioning. We derive closed-form variance decompositions for common INR components and show that positional encoding reshapes input similarity, spherical normalization reduces variance via layerwise scaling, and Hadamard modulation introduces additional similarity factors strictly below one, yielding multiplicative variance reduction. This unified view explains how diverse INR architectures mitigate spectral bias by improving NTK conditioning. Experiments across multiple tasks confirm the predicted variance reductions and demonstrate faster, more stable convergence with improved reconstruction quality.",human
"Atmospheric turbulence imposes a fundamental limitation across a broad range of applications, including optical imaging, remote sensing, and free-space optical communication. Recent advances in adaptive optics, wavefront shaping, and machine learning, driven by synergistic progress in fundamental theories, optoelectronic hardware, and computational algorithms, have demonstrated substantial potential in mitigating turbulence-induced distortions. Recently, active convolved illumination (ACI) was proposed as a versatile and physics-driven technique for transmitting structured light beams with minimal distortion through highly challenging turbulent regimes. While distinct in its formulation, ACI shares conceptual similarities with other physics-driven distortion correction approaches and stands to benefit from complementary integration with data-driven deep learning (DL) models. Inspired by recent work coupling deep learning with traditional turbulence mitigation strategies, the present work investigates the feasibility of integrating ACI with neural network-based methods. We outline a conceptual framework for coupling ACI with data-driven models and identify conditions under which learned representations can meaningfully support ACI's correlation-injection mechanism. As a representative example, we employ a convolutional neural network (CNN) together with a transfer-learning approach to examine how a learned model may operate in tandem with ACI. This exploratory study demonstrates feasible implementation pathways and establishes an early foundation for assessing the potential of future ACI-DL hybrid architectures, representing a step toward evaluating broader synergistic interactions between ACI and modern DL models.",human
"Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.",human
"Reinforcement learning (RL) agents operating in complex, natural language environments often struggle with sparse and delayed rewards, hindering efficient exploration and learning. This paper addresses the challenge of learning grounded language representations for improved decision-making in such settings. We introduce a novel hierarchical reinforcement learning framework, termed Contextual Abstraction with Language Grounding (CALG), which integrates a high-level policy for abstract goal selection with a low-level policy for primitive action execution. The high-level policy leverages a pre-trained language model fine-tuned on environment-specific language descriptions to generate contextual embeddings representing abstract goals. These embeddings are then used to guide the low-level policy, promoting exploration of relevant state-action spaces. We evaluate CALG on the TextWorld environment, a procedurally generated text-based game that requires understanding and reasoning about natural language instructions. Experimental results demonstrate that CALG significantly outperforms existing RL baselines, including those employing flat action spaces and hierarchical approaches without explicit language grounding. Specifically, CALG achieves a 35% improvement in task completion rate compared to the best-performing baseline, demonstrating its efficacy in learning and utilizing grounded language representations for effective navigation and problem-solving in complex, language-rich environments. These findings highlight the potential of CALG as a promising approach for developing RL agents capable of interacting with the world through natural language.",ai
"Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.",human
"Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.",human
"We address the challenge of scaling automated theorem proving in first-order logic with equality, specifically focusing on problems arising from formal verification. Traditional saturation-based provers often struggle with large theories containing numerous axioms and complex term structures, leading to search space explosion and inefficient resource utilization. We introduce a novel reasoning framework, Equality-Aware Term Indexing and Selection (EATIS), which leverages a combination of term indexing techniques and reinforcement learning to guide the proof search. EATIS incorporates an enhanced superposition calculus optimized for equality reasoning, coupled with a term index that dynamically prioritizes clauses based on their potential for generating relevant inferences. A reinforcement learning agent is trained to select promising clauses from the index, learning a policy that balances exploration and exploitation of the search space. Empirical evaluation on a suite of benchmark problems from the TPTP library and formal verification domains demonstrates that EATIS significantly outperforms state-of-the-art automated theorem provers, achieving higher success rates and requiring fewer inferences to prove theorems. The results indicate that EATIS effectively mitigates the search space explosion problem, enabling more efficient and scalable automated reasoning in complex logical theories.",ai
"We address the problem of efficiently verifying the satisfiability of quantified bit-vector formulas (QBV). QBV solving is a crucial component in many formal verification and synthesis tasks, but its computational complexity often limits its applicability. Current approaches frequently rely on quantifier elimination techniques, which can suffer from exponential blowup in formula size. We propose a novel approach leveraging conflict-driven clause learning (CDCL) within a counterexample-guided abstraction refinement (CEGAR) loop, specifically tailored to the bit-vector domain. Our method iteratively refines an abstract, quantifier-free bit-vector formula by analyzing spurious counterexamples produced by a standard SAT solver. The refinement process involves generating new clauses that block these counterexamples in subsequent SAT iterations, effectively strengthening the abstraction. A key contribution is a novel strategy for extracting useful clauses from the unsatisfiable core of the SAT solver's proof, focusing on bit-level dependencies within the bit-vector variables. Empirical evaluation on a benchmark suite of challenging QBV instances demonstrates that our CEGAR-based approach significantly outperforms state-of-the-art quantifier elimination solvers, achieving a substantial reduction in solving time and an increased number of solved instances. We observe particularly strong performance on formulas with deeply nested quantifiers and complex bit-vector operations.",ai
"We investigate the problem of sample inefficiency in reinforcement learning (RL) when applied to complex, sparse-reward environments. Standard RL algorithms often require an exorbitant number of interactions to achieve acceptable performance, particularly when rewards are infrequent and delayed. To mitigate this, we propose a novel hierarchical reinforcement learning framework integrating intrinsic motivation and curriculum learning. Our method decomposes the task into a hierarchy of sub-goals, with a high-level manager policy learning to set sub-goals and a low-level worker policy learning to achieve them. Intrinsic rewards, based on surprise and progress towards previously observed states, guide the worker policy during exploration. Crucially, a curriculum learning strategy adaptively adjusts the difficulty of the sub-goals presented to the manager, starting with easier tasks and progressively increasing complexity. We evaluate our approach on a challenging simulated robotic manipulation environment with sparse rewards. Empirical results demonstrate that our hierarchical framework significantly outperforms state-of-the-art baselines, achieving a substantial reduction in sample complexity and a higher success rate in reaching the ultimate task goal. Specifically, our method achieves a 40% improvement in asymptotic performance and converges approximately 3 times faster than a comparable flat RL approach. Furthermore, analysis of the learned policies reveals the emergence of interpretable and reusable sub-skills, facilitating generalization to related tasks.",ai
"We address the problem of learning structured knowledge representations from unstructured textual data. Traditional approaches often struggle to capture nuanced relationships and dependencies between entities, leading to suboptimal performance in downstream tasks such as reasoning and information retrieval. We propose a novel method, Knowledge Graph Enhanced Variational Autoencoder (KGE-VAE), which combines the representational power of variational autoencoders with the structural information encoded in knowledge graphs. Specifically, KGE-VAE leverages a graph neural network to propagate information from a background knowledge graph to inform the latent space of a variational autoencoder trained on text corpora. This process encourages the learned latent representations to reflect both the semantic content of the text and the relational structure present in the knowledge graph. Experimental results on benchmark datasets for knowledge graph completion and relation extraction demonstrate that KGE-VAE significantly outperforms state-of-the-art methods, achieving improvements of up to 15% in mean reciprocal rank and hit rate. Furthermore, ablation studies confirm the importance of both the variational autoencoder and the knowledge graph components in achieving these results. Our findings suggest that KGE-VAE offers a promising approach for learning robust and interpretable knowledge representations from unstructured data.",ai
"We address the challenge of decentralized learning in multi-agent systems operating in non-stationary environments, where agent policies and environment dynamics evolve over time, leading to poor convergence and instability. Specifically, we consider the problem of achieving consensus among agents with heterogeneous reward structures and limited communication bandwidth, while simultaneously adapting to a dynamically changing environment. We propose a novel decentralized policy optimization algorithm, , which incorporates a proximal policy update step regularized by a consensus term. This term penalizes deviations from the average policy of the agent's neighborhood, promoting agreement and stability. To mitigate the impact of delayed and noisy communication, we employ a moving-average estimator for the neighborhood policy. Empirically, we evaluate CRPPO on a set of challenging benchmark tasks, including cooperative navigation and resource allocation scenarios, under varying degrees of non-stationarity. Results demonstrate that CRPPO significantly outperforms existing decentralized policy gradient algorithms, exhibiting faster convergence rates, improved policy stability, and higher asymptotic performance. Moreover, our analysis reveals that the consensus regularization effectively reduces policy divergence and mitigates the impact of adversarial policy updates from neighboring agents. These findings highlight the potential of CRPPO for enabling robust and scalable decentralized learning in dynamic multi-agent settings.",ai
"In low-resource languages like Burmese, classification tasks often fine-tune only the final classification layer, keeping pre-trained encoder weights frozen. While Multi-Layer Perceptrons (MLPs) are commonly used, their fixed non-linearity can limit expressiveness and increase computational cost. This work explores Kolmogorov-Arnold Networks (KANs) as alternative classification heads, evaluating Fourier-based FourierKAN, Spline-based EfficientKAN, and Grid-based FasterKAN-across diverse embeddings including TF-IDF, fastText, and multilingual transformers (mBERT, Distil-mBERT). Experimental results show that KAN-based heads are competitive with or superior to MLPs. EfficientKAN with fastText achieved the highest F1-score (0.928), while FasterKAN offered the best trade-off between speed and accuracy. On transformer embeddings, EfficientKAN matched or slightly outperformed MLPs with mBERT (0.917 F1). These findings highlight KANs as expressive, efficient alternatives to MLPs for low-resource language classification.",human
"We address the challenge of sample inefficiency in reinforcement learning (RL) for sparse reward environments. Standard off-policy algorithms often struggle to efficiently explore such environments, leading to protracted training times and suboptimal policies. To mitigate this, we propose a novel approach incorporating a learned intrinsic reward signal derived from a temporally consistent world model. Our method, termed ""Temporally Aligned Intrinsic Motivation (TAIM),"" trains a recurrent neural network to predict future states conditioned on the current state and action. Discrepancies between predicted and actual states are used to generate an intrinsic reward, guiding exploration towards areas of the state space where the agent's understanding is incomplete. Crucially, TAIM employs a temporal alignment loss to ensure the world model's predictions remain consistent over multiple time steps, preventing the intrinsic reward from becoming overly sensitive to minor stochastic variations. We evaluate TAIM on a suite of challenging sparse reward navigation tasks, demonstrating a significant improvement in sample efficiency and asymptotic performance compared to state-of-the-art off-policy RL algorithms and established intrinsic motivation techniques. Specifically, TAIM achieves a 30-50% reduction in the number of environment interactions required to reach expert-level performance, and in certain scenarios, discovers optimal policies where baseline methods fail entirely. These results highlight the potential of temporally consistent world models to enhance exploration in sparse reward reinforcement learning.",ai
"Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers show its effectiveness, practical adoption is hindered as existing implementations are often tied to unmaintained and isolated research codebases. To address this, we introduce promptolution, a unified and modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers while remaining agnostic to the underlying LLM implementation.",human
"Empirical power--law scaling has been widely observed across modern deep learning systems, yet its theoretical origins and scope of validity remain incompletely understood. The Generalized Resolution--Shell Dynamics (GRSD) framework models learning as spectral energy transport across logarithmic resolution shells, providing a coarse--grained dynamical description of training. Within GRSD, power--law scaling corresponds to a particularly simple renormalized shell dynamics; however, such behavior is not automatic and requires additional structural properties of the learning process. In this work, we identify a set of sufficient conditions under which the GRSD shell dynamics admits a renormalizable coarse--grained description. These conditions constrain the learning configuration at multiple levels, including boundedness of gradient propagation in the computation graph, weak functional incoherence at initialization, controlled Jacobian evolution along training, and log--shift invariance of renormalized shell couplings. We further show that power--law scaling does not follow from renormalizability alone, but instead arises as a rigidity consequence: once log--shift invariance is combined with the intrinsic time--rescaling covariance of gradient flow, the renormalized GRSD velocity field is forced into a power--law form.",human
"In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.",human
"**Abstract:** The segmentation of anatomical structures in medical imaging is a critical task for diagnosis, treatment planning, and surgical navigation. However, manual segmentation is time-consuming and subject to inter-observer variability. We address the challenge of automated segmentation of the left atrium (LA) in cardiac magnetic resonance (CMR) images, a task complicated by significant shape variation and subtle boundary delineation. This study proposes a novel deep learning architecture, termed Context-Aware Attentive U-Net (CAAU-Net), designed to leverage both global contextual information and local boundary details. CAAU-Net integrates a spatial attention mechanism that adaptively weights feature maps based on their relevance to the target structure. Furthermore, we incorporate a multi-scale contextual pathway to capture long-range dependencies, enabling the model to better differentiate the LA from surrounding tissues. We evaluate CAAU-Net on a dataset of 200 3D CMR volumes, comparing its performance against a standard U-Net architecture and other state-of-the-art segmentation methods. Quantitative results demonstrate that CAAU-Net achieves significantly improved Dice scores (mean ± standard deviation: 0.91 ± 0.03) and Hausdorff distances (4.2 ± 1.1 mm) compared to baseline models. Qualitative analysis further confirms the enhanced ability of CAAU-Net to accurately segment the LA, particularly in challenging regions with poor contrast and complex anatomical variations. This work provides a promising approach for automating LA segmentation in CMR images, potentially leading to improved clinical workflows.",ai
"Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.",human
"Dynamical systems models such as recurrent neural networks (RNNs) are increasingly popular in theoretical neuroscience for hypothesis-generation and data analysis. Evaluating the dynamics in such models is key to understanding their learned generative mechanisms. However, such evaluation is impeded by two major challenges: First, comparison of learned dynamics across models is difficult because there is no enforced equivalence of their coordinate systems. Second, identification of mechanistically important low-dimensional motifs (e.g., limit sets) is intractable in high-dimensional nonlinear models such as RNNs. Here, we propose a comprehensive framework to address these two issues, termed Diffeomorphic vector field alignment FOR learned Models (DFORM). DFORM learns a nonlinear coordinate transformation between the state spaces of two dynamical systems, which aligns their trajectories in a maximally one-to-one manner. In so doing, DFORM enables an assessment of whether two models exhibit topological equivalence, i.e., similar mechanisms despite differences in coordinate systems. A byproduct of this method is a means to locate dynamical motifs on low-dimensional manifolds embedded within higher-dimensional systems. We verified DFORM's ability to identify linear and nonlinear coordinate transformations using canonical topologically equivalent systems, RNNs, and systems related by nonlinear flows. DFORM was also shown to provide a quantification of similarity between topologically distinct systems. We then demonstrated that DFORM can locate important dynamical motifs including invariant manifolds and saddle limit sets within high-dimensional models. Finally, using a set of RNN models trained on human functional MRI (fMRI) recordings, we illustrated that DFORM can identify limit cycles from high-dimensional data-driven models, which agreed well with prior numerical analysis.",human
"Diffusion models struggle to produce samples that respect constraints, a common requirement in scientific applications. Recent approaches have introduced regularization terms in the loss or guidance methods during sampling to enforce such constraints, but they bias the generative model away from the true data distribution. This is a problem, especially when the constraint is misspecified, a common issue when formulating constraints on scientific data. In this paper, instead of changing the loss or the sampling loop, we integrate a guidance-inspired adjustment into the denoiser itself, giving it a soft inductive bias towards constraint-compliant samples. We show that these softly constrained denoisers exploit constraint knowledge to improve compliance over standard denoisers, and maintain enough flexibility to deviate from it when there is misspecification with observed data.",human
"Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.",human
"We address the critical challenge of privacy preservation in machine learning (ML) deployments, specifically focusing on scenarios where sensitive user data is required for model training. Existing approaches, such as differential privacy, often introduce significant utility trade-offs, limiting their applicability in high-stakes domains. This work introduces a novel Federated Learning (FL) framework integrating homomorphic encryption (HE) and secure aggregation to mitigate privacy risks while maintaining model accuracy. Our method leverages a ciphertext-based aggregation scheme, enabling the server to compute the global model update without accessing individual user data in the clear. We employ a carefully designed HE scheme optimized for the specific computational demands of deep neural network training. Furthermore, we introduce a gradient sparsification technique to reduce communication overhead inherent in FL, thereby improving scalability. We evaluate our framework on benchmark datasets for image classification and natural language processing, demonstrating that our approach achieves comparable accuracy to centralized training with significantly enhanced privacy guarantees. Empirically, our method reduces the privacy loss (measured by epsilon) by orders of magnitude compared to standard differentially private FL, while incurring a minimal performance degradation of less than 2% in accuracy for ResNet-18 on CIFAR-10. Our results highlight the potential of HE-based FL as a practical solution for privacy-preserving ML in real-world applications.",ai
"We investigate the problem of sample-efficient reinforcement learning in partially observable environments, specifically addressing scenarios where the underlying state dynamics are subject to non-stationary stochastic noise. Traditional reinforcement learning algorithms often struggle in such environments due to the curse of history, necessitating long trajectories for accurate state estimation. We propose a novel approach, Noise-Aware Predictive State Representation Learning (NAPSRL), which explicitly models the uncertainty inherent in state transitions. NAPSRL leverages a recurrent neural network architecture to learn a latent state representation conditioned not only on past observations but also on an estimated noise distribution. This distribution is learned concurrently using a variational autoencoder, enabling the agent to disentangle deterministic dynamics from stochastic fluctuations. Empirically, we evaluate NAPSRL on a suite of partially observable grid-world environments with varying levels of dynamic noise. Results demonstrate that NAPSRL significantly outperforms existing state-of-the-art methods, including DRQN and Deep Variational Information Bottleneck, in terms of both sample efficiency and asymptotic performance. Ablation studies further validate the importance of explicit noise modeling in enhancing robustness and generalization to unseen noise levels. These findings highlight the potential of NAPSRL for addressing challenging reinforcement learning problems in real-world scenarios characterized by uncertainty and partial observability.",ai
"We address the challenge of achieving coordinated exploration in decentralized multi-agent reinforcement learning (MARL) environments characterized by sparse rewards and partial observability. Standard independent learning approaches suffer from the curse of dimensionality in the joint action space and struggle to discover effective exploration strategies. We propose a novel method, Cooperative Trajectory Partitioning (CTP), that leverages communication to dynamically decompose and recombine exploration trajectories across agents. CTP explicitly encourages agents to specialize in exploring different segments of the state-action space by sharing information about trajectory utility, quantified by realized rewards and entropy of future actions. This specialization facilitates the discovery of diverse and potentially synergistic exploration paths. To mitigate the impact of non-stationarity, we employ a decentralized critic that learns a factored value function, enabling each agent to evaluate the contribution of its individual actions to the team's overall performance. Empirical evaluations on a suite of challenging cooperative navigation and resource gathering tasks demonstrate that CTP significantly outperforms independent learners and existing communication-based MARL algorithms. Specifically, CTP achieves higher cumulative rewards and faster learning convergence, indicating its effectiveness in coordinating exploration and discovering optimal joint policies in complex, decentralized environments.",ai
"## Privacy-Preserving Natural Language Inference via Federated Learning and Differential Privacy The proliferation of Natural Language Processing (NLP) models has led to increased reliance on sensitive user data for training, raising significant privacy concerns. This work addresses the problem of training robust Natural Language Inference (NLI) models while mitigating data leakage risks. We propose a novel federated learning framework incorporating differential privacy mechanisms specifically tailored for NLI tasks. Our approach, Federated Differentially Private NLI (FedDP-NLI), employs a hierarchical aggregation strategy, allowing for both local model updates within client devices and global model aggregation on a central server. Crucially, we introduce noise injection at both the sentence embedding level and the final classification layer to satisfy differential privacy guarantees. We evaluate FedDP-NLI on the GLUE benchmark dataset, specifically focusing on the MNLI and QQP tasks. Our experiments demonstrate that FedDP-NLI achieves competitive performance compared to centrally trained models, even under stringent privacy budgets (ε ≤ 1.0). Further analysis reveals that the proposed hierarchical noise injection strategy effectively balances utility and privacy, significantly outperforming simpler approaches that add noise only at the classification layer. These results highlight the feasibility and efficacy of FedDP-NLI in enabling privacy-preserving training of NLI models in decentralized settings.",ai
"The accurate and efficient segmentation of anatomical structures in medical imaging is crucial for diagnosis, treatment planning, and surgical navigation. However, the inherent complexity and variability of medical images, coupled with the time-consuming nature of manual annotation, pose significant challenges. This work addresses the problem of automated segmentation of computed tomography (CT) scans of the liver using a novel deep learning framework. We propose a multi-scale, attention-guided convolutional neural network (MSAG-CNN) architecture that leverages information from multiple resolutions to capture both global context and fine-grained details. Attention mechanisms are integrated at each scale to emphasize relevant features and suppress noise, thereby improving segmentation accuracy. The proposed MSAG-CNN was trained and evaluated on a publicly available dataset of liver CT scans. Experimental results demonstrate that the proposed approach achieves a Dice similarity coefficient (DSC) of 0.92 ± 0.03 and an Intersection over Union (IoU) of 0.86 ± 0.04, surpassing the performance of several state-of-the-art segmentation methods. Furthermore, the model exhibits robustness to variations in image quality and anatomical shape, indicating its potential for clinical translation. These findings suggest that the MSAG-CNN framework provides a valuable tool for automated liver segmentation in CT imaging.",ai
"The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.",human
"Industry 4.0 is growing quickly, which has changed smart production by encouraging the use of real-time tracking, machine learning, and AI-driven systems to make operations run more smoothly. The main focus of this dissertation is on creating and testing an intelligent production system for XRIT that solves important problems like energy management, predictive maintenance, and AI-powered decision support. Machine learning models are built into the system, such as the Random Forest Classifier for proactive maintenance and the Isolation Forest for finding outliers. These models help with decision-making and reducing downtime. Streamlit makes real-time data visualisation possible, giving workers access to dashboards that they can interact with and see real-time observations.The system was tested with fake data and is made to be scalable, so it can be used in real time in XRIT's production setting. Adding an AI-powered virtual assistant made with GPT-4 lets workers get real-time, useful information that makes complicated questions easier to answer and improves operational decisions. The testing shows that the system makes working efficiency, energy management, and the ability to plan repairs much better. Moving the system to real-time data merging and looking for other ways to make it better will be the main focus of future work.",human
"We address the challenge of sample inefficiency in reinforcement learning (RL) for continuous control tasks with sparse rewards, focusing on environments where exploration is critical for discovering goal states. Current state-of-the-art methods often struggle in such scenarios due to the difficulty in learning effective exploration policies. We propose a novel approach integrating a learned intrinsic motivation signal based on a variational autoencoder (VAE) with a hierarchical RL framework. The VAE is trained to reconstruct past states, and the reconstruction error serves as an intrinsic reward, encouraging the agent to visit novel states. This intrinsic reward signal is then used within a two-level hierarchy: a meta-controller responsible for setting subgoals in the latent space of the VAE, and a lower-level controller trained to achieve these subgoals using a proximal policy optimization (PPO) algorithm. We evaluate our method on a suite of challenging continuous control tasks with sparse rewards, demonstrating significant improvements in sample efficiency and overall performance compared to baseline algorithms including vanilla PPO, curiosity-driven exploration, and hindsight experience replay. Specifically, our approach achieves a 2-3x increase in success rate and a faster learning speed on tasks requiring complex exploration strategies. The results highlight the effectiveness of combining learned intrinsic motivation with hierarchical control for addressing sample inefficiency in sparse reward RL environments.",ai
"Federated Learning (FL) enables collaborative model training across distributed devices while safeguarding data and user privacy. However, FL remains susceptible to privacy threats that can compromise data via direct means. That said, indirectly compromising the confidentiality of the FL model architecture (e.g., a convolutional neural network (CNN) or a recurrent neural network (RNN)) on a client device by an outsider remains unexplored. If leaked, this information can enable next-level attacks tailored to the architecture. This paper proposes a novel side-channel fingerprinting attack, leveraging flow-level and packet-level statistics of encrypted wireless traffic from an FL client to infer its deep learning model architecture. We name it FLARE, a fingerprinting framework based on FL Architecture REconnaissance. Evaluation across various CNN and RNN variants-including pre-trained and custom models trained over IEEE 802.11 Wi-Fi-shows that FLARE achieves over 98% F1-score in closed-world and up to 91% in open-world scenarios. These results reveal that CNN and RNN models leak distinguishable traffic patterns, enabling architecture fingerprinting even under realistic FL settings with hardware, software, and data heterogeneity. To our knowledge, this is the first work to fingerprint FL model architectures by sniffing encrypted wireless traffic, exposing a critical side-channel vulnerability in current FL systems.",human
"Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.",human
"We address the problem of identifying causal effects in natural language processing (NLP) tasks where observational data suffers from confounding. Traditional observational methods are often biased due to the presence of latent confounders that simultaneously influence both treatment assignment and outcome. We propose a novel framework, Counterfactual Language Modeling with Latent Confounders (CLM-LC), which integrates structural causal models (SCMs) with latent variable modeling to mitigate confounding bias in NLP. Specifically, CLM-LC learns a latent representation of confounders from the observational data using a variational autoencoder (VAE). This latent representation is then incorporated into a counterfactual language model, enabling estimation of potential outcomes under different treatment assignments. We evaluate CLM-LC on two benchmark NLP tasks: sentiment analysis with exposure to toxic language and text style transfer. Our results demonstrate that CLM-LC significantly reduces bias in causal effect estimation compared to standard observational methods, achieving improvements of up to 15% in terms of average treatment effect identification accuracy. Furthermore, we show that the learned latent confounder representation captures meaningful semantic information related to the bias present in the data. CLM-LC provides a principled approach to causal inference in NLP, offering a pathway to more reliable and fair downstream applications.",ai
"Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.",human
"Climate adaptation strategies are proposed in response to climate change. They are practised in agriculture to sustain food production. These strategies can be found in unstructured data (for example, scientific literature from the Elsevier website) or structured (heterogeneous climate data via government APIs). We present Climate Adaptation question-answering with Improved Readability and Noted Sources (CAIRNS), a framework that enables experts -- farmer advisors -- to obtain credible preliminary answers from complex evidence sources from the web. It enhances readability and citation reliability through a structured ScholarGuide prompt and achieves robust evaluation via a consistency-weighted hybrid evaluator that leverages inter-model agreement with experts. Together, these components enable readable, verifiable, and domain-grounded question-answering without fine-tuning or reinforcement learning. Using a previously reported dataset of expert-curated question-answers, we show that CAIRNS outperforms the baselines on most of the metrics. Our thorough ablation study confirms the results on all metrics. To validate our LLM-based evaluation, we also report an analysis of correlations against human judgment.",human
"We address the challenge of training machine learning models while preserving the privacy of the underlying training data. Specifically, we focus on scenarios where data is distributed across multiple parties and cannot be directly shared due to privacy regulations or competitive sensitivities. Our approach leverages a novel combination of federated learning and differential privacy mechanisms. We propose a differentially private federated averaging algorithm that incorporates Gaussian noise injection both at the individual client level and at the server aggregation level. This dual-level noise addition provides robust privacy guarantees against various adversarial attacks, including membership inference and model inversion. Furthermore, we introduce a dynamic noise adaptation strategy based on Rényi differential privacy accounting, which allows us to tighten privacy bounds and optimize the trade-off between model utility and privacy protection. Empirical evaluations on benchmark datasets demonstrate that our proposed method achieves comparable performance to non-private federated learning baselines, while provably satisfying strong differential privacy guarantees. We quantify the privacy-utility trade-off and showcase the effectiveness of our dynamic noise adaptation strategy in mitigating performance degradation associated with privacy constraints. The results highlight the feasibility of deploying privacy-preserving machine learning models in real-world distributed settings.",ai
"Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.",human
"Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.",human
"This paper addresses the challenging computational problem of estimating intractable expectations over discrete domains. Existing approaches, including Monte Carlo and Russian Roulette estimators, are consistent but often require a large number of samples to achieve accurate results. We propose a novel estimator, , which is an extension of Bayesian quadrature to discrete domains. It is more sample efficient than alternatives due to its ability to make use of prior information about the integrand through a Gaussian process. We show this through theory, deriving a convergence rate significantly faster than Monte Carlo in a broad range of settings. We also demonstrate empirically that our proposed method does indeed require fewer samples on several synthetic settings as well as for parameter estimation for Conway-Maxwell-Poisson and Potts models.",human
"Advancing artificial intelligence for physical sciences requires representations that are both interpretable and compatible with the underlying laws of nature. We introduce METASTRINGS, a symbolic language for photonics that expresses nanostructures as textual sequences encoding materials, geometries, and lattice configurations. Analogous to molecular textual representations in chemistry, METASTRINGS provides a framework connecting human interpretability with computational design by capturing the structural hierarchy of photonic metasurfaces. Building on this representation, we develop Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning. Across various design tasks, the model achieves <3% mean-squared spectral error and maintains >98% syntactic validity, generating diverse metasurface prototypes whose experimentally measured optical responses match their target spectra. These results demonstrate that Meta-GPT can learn the compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.",human
"Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.",human
"Inferring causal relationships from observational text data presents a significant challenge due to inherent confounding and selection biases. Traditional statistical methods often fail to accurately capture causal effects in such settings. This work addresses the problem of identifying causal effects of textual interventions on downstream outcomes, specifically focusing on sentiment modification through strategic phrase insertion. We propose a novel framework that integrates propensity score matching with structural causal models to mitigate confounding bias. The approach involves constructing a fine-grained representation of the text using contextualized embeddings, estimating the propensity score for treatment assignment (phrase insertion) conditioned on observed confounders, and employing matching techniques to create pseudo-randomized treatment and control groups. We further refine the causal effect estimation by incorporating instrumental variable regression to address potential unobserved confounding. Empirical evaluation on a benchmark dataset of product reviews demonstrates that our method achieves a substantial reduction in bias compared to standard regression techniques, yielding more accurate estimates of the causal effect of sentiment modification on product ratings. Results show that targeted insertion of positive phrases has a statistically significant and positive impact on product ratings, even after controlling for confounding factors such as review length and existing sentiment. This refined causal estimation provides valuable insights for applications such as personalized persuasion and counterfactual text generation.",ai
"Energy consumption is a growing concern in several fields, from mobile devices to large data centers. Developers need detailed data on the energy consumption of their software to mitigate consumption issues. Previous approaches have a broader focus, such as on specific functions or programs, rather than source code statements. They primarily focus on estimating the CPU's energy consumption using point estimates, thereby disregarding other hardware effects and limiting their use for statistical reasoning and explainability. We developed a novel methodology to address the limitations of measuring only the CPU's consumption and using point estimates, focusing on predicting the energy usage of statically typed JVM-based programming languages, such as Java and Scala. We measure the energy consumption of Bytecode patterns, the translation from the programming language's source code statement to their Java Bytecode representation. With the energy measurements, we construct a statistical model using Bayesian statistics, which allows us to predict the energy consumption through statistical distributions and analyze individual factors. The model includes three factors we obtain statically from the code: data size, data type, operation, and one factor about the hardware platform the code executes on: device. To validate our methodology, we implemented it for Java and evaluated its energy predictions on unseen programs. We observe that all four factors are influential, notably that two devices of the same model may differ in energy consumption and that the operations and data types cause consumption differences. The experiments also show that the energy prediction of programs closely follows the program's real energy consumption, validating our approach. Our work presents a methodology for constructing an energy model that future work, such as verification tools, can use for their energy estimates.",human
"De novo drug design is a crucial component of modern drug development, yet navigating the vast chemical space to find synthetically accessible, high-affinity candidates remains a significant challenge. Reinforcement Learning (RL) enhances this process by enabling multi-objective optimization and exploration of novel chemical space - capabilities that traditional supervised learning methods lack. In this work, we introduce , a fully integrated, target-agnostic molecular design framework based on Reinforcement Learning. Unlike models requiring target-specific fine-tuning, ReACT-Drug utilizes a generalist approach by leveraging ESM-2 protein embeddings to identify similar proteins for a given target from a knowledge base such as Protein Data Base (PDB). Thereafter, the known drug ligands corresponding to such proteins are decomposed to initialize a fragment-based search space, biasing the agent towards biologically relevant subspaces. For each such fragment, the pipeline employs a Proximal Policy Optimization (PPO) agent guiding a ChemBERTa-encoded molecule through a dynamic action space of chemically valid, reaction-template-based transformations. This results in the generation of drug candidates with competitive binding affinities and high synthetic accessibility, while ensuring 100\% chemical validity and novelty as per MOSES benchmarking. This architecture highlights the potential of integrating structural biology, deep representation learning, and chemical synthesis rules to automate and accelerate rational drug design. The dataset and code are available at https://github.com/YadunandanRaman/ReACT-Drug/.",human
"Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion. However, developing AI-driven chatbots for this purpose requires large, high-quality question-and-answer (Q&A) datasets, which are typically expensive and resource-intensive to produce, especially for low-resource languages like Persian. In this paper, we introduce MegaChat, the first fully synthetic Persian Q&A dataset designed to evaluate intelligent sales chatbots in Telegram-based e-commerce. We propose a novel, automated multi-agent architecture that generates persona-aware Q&A pairs by collecting data from active Telegram shopping channels. The system employs specialized agents for question generation, validation, and refinement, ensuring the production of realistic and diverse conversational data. To evaluate answer generation, we compare three classic retrieval-augmented generation (RAG) models with our advanced agentic system, which features multi-query retrieval, reranking, and persona-aligned response synthesis. Using GPT-5.1 for evaluation across six quality dimensions, our results show that the agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating its ability to generate scalable, high-quality datasets without relying on expensive human annotation or complex fine-tuning. MegaChat provides SMEs with an efficient, cost-effective solution for building intelligent customer engagement systems in specialized commercial domains, enabling advancements in multilingual conversational AI for low-resource languages. Download: https://github.com/MegaChat-Tech/MegaChat-DataSet",human
"We address the problem of decentralized policy learning in cooperative multi-agent systems operating in partially observable environments. Specifically, we consider scenarios where agents possess individual observation histories and must learn to coordinate their actions to maximize a shared reward. Existing approaches often rely on communication channels or centralized training schemes, limiting their applicability in realistic settings. We propose a novel method, Implicit Coordination via Predictive Re-estimation (ICPR), which leverages the concept of predictive state representations (PSRs) to implicitly facilitate coordination. ICPR trains each agent to predict the future observations and rewards of other agents, given its own observation history and a shared coordination prior. This prior is subsequently re-estimated using a Bayesian update rule based on the realized joint trajectories, fostering emergent coordination without explicit communication. We evaluate ICPR on a suite of challenging cooperative navigation tasks, demonstrating superior performance compared to state-of-the-art decentralized reinforcement learning algorithms. Furthermore, we provide an analysis of the learned PSRs, illustrating how ICPR enables agents to effectively infer the intentions and strategies of their teammates, leading to improved collaborative behavior and enhanced robustness in stochastic environments. The results suggest that implicit coordination through predictive modeling offers a promising direction for decentralized multi-agent learning.",ai
"The evaluation of Large Language Models (LLMs) in complex reasoning domains typically relies on performance alignment with ground-truth oracles. In the domain of chess, this standard manifests as accuracy benchmarks against strong engines like Stockfish. However, high scalar accuracy does not necessarily imply robust conceptual understanding. This paper argues that standard accuracy metrics fail to distinguish between genuine geometric reasoning and the superficial memorization of canonical board states. To address this gap, we propose a Geometric Stability Framework, a novel evaluation methodology that rigorously tests model consistency under invariant transformations-including board rotation, mirror symmetry, color inversion, and format conversion. We applied this framework to a comparative analysis of six state-of-the-art LLMs including GPT-5.1, Claude Sonnet 4.5, and Kimi K2 Turbo, utilizing a dataset of approximately 3,000 positions. Our results reveal a significant Accuracy-Stability Paradox. While models such as GPT-5.1 achieve near-optimal accuracy on standard positions, they exhibit catastrophic degradation under geometric perturbation, specifically in rotation tasks where error rates surge by over 600%. This disparity suggests a reliance on pattern matching over abstract spatial logic. Conversely, Claude Sonnet 4.5 and Kimi K2 Turbo demonstrate superior dual robustness, maintaining high consistency across all transformation axes. Furthermore, we analyze the trade-off between helpfulness and safety, identifying Gemini 2.5 Flash as the leader in illegal state rejection (96.0%). We conclude that geometric stability provides an orthogonal and essential metric for AI evaluation, offering a necessary proxy for disentangling reasoning capabilities from data contamination and overfitting in large-scale models.",human
"Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.",human
"Safe Bayesian optimization (BO) with Gaussian processes is an effective tool for tuning control policies in safety-critical real-world systems, specifically due to its sample efficiency and safety guarantees. However, most safe BO algorithms assume homoscedastic sub-Gaussian measurement noise, an assumption that does not hold in many relevant applications. In this article, we propose a straightforward yet rigorous approach for safe BO across noise models, including homoscedastic sub-Gaussian and heteroscedastic heavy-tailed distributions. We provide a high-probability bound on the measurement noise via the scenario approach, integrate these bounds into high probability confidence intervals, and prove safety and optimality for our proposed safe BO algorithm. We deploy our algorithm in synthetic examples and in tuning a controller for the Franka Emika manipulator in simulation.",human
"Agentic AI is increasingly being explored and introduced in both manually driven and autonomous vehicles, leading to the notion of Agentic Vehicles (AgVs), with capabilities such as memory-based personalization, goal interpretation, strategic reasoning, and tool-mediated assistance. While frameworks such as the OWASP Agentic AI Security Risks highlight vulnerabilities in reasoning-driven AI systems, they are not designed for safety-critical cyber-physical platforms such as vehicles, nor do they account for interactions with other layers such as perception, communication, and control layers. This paper investigates security threats in AgVs, including OWASP-style risks and cyber-attacks from other layers affecting the agentic layer. By introducing a role-based architecture for agentic vehicles, consisting of a Personal Agent and a Driving Strategy Agent, we will investigate vulnerabilities in both agentic AI layer and cross-layer risks, including risks originating from upstream layers (e.g., perception layer, control layer, etc.). A severity matrix and attack-chain analysis illustrate how small distortions can escalate into misaligned or unsafe behavior in both human-driven and autonomous vehicles. The resulting framework provides the first structured foundation for analyzing security risks of agentic AI in both current and emerging vehicle platforms.",human
"Electroencephalography (EEG) signal decoding is a key technology that translates brain activity into executable commands, laying the foundation for direct brain-machine interfacing and intelligent interaction. To address the inherent spatiotemporal heterogeneity of EEG signals, this paper proposes a multi-branch parallel architecture, where each temporal scale is equipped with an independent spatial feature extraction module. To further enhance multi-branch feature fusion, we propose a Fusion of Multiscale Features via Centralized Sparse-attention Network (EEG-CSANet), a centralized sparse-attention network. It employs a main-auxiliary branch architecture, where the main branch models core spatiotemporal patterns via multiscale self-attention, and the auxiliary branch facilitates efficient local interactions through sparse cross-attention. Experimental results show that EEG-CSANet achieves state-of-the-art (SOTA) performance across five public datasets (BCIC-IV-2A, BCIC-IV-2B, HGD, SEED, and SEED-VIG), with accuracies of 88.54%, 91.09%, 99.43%, 96.03%, and 90.56%, respectively. Such performance demonstrates its strong adaptability and robustness across various EEG decoding tasks. Moreover, extensive ablation studies are conducted to enhance the interpretability of EEG-CSANet. In the future, we hope that EEG-CSANet could serve as a promising baseline model in the field of EEG signal decoding. The source code is publicly available at: https://github.com/Xiangrui-Cai/EEG-CSANet",human
"### Abstract Accurate and efficient segmentation of anatomical structures in medical images is crucial for computer-aided diagnosis and treatment planning. However, the high variability in image quality, anatomical shape, and the presence of noise and artifacts pose significant challenges to existing segmentation algorithms. This paper introduces a novel deep learning-based framework for automated medical image segmentation, leveraging a multi-scale convolutional neural network (CNN) architecture coupled with attention mechanisms. The proposed method incorporates a U-Net backbone for coarse-grained segmentation, followed by a residual attention module to refine the initial segmentation map by selectively focusing on relevant features and suppressing irrelevant background noise. This attention module is designed to enhance the network's ability to discern subtle anatomical boundaries and improve robustness to variations in image contrast and resolution. We evaluate the performance of our framework on a publicly available dataset of abdominal CT scans, focusing on the segmentation of liver, kidneys, and spleen. Experimental results demonstrate that the proposed method achieves state-of-the-art performance, surpassing existing methods in terms of Dice coefficient, Jaccard index, and Hausdorff distance. Specifically, our framework achieves a mean Dice coefficient of 0.92 for liver, 0.88 for kidneys, and 0.90 for spleen, representing a significant improvement over existing deep learning-based approaches. These findings suggest that our method offers a promising solution for accurate and efficient medical image segmentation, with potential applications in clinical practice.",ai
"This paper studies a machine learning regression problem as a multivariate approximation problem using the framework of the theory of random functions. An ab initio derivation of a regression method is proposed, starting from postulates of indifference. It is shown that if a probability measure on an infinite-dimensional function space possesses natural symmetries (invariance under translation, rotation, scaling, and Gaussianity), then the entire solution scheme, including the kernel form, the type of regularization, and the noise parameterization, follows analytically from these postulates. The resulting kernel coincides with a generalized polyharmonic spline; however, unlike existing approaches, it is not chosen empirically but arises as a consequence of the indifference principle. This result provides a theoretical foundation for a broad class of smoothing and interpolation methods, demonstrating their optimality in the absence of a priori information.",human
"Image classification is hindered by subtle inter-class differences and substantial intra-class variations, which limit the effectiveness of existing contrastive learning methods. Supervised contrastive approaches based on the InfoNCE loss suffer from negative-sample dilution and lack adaptive decision boundaries, thereby reducing discriminative power in fine-grained recognition tasks. To address these limitations, we propose Sigmoid-based Common and Style Supervised Contrastive Learning (SCS-SupCon). Our framework introduces a sigmoid-based pairwise contrastive loss with learnable temperature and bias parameters to enable adaptive decision boundaries. This formulation emphasizes hard negatives, mitigates negative-sample dilution, and more effectively exploits supervision. In addition, an explicit style-distance constraint further disentangles style and content representations, leading to more robust feature learning. Comprehensive experiments on six benchmark datasets, including CUB200-2011 and Stanford Dogs, demonstrate that SCS-SupCon achieves state-of-the-art performance across both CNN and Transformer backbones. On CIFAR-100 with ResNet-50, SCS-SupCon improves top-1 accuracy over SupCon by approximately 3.9 percentage points and over CS-SupCon by approximately 1.7 points under five-fold cross-validation. On fine-grained datasets, it outperforms CS-SupCon by 0.4--3.0 points. Extensive ablation studies and statistical analyses further confirm the robustness and generalization of the proposed framework, with Friedman tests and Nemenyi post-hoc evaluations validating the stability of the observed improvements.",human
"Medical image analysis benefits significantly from automated systems capable of identifying subtle anomalies indicative of disease. However, current approaches often struggle with the high degree of variability in image acquisition parameters and the inherent ambiguity in pathological presentation. This paper presents a novel framework for medical image segmentation and classification, leveraging recent advances in convolutional neural networks (CNNs) and transfer learning. Specifically, we propose a multi-stage architecture that first employs a pre-trained ResNet-50 network, fine-tuned on a large dataset of diverse medical imaging modalities, to extract robust feature representations. These features are then fed into a modified U-Net architecture, optimized for precise segmentation of target regions. Finally, a dedicated classification module utilizes the segmented regions to predict disease presence and severity. We evaluated our framework on two distinct medical imaging datasets: a lung CT dataset for pulmonary nodule detection and a retinal fundus image dataset for diabetic retinopathy grading. Results demonstrate a significant improvement over baseline methods, achieving a mean Intersection over Union (IoU) of 0.85 for nodule segmentation and an Area Under the ROC Curve (AUC) of 0.92 for retinopathy grading. These findings suggest that our framework offers a promising approach to improve the accuracy and efficiency of medical image analysis, potentially leading to earlier and more accurate diagnoses.",ai
"We address the critical challenge of automated lesion detection and segmentation in medical imaging, specifically focusing on pulmonary nodule identification in computed tomography (CT) scans. Manual analysis of such scans is time-consuming and prone to inter-observer variability, hindering timely diagnosis and treatment. We propose a novel deep learning framework, termed Multi-Scale Context Aware Network (MSCANet), designed to leverage both local and global contextual information for improved nodule delineation. MSCANet incorporates a hierarchical convolutional architecture with residual connections to facilitate feature extraction across multiple scales. Furthermore, we introduce a novel attention mechanism that selectively emphasizes relevant features, suppressing irrelevant background noise and enhancing nodule identification. The model is trained end-to-end using a dataset of clinically acquired CT scans with expert-annotated ground truth. Our experimental results demonstrate that MSCANet achieves state-of-the-art performance, outperforming existing methods in terms of Dice coefficient (0.82 ± 0.03) and Intersection over Union (0.71 ± 0.04). Moreover, the proposed model exhibits enhanced robustness to variations in nodule size, shape, and density, indicating its potential for reliable integration into clinical workflows. These findings suggest that MSCANet offers a significant advancement in automated pulmonary nodule detection and segmentation, contributing to improved diagnostic accuracy and efficiency.",ai
"We address the challenge of efficiently solving complex symbolic reasoning problems involving large knowledge bases and intricate logical constraints. Traditional automated reasoning systems often struggle with scalability and performance in such scenarios. We propose a novel approach, termed Constraint-Guided Abstraction and Refinement (CGAR), which combines hierarchical abstraction techniques with constraint programming. CGAR initially generates an abstract representation of the problem, focusing on critical high-level constraints, and uses a constraint solver to identify potential solutions within this abstracted space. These candidate solutions are then refined iteratively by incorporating more detailed aspects of the knowledge base and logical constraints, guided by the unsatisfied constraints identified in the previous refinement step. This process continues until a valid solution is found, or a predefined refinement threshold is reached. We evaluated CGAR on a benchmark suite of knowledge-intensive reasoning tasks, including problems from ontology reasoning and program verification. Our experimental results demonstrate that CGAR significantly outperforms state-of-the-art resolution-based and tableau-based reasoners in terms of both solution time and memory usage, achieving speedups of up to an order of magnitude in certain cases. Furthermore, CGAR exhibits superior scalability, enabling it to solve problems that are intractable for existing methods. The observed performance gains suggest that CGAR offers a promising avenue for tackling complex symbolic reasoning problems.",ai
"We investigate the problem of automated pulmonary nodule detection in computed tomography (CT) scans, a crucial task for early diagnosis of lung cancer. Manual review of such scans is time-consuming and prone to inter-observer variability, motivating the development of robust and accurate automated solutions. We propose a novel multi-scale convolutional neural network (CNN) architecture, termed MS-NoduleNet, that incorporates contextual information at varying resolutions to improve nodule identification, particularly for small and subtle nodules. MS-NoduleNet utilizes a three-dimensional (3D) CNN backbone with multiple feature extraction branches operating on different receptive fields. Feature maps from these branches are then fused via a learnable attention mechanism, enabling the network to adaptively weigh the importance of different scales. The model was trained and evaluated on the LUNA16 benchmark dataset, achieving a state-of-the-art Continuous Average Recall (CAR) score of 0.893 at 8 false positives per scan. Ablation studies demonstrate the effectiveness of the multi-scale feature extraction and attention mechanism. Specifically, the proposed architecture demonstrates a statistically significant improvement in detecting nodules smaller than 6mm compared to single-scale CNN approaches, suggesting its potential for improving early detection rates in clinical practice.",ai
"Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.",human
"Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present , the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.",human
"Feature and instance co-selection, which aims to reduce both feature dimensionality and sample size by identifying the most informative features and instances, has attracted considerable attention in recent years. However, when dealing with unlabeled incomplete multi-view data, where some samples are missing in certain views, existing methods typically first impute the missing data and then concatenate all views into a single dataset for subsequent co-selection. Such a strategy treats co-selection and missing data imputation as two independent processes, overlooking potential interactions between them. The inter-sample relationships gleaned from co-selection can aid imputation, which in turn enhances co-selection performance. Additionally, simply merging multi-view data fails to capture the complementary information among views, ultimately limiting co-selection effectiveness. To address these issues, we propose a novel co-selection method, termed Joint learning of Unsupervised multI-view feature and instance Co-selection with cross-viEw imputation (JUICE). JUICE first reconstructs incomplete multi-view data using available observations, bringing missing data recovery and feature and instance co-selection together in a unified framework. Then, JUICE leverages cross-view neighborhood information to learn inter-sample relationships and further refine the imputation of missing values during reconstruction. This enables the selection of more representative features and instances. Extensive experiments demonstrate that JUICE outperforms state-of-the-art methods.",human
"Graph Neural Networks (GNNs) have emerged as a powerful paradigm for representation learning on graph-structured data. However, many existing GNN architectures struggle to effectively capture long-range dependencies and global contextual information within large, complex graphs, often resulting in suboptimal performance in tasks requiring reasoning across distant nodes. This work addresses this limitation by proposing a novel GNN architecture, Graph Transformer Networks with Adaptive Propagation (GTNAP), which integrates transformer-based attention mechanisms with adaptive message passing strategies. GTNAP leverages multi-head attention to capture long-range dependencies between nodes, dynamically adjusting the propagation strength between node pairs based on their relevance. Furthermore, we introduce a learnable aggregation function that adaptively weights messages from different neighbor nodes, enabling the network to prioritize informative features and mitigate the impact of noisy or irrelevant information. We evaluate GTNAP on several benchmark graph classification and node classification datasets, including ogbn-products and ogbn-arxiv. Empirical results demonstrate that GTNAP significantly outperforms state-of-the-art GNN models, achieving improvements of up to 5% in node classification accuracy and 3% in graph classification accuracy. These findings suggest that GTNAP provides a more effective framework for learning representations on complex graphs by better capturing long-range dependencies and adapting to the specific characteristics of the graph structure.",ai
"We address the problem of causal structure learning from observational data in settings with unobserved confounders and feedback loops, a scenario where traditional constraint-based and score-based methods often fail to recover the true causal graph. Existing approaches either make strong assumptions about the functional relationships between variables or require interventional data. We propose a novel method, Cyclic Causal Discovery with Latent Variable Identification (CCDLVI), which leverages the principle of Instrumental Variable (IV) identification and subsequent constraint-based testing to uncover the underlying causal graph. CCDLVI first identifies potential IVs using conditional independence tests, followed by a tailored conditional independence test based on the identified IVs to construct the causal skeleton. A custom orientation rule, incorporating the cyclic nature of the graph, is then applied to orient edges and resolve ambiguities. We evaluate CCDLVI on synthetic datasets generated from both linear and non-linear structural equation models with varying levels of confounding and feedback. Our results demonstrate that CCDLVI significantly outperforms existing methods in terms of Structural Hamming Distance (SHD) and False Discovery Rate (FDR), particularly in high-dimensional settings and when the number of unobserved confounders is substantial. Furthermore, we apply CCDLVI to a real-world gene regulatory network dataset, showing its potential in uncovering plausible causal relationships between genes in the presence of complex feedback mechanisms.",ai
"We investigate the challenge of coordinating decentralized agents in complex, partially observable environments where communication is unreliable and bandwidth-constrained. Specifically, we address the problem of learning robust communication protocols that enable efficient task completion despite intermittent signal loss. We propose a novel communication learning framework, Recurrent Attention Communication with Latent Dropout (RAC-LD), which integrates a recurrent neural network with an attention mechanism and a latent dropout layer. The attention mechanism allows agents to selectively focus on relevant messages, while the latent dropout layer introduces stochasticity during training to simulate unreliable communication channels. This encourages agents to develop more robust and adaptable communication strategies. We evaluate RAC-LD on a challenging cooperative navigation task involving multiple agents navigating a gridworld environment under varying levels of communication noise and bandwidth limitations. Experimental results demonstrate that RAC-LD significantly outperforms existing communication learning methods, exhibiting improved coordination and task completion rates, particularly in scenarios with high communication unreliability. Further analysis reveals that RAC-LD learns more compact and informative communication signals compared to baseline approaches, highlighting its ability to adapt to bandwidth constraints. The proposed method shows promise for enabling effective multi-agent collaboration in real-world scenarios characterized by noisy and limited communication channels.",ai
"This work investigates the efficacy of transformer-based architectures in addressing the inherent challenges of long-range dependency modeling in natural language processing. Specifically, we focus on the problem of capturing contextual information across extended textual sequences, a limitation often observed in recurrent neural networks. Our methodology leverages a modified transformer architecture incorporating a novel positional encoding scheme that emphasizes relative positional information between tokens, mitigating the quadratic computational complexity associated with standard self-attention mechanisms when processing lengthy inputs. Furthermore, we introduce a sparse attention variant, restricting attention to a dynamically selected subset of input tokens based on a learned relevance metric. We evaluate our approach on three benchmark datasets: the Long Range Arena (LRA) suite, the Gutenberg Project, and a proprietary dataset of scientific literature abstracts. Empirical results demonstrate that our modified transformer architecture achieves substantial improvements in performance, exhibiting a 15% average increase in accuracy on the LRA benchmark compared to standard transformer models. Moreover, we observe a significant reduction in computational cost during training and inference, enabling the effective processing of sequences exceeding 10,000 tokens. These findings suggest the potential of our approach for enhancing the capabilities of transformer models in handling long-range dependencies within diverse NLP tasks.",ai
"Graph Neural Networks (GNNs) have demonstrated promising performance in various node and graph-level prediction tasks. However, existing GNN architectures often struggle to effectively capture long-range dependencies and complex relational reasoning within large, sparse graphs, leading to sub-optimal performance in domains requiring intricate knowledge propagation. This work addresses this limitation by introducing a novel GNN architecture, the Attentive Relational Propagation Network (ARPN), which incorporates a multi-head attention mechanism over relational paths to facilitate efficient information aggregation across distant nodes. ARPN dynamically learns the importance of different relational paths between nodes, allowing for selective and context-aware message passing. Furthermore, we introduce a path encoding strategy that captures the semantic information encoded within the relationships traversed by each path, enhancing the expressiveness of the model. We evaluate ARPN on several benchmark datasets, including the Open Graph Benchmark (OGB) and a knowledge graph completion task. Experimental results demonstrate that ARPN significantly outperforms existing GNN architectures, achieving state-of-the-art performance in link prediction and node classification. Ablation studies confirm the effectiveness of the attentive relational path aggregation and the path encoding mechanism, validating their contributions to the overall performance improvement.",ai
"Numerical simulation of time-dependent partial differential equations (PDEs) is central to scientific and engineering applications, but high-fidelity solvers are often prohibitively expensive for long-horizon or time-critical settings. Neural operator (NO) surrogates offer fast inference across parametric and functional inputs; however, most autoregressive NO frameworks remain vulnerable to compounding errors, and ensemble-averaged metrics provide limited guarantees for individual inference trajectories. In practice, error accumulation can become unacceptable beyond the training horizon, and existing methods lack mechanisms for online monitoring or correction. To address this gap, we propose ANCHOR (Adaptive Numerical Correction for High-fidelity Operator Rollouts), an online, instance-aware hybrid inference framework for stable long-horizon prediction of nonlinear, time-dependent PDEs. ANCHOR treats a pretrained NO as the primary inference engine and adaptively couples it with a classical numerical solver using a physics-informed, residual-based error estimator. Inspired by adaptive time-stepping in numerical analysis, ANCHOR monitors an exponential moving average (EMA) of the normalized PDE residual to detect accumulating error and trigger corrective solver interventions without requiring access to ground-truth solutions. We show that the EMA-based estimator correlates strongly with the true relative L2 error, enabling data-free, instance-aware error control during inference. Evaluations on four canonical PDEs: 1D and 2D Burgers', 2D Allen-Cahn, and 3D heat conduction, demonstrate that ANCHOR reliably bounds long-horizon error growth, stabilizes extrapolative rollouts, and significantly improves robustness over standalone neural operators, while remaining substantially more efficient than high-fidelity numerical solvers.",human
"Graph Neural Networks (GNNs) have emerged as a powerful paradigm for representation learning on graph-structured data. However, many existing GNN architectures struggle with capturing long-range dependencies and distinguishing between structurally similar nodes, leading to sub-optimal performance in complex graph analysis tasks. This paper addresses these limitations by introducing a novel GNN framework, the Adaptive Path Aggregation Network (APAN). APAN leverages a learnable path selection mechanism, dynamically identifying and aggregating information from relevant paths between nodes, thus mitigating the vanishing gradient problem associated with deep GNNs. Furthermore, APAN incorporates a structural context encoder that explicitly captures the local graph topology surrounding each node, enabling improved differentiation between structurally similar nodes. We evaluate APAN on a diverse set of graph benchmark datasets, including node classification, graph classification, and link prediction tasks. Experimental results demonstrate that APAN consistently outperforms state-of-the-art GNN architectures, achieving significant improvements in accuracy and generalization performance. Specifically, APAN exhibits a 5-10% increase in accuracy on node classification tasks involving complex graph structures. These findings highlight the effectiveness of adaptive path aggregation and structural context encoding in enhancing GNN performance for a broad range of graph learning applications.",ai
"With 3D data rapidly emerging as an important form of multimedia information, 3D human mesh recovery technology has also advanced accordingly. However, current methods mainly focus on handling humans wearing tight clothing and perform poorly when estimating body shapes and poses under diverse clothing, especially loose garments. To this end, we make two key insights: (1) tailoring clothing to fit the human body can mitigate the adverse impact of clothing on 3D human mesh recovery, and (2) utilizing human visual information from large foundational models can enhance the generalization ability of the estimation. Based on these insights, we propose ClothHMR, to accurately recover 3D meshes of humans in diverse clothing. ClothHMR primarily consists of two modules: clothing tailoring (CT) and FHVM-based mesh recovering (MR). The CT module employs body semantic estimation and body edge prediction to tailor the clothing, ensuring it fits the body silhouette. The MR module optimizes the initial parameters of the 3D human mesh by continuously aligning the intermediate representations of the 3D mesh with those inferred from the foundational human visual model (FHVM). ClothHMR can accurately recover 3D meshes of humans wearing diverse clothing, precisely estimating their body shapes and poses. Experimental results demonstrate that ClothHMR significantly outperforms existing state-of-the-art methods across benchmark datasets and in-the-wild images. Additionally, a web application for online fashion and shopping powered by ClothHMR is developed, illustrating that ClothHMR can effectively serve real-world usage scenarios. The code and model for ClothHMR are available at: .",human
"Neural network constraint satisfaction is crucial for safety-critical applications such as power system optimization, robotic path planning, and autonomous driving. However, existing constraint satisfaction methods face efficiency-applicability trade-offs, with hard constraint methods suffering from either high computational complexity or restrictive assumptions on constraint structures. The Sampling Kaczmarz-Motzkin (SKM) method is a randomized iterative algorithm for solving large-scale linear inequality systems with favorable convergence properties, but its argmax operations introduce non-differentiability, posing challenges for neural network applications. This work proposes the Trainable Sampling Kaczmarz-Motzkin Network (T-SKM-Net) framework and, for the first time, systematically integrates SKM-type methods into neural network constraint satisfaction. The framework transforms mixed constraint problems into pure inequality problems through null space transformation, employs SKM for iterative solving, and maps solutions back to the original constraint space, efficiently handling both equality and inequality constraints. We provide theoretical proof of post-processing effectiveness in expectation and end-to-end trainability guarantees based on unbiased gradient estimators, demonstrating that despite non-differentiable operations, the framework supports standard backpropagation. On the DCOPF case118 benchmark, our method achieves 4.27ms/item GPU serial forward inference with 0.0025% max optimality gap with post-processing mode and 5.25ms/item with 0.0008% max optimality gap with joint training mode, delivering over 25 speedup compared to the pandapower solver while maintaining zero constraint violations under given tolerance.",human
"Bayesian Neural Networks (BNNs) provide principled uncertainty quantification but suffer from substantial computational and memory overhead compared to deterministic networks. While quantization techniques have successfully reduced resource requirements in standard deep learning models, their application to probabilistic models remains largely unexplored. We introduce a systematic multi-level quantization framework for Stochastic Variational Inference based BNNs that distinguishes between three quantization strategies: Variational Parameter Quantization (VPQ), Sampled Parameter Quantization (SPQ), and Joint Quantization (JQ). Our logarithmic quantization for variance parameters, and specialized activation functions to preserve the distributional structure are essential for calibrated uncertainty estimation. Through comprehensive experiments on Dirty-MNIST, we demonstrate that BNNs can be quantized down to 4-bit precision while maintaining both classification accuracy and uncertainty disentanglement. At 4 bits, Joint Quantization achieves up to 8x memory reduction compared to floating-point implementations with minimal degradation in epistemic and aleatoric uncertainty estimation. These results enable deployment of BNNs on resource-constrained edge devices and provide design guidelines for future analog ""Bayesian Machines"" operating at inherently low precision.",human
"**Automated Detection of Pulmonary Embolism in Computed Tomography Angiography using Convolutional Neural Networks** Pulmonary embolism (PE) remains a significant cause of morbidity and mortality, demanding timely diagnosis via Computed Tomography Angiography (CTA). Manual interpretation of CTA scans is labor-intensive and susceptible to inter-observer variability, motivating the development of automated diagnostic tools. We propose a novel multi-stage convolutional neural network (CNN) architecture for accurate and efficient PE detection in CTA images. The system comprises a 3D CNN for volumetric segmentation of the pulmonary arteries, followed by a cascade of 2D CNNs operating on axial slices for the identification of emboli within the segmented regions. A key innovation is the incorporation of attention mechanisms within the 2D CNNs to focus on regions of interest, thereby mitigating false positive detections arising from anatomical variations. The proposed method was evaluated on a retrospectively collected dataset of 200 CTA scans, comprising 100 positive and 100 negative PE cases, verified by expert radiologists. Our approach achieved a sensitivity of 92% at a specificity of 88% on the held-out test set, demonstrating a significant improvement over existing baseline methods. The area under the ROC curve (AUC) was 0.95, indicating excellent diagnostic performance. These results suggest that the proposed system offers a promising avenue for augmenting radiologists' workflows and improving the efficiency and accuracy of PE diagnosis.",ai
"We address the challenge of effectively representing complex relational knowledge for enhanced downstream reasoning in natural language processing. Existing knowledge representation paradigms often struggle to capture nuanced semantic dependencies and intricate inter-entity relationships inherent in real-world scenarios. This limitation hinders the performance of tasks requiring sophisticated inference, such as question answering and commonsense reasoning. We introduce a novel framework, the Graph-Augmented Semantic Embedding Network (GASEN), which combines pre-trained language models with dynamically constructed knowledge graphs. GASEN leverages entity linking and relation extraction to construct a contextualized knowledge graph specific to the input text. This graph is then integrated into the language model via a graph attention mechanism, allowing the model to explicitly attend to relevant relationships and dependencies during semantic encoding. We evaluate GASEN on multiple benchmark datasets for question answering (CommonsenseQA) and knowledge graph completion (WN18RR). Our results demonstrate that GASEN significantly outperforms state-of-the-art baselines, achieving improvements of 5.2% on CommonsenseQA and 3.8% on WN18RR in terms of accuracy and mean reciprocal rank, respectively. These findings underscore the efficacy of our graph-augmented approach in improving the representation of complex relational knowledge and its positive impact on downstream reasoning tasks.",ai
"Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.",human
"### Abstract Accurate and efficient segmentation of anatomical structures in medical images is crucial for computer-aided diagnosis, treatment planning, and disease monitoring. Manual segmentation is time-consuming and prone to inter-observer variability. This work addresses the challenge of automated segmentation of the left ventricle (LV) in cardiac magnetic resonance (CMR) images, which is essential for assessing cardiac function. We propose a novel deep learning architecture based on a hybrid convolutional neural network (CNN) and transformer framework. The CNN component extracts local features while the transformer module captures long-range dependencies within the image, thereby improving contextual understanding. Specifically, we employ a U-Net architecture with ResNet blocks as the encoder and decoder, and integrate a transformer layer at the bottleneck to model global relationships between features. We evaluate our method on the publicly available Automated Cardiac Diagnosis Challenge (ACDC) dataset. Our model achieves a Dice score of 0.93 ± 0.02, a Hausdorff distance of 6.2 ± 1.5 mm, and an average surface distance of 0.8 ± 0.1 mm for LV segmentation. Comparative analysis demonstrates that our hybrid CNN-transformer approach outperforms state-of-the-art CNN-based methods and provides a significant improvement in segmentation accuracy, particularly in cases with complex anatomical variations. These results suggest that the proposed method offers a robust and reliable solution for automated LV segmentation in CMR images.",ai
"Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.",human
"Immersive formats such as 360° and 6DoF point cloud videos require high bandwidth and low latency, posing challenges for real-time AR/VR streaming. This work focuses on reducing bandwidth consumption and encryption/decryption delay, two key contributors to overall latency. We design a system that downsamples point cloud content at the origin server and applies partial encryption. At the client, the content is decrypted and upscaled using an ML-based super-resolution model. Our evaluation demonstrates a nearly linear reduction in bandwidth/latency, and encryption/decryption overhead with lower downsampling resolutions, while the super-resolution model effectively reconstructs the original full-resolution point clouds with minimal error and modest inference time.",human
"Neural network controllers increasingly demand millions of parameters, and language model approaches push into the billions. For embedded aerospace systems with strict power and latency constraints, this scaling is prohibitive. We present Tiny Recursive Control (TRC), a neural architecture based on a counterintuitive principle: capacity can emerge from iteration depth rather than parameter count. TRC applies compact networks (approximately 1.5M parameters) repeatedly through a two-level hierarchical latent structure, refining control sequences by simulating trajectories and correcting based on tracking error. Because the same weights process every refinement step, adding iterations increases computation without increasing memory. We evaluate TRC on nonlinear control problems including oscillator stabilization and powered descent with fuel constraints. Across these domains, TRC achieves near-optimal control costs while requiring only millisecond-scale inference on GPU and under 10~MB memory, two orders of magnitude smaller than language model baselines. These results demonstrate that recursive reasoning, previously confined to discrete tasks, transfers effectively to continuous control synthesis.",human
"Mixture-of-experts (MoE) architectures used in large language models (LLMs) achieve state-of-the-art performance across diverse tasks yet face practical challenges such as deployment complexity and low activation efficiency. Expert pruning has thus emerged as a promising solution to reduce computational overhead and simplify the deployment of MoE models. However, existing expert pruning approaches conventionally rely on local importance metrics and often apply uniform layer-wise pruning, leveraging only partial evaluation signals and overlooking the heterogeneous contributions of experts across layers. To address these limitations, we propose an expert pruning approach based on the trajectory of activated experts across layers, which treats MoE as a weighted computation graph and casts expert selection as a global optimal path planning problem. Within this framework, we integrate complementary importance signals from reconstruction error, routing probabilities, and activation strength at the trajectory level, which naturally yields non-uniform expert retention across layers. Experiments show that our approach achieves superior pruning performance on nearly all tasks compared with most existing approaches.",human
"Differential privacy (DP) offers a rigorous framework for mitigating privacy risks in machine learning, but often comes at the cost of reduced model utility. This paper addresses the problem of preserving data privacy while maintaining high model accuracy in scenarios with limited computational resources. We propose a novel approach, Federated Distillation with Optimized Noise (FedDistill-ON), that combines federated learning, knowledge distillation, and a dynamically adjusted noise injection mechanism. FedDistill-ON leverages federated averaging to train a global teacher model, which then distills its knowledge to smaller, client-side student models. Critically, the noise applied during the distillation process is optimized based on client-specific data sensitivity, allowing for tighter privacy bounds and reduced utility loss. We evaluate FedDistill-ON on benchmark image classification and natural language processing datasets. Experimental results demonstrate that FedDistill-ON achieves significantly higher accuracy compared to standard differentially private federated learning algorithms with similar privacy guarantees (measured by ε and δ). Specifically, our method yields an average improvement of 5-8% in accuracy for image classification and 3-5% for text classification across various privacy budgets. The findings suggest that FedDistill-ON provides a practical and effective solution for privacy-preserving machine learning in resource-constrained environments.",ai
"We address the problem of automated detection and segmentation of pulmonary nodules in computed tomography (CT) scans, a critical task for early diagnosis of lung cancer. Manual analysis of these scans is time-consuming and subject to inter-observer variability. We propose a novel deep learning architecture, a cascaded multi-scale convolutional neural network (CMS-CNN), designed to leverage contextual information at varying resolutions for improved nodule identification and precise delineation. The CMS-CNN incorporates three distinct stages: (1) a coarse-grained 3D CNN for initial candidate nodule detection, (2) a refined 3D CNN that operates on multi-scale feature maps derived from the initial stage to reduce false positives, and (3) a fully convolutional network (FCN) for precise segmentation of the identified nodules. The network incorporates residual connections to facilitate gradient flow and batch normalization to accelerate training. The proposed method was evaluated on the publicly available LIDC-IDRI dataset. Our results demonstrate a significant improvement in both detection and segmentation performance compared to state-of-the-art methods. Specifically, we achieve a sensitivity of 92.5% at 4 false positives per scan for nodule detection and a Dice similarity coefficient of 81.2% for nodule segmentation. These findings suggest the potential of the CMS-CNN architecture as a valuable tool for radiologists, enabling more efficient and accurate lung cancer screening.",ai
"In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.",human
"Industrial symbiosis fosters circularity by enabling firms to repurpose residual resources, yet its emergence is constrained by socio-spatial frictions that shape costs, matching opportunities, and market efficiency. Existing models often overlook the interaction between spatial structure, market design, and adaptive firm behavior, limiting our understanding of where and how symbiosis arises. We develop an agent-based model where heterogeneous firms trade byproducts through a spatially embedded double-auction market, with prices and quantities emerging endogenously from local interactions. Leveraging reinforcement learning, firms adapt their bidding strategies to maximize profit while accounting for transport costs, disposal penalties, and resource scarcity. Simulation experiments reveal the economic and spatial conditions under which decentralized exchanges converge toward stable and efficient outcomes. Counterfactual regret analysis shows that sellers' strategies approach a near Nash equilibrium, while sensitivity analysis highlights how spatial structures and market parameters jointly govern circularity. Our model provides a basis for exploring policy interventions that seek to align firm incentives with sustainability goals, and more broadly demonstrates how decentralized coordination can emerge from adaptive agents in spatially constrained markets.",human
"This work investigates the efficacy of transformer-based architectures in addressing the challenge of long-range dependency modeling in natural language processing. Specifically, we examine the limitations of standard attention mechanisms in capturing relationships across extended sequences, which are crucial for tasks such as document summarization and machine translation of lengthy texts. To mitigate these limitations, we propose a novel variant of the transformer architecture incorporating a sparse attention mechanism coupled with a learnable compression layer. The sparse attention mechanism reduces computational complexity by attending only to a subset of input tokens, while the compression layer learns a reduced-dimensional representation of the input sequence, facilitating the capture of global context. We evaluate our proposed model on several benchmark datasets, including the CNN/DailyMail summarization dataset and the WMT'14 English-German translation dataset. Experimental results demonstrate that our architecture significantly outperforms standard transformer models, achieving an average improvement of 2.3 ROUGE-L points on the CNN/DailyMail dataset and a 1.8 BLEU score increase on the WMT'14 dataset. These findings suggest that strategically designed attention mechanisms and learned representations can substantially enhance the performance of transformer-based models in NLP tasks requiring the processing of long sequences.",ai
"We investigate the application of Transformer architectures to the task of natural language processing, specifically focusing on sequence-to-sequence problems exhibiting long-range dependencies. Recurrent neural networks, traditionally employed in such contexts, often struggle with vanishing gradients and the inherent sequential processing paradigm, limiting their capacity to capture distant relationships. We propose an alternative approach leveraging the self-attention mechanism inherent within the Transformer framework to directly model dependencies between all input tokens, irrespective of their relative positions. This parallelization allows for significant speedups during training and enables the network to effectively capture long-range contextual information. We evaluate our proposed model on two benchmark datasets: machine translation (WMT 2014 English-German) and abstractive summarization (CNN/DailyMail). Results demonstrate that the Transformer architecture achieves state-of-the-art performance on these tasks, exceeding the accuracy of comparable recurrent models while exhibiting a substantial reduction in training time. Furthermore, we conduct an ablation study to analyze the impact of different components within the Transformer, highlighting the importance of multi-head attention and positional encodings for optimal performance. These findings suggest that the Transformer represents a promising alternative to recurrent architectures for a broad range of sequence-to-sequence NLP tasks.",ai
"Neural scaling laws and double-descent phenomena suggest that deep-network training obeys a simple macroscopic structure despite highly nonlinear optimization dynamics. We derive such structure directly from gradient descent in function space. For mean-squared error loss, the training error evolves as with , a time-dependent self-adjoint operator induced by the network Jacobian. Using Kato perturbation theory, we obtain an exact system of coupled modewise ODEs in the instantaneous eigenbasis of . To extract macroscopic behavior, we introduce a logarithmic spectral-shell coarse-graining and track quadratic error energy across shells. Microscopic interactions within each shell cancel identically at the energy level, so shell energies evolve only through dissipation and external inter-shell interactions. We formalize this via a assumption, under which cumulative microscopic effects reduce to a controlled net flux across shell boundaries. Assuming an effective power-law spectral transport in a relevant resolution range, the shell dynamics admits a self-similar solution with a moving resolution frontier and explicit scaling exponents. This framework explains neural scaling laws and double descent, and unifies lazy (NTK-like) training and feature learning as two limits of the same spectral-shell dynamics.",human
"We address the problem of effectively representing and reasoning with complex, hierarchical knowledge structures, specifically focusing on domains characterized by intricate relationships between entities and attributes. Existing knowledge representation formalisms often struggle to simultaneously maintain expressive power, computational tractability, and adaptability to evolving knowledge. We introduce a novel approach leveraging a tensor factorization-based method integrated with a probabilistic logic framework. This allows for representing entities and their attributes as tensors, capturing multi-dimensional relationships, while probabilistic logic provides a mechanism for reasoning under uncertainty and handling inconsistencies within the knowledge base. Tensor factorization facilitates automated knowledge acquisition and completion by identifying latent relationships and predicting missing attributes. We evaluate our approach on benchmark datasets representing both semantic knowledge graphs and biological networks, demonstrating significant improvements in link prediction accuracy compared to existing methods such as RDF2Vec and ComplEx. Furthermore, we show that the probabilistic logic component enables robust reasoning in the presence of noisy or incomplete data, leading to more accurate inference results. The proposed framework offers a promising avenue for representing and reasoning with complex knowledge in a scalable and adaptable manner.",ai
"## Transformer Architectures for Enhanced Sequence Modeling in Natural Language Processing Sequence modeling is a fundamental task in natural language processing (NLP), with applications ranging from machine translation to text summarization. Recurrent neural networks (RNNs), traditionally employed for these tasks, suffer from inherent limitations in parallelization and difficulty in capturing long-range dependencies. This work investigates the efficacy of Transformer architectures, specifically focusing on self-attention mechanisms, as an alternative approach for sequence modeling in NLP. We propose a novel adaptation of the standard Transformer architecture incorporating a learned positional encoding scheme and optimized layer normalization placement to improve gradient flow during training. This modified architecture is evaluated on three benchmark NLP tasks: machine translation (WMT 2014 English-German), text summarization (CNN/DailyMail), and sentiment analysis (Stanford Sentiment Treebank). Empirical results demonstrate that the proposed architecture achieves superior performance compared to baseline Transformer models and competitive performance against state-of-the-art RNN-based approaches, particularly in tasks requiring the modeling of long-range contextual information. Analysis of the attention weights reveals the architecture's capability to capture intricate relationships between words within a sequence, facilitating more accurate and nuanced representations of natural language. These findings suggest that Transformer architectures offer a robust and scalable solution for a wide array of sequence modeling challenges in NLP.",ai
"An emerging computing paradigm, so-called next-generation reservoir computing (NG-RC) is investigated. True to its namesake, NG-RC requires no actual reservoirs for input data mixing but rather computing the polynomial terms directly from the time series inputs. However, benchmark tests so far reported have been one-sided, limited to prediction tasks of temporal waveforms such as Lorenz 63 attractor and Mackey-Glass chaotic signal. We will demonstrate for the first time that NG-RC can perform classification task as good as conventional RC. This validates the versatile computational capability of NG-RC in tasks of both prediction and classification.",human
"We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.",human
"In the face of increasing financial uncertainty and market complexity, this study presents a novel risk-aware financial forecasting framework that integrates advanced machine learning techniques with intuitionistic fuzzy multi-criteria decision-making (MCDM). Tailored to the BIST 100 index and validated through a case study of a major defense company in Türkiye, the framework fuses structured financial data, unstructured text data, and macroeconomic indicators to enhance predictive accuracy and robustness. It incorporates a hybrid suite of models, including extreme gradient boosting (XGBoost), long short-term memory (LSTM) network, graph neural network (GNN), to deliver probabilistic forecasts with quantified uncertainty. The empirical results demonstrate high forecasting accuracy, with a net profit mean absolute percentage error (MAPE) of 3.03% and narrow 95% confidence intervals for key financial indicators. The risk-aware analysis indicates a favorable risk-return profile, with a Sharpe ratio of 1.25 and a higher Sortino ratio of 1.80, suggesting relatively low downside volatility and robust performance under market fluctuations. Sensitivity analysis shows that the key financial indicator predictions are highly sensitive to variations of inflation, interest rates, sentiment, and exchange rates. Additionally, using an intuitionistic fuzzy MCDM approach, combining entropy weighting, evaluation based on distance from the average solution (EDAS), and the measurement of alternatives and ranking according to compromise solution (MARCOS) methods, the tabular data learning network (TabNet) outperforms the other models and is identified as the most suitable candidate for deployment. Overall, the findings of this work highlight the importance of integrating advanced machine learning, risk quantification, and fuzzy MCDM methodologies in financial forecasting, particularly in emerging markets.",human
"We investigate the problem of enhancing automated reasoning capabilities in complex logical domains, specifically those characterized by high branching factors and long proof trajectories. Traditional theorem provers often struggle with such domains due to the exponential growth in the search space. We propose a novel method, termed Guided Deductive Inference (GDI), that leverages a learned heuristic function to prioritize promising inference steps. GDI employs a deep neural network, trained on a dataset of successful proofs, to estimate the probability of a given inference rule application leading to a valid proof state. This learned probability is then integrated into a modified best-first search algorithm, biasing the search towards more likely successful paths. We evaluate GDI on a benchmark suite of challenging mathematical problems derived from formalizations of set theory and basic number theory. Experimental results demonstrate that GDI significantly outperforms several baseline theorem provers, including traditional depth-first and breadth-first search strategies, in terms of both the number of solved problems and the average proof length. Furthermore, we observe that GDI is able to discover shorter and more elegant proofs compared to those generated by unguided search, suggesting that the learned heuristic effectively captures underlying structural patterns in the logical domain. This work provides empirical evidence that machine learning techniques can substantially improve the efficiency and effectiveness of automated reasoning systems.",ai
"Self-referential learning -- training a model on data it generated itself -- promises boundless scalability but chronically suffers from model collapse: language models degenerate into repetitive text, GANs drop modes, and reinforcement-learning policies over-exploit. Although practitioners employ ad~hoc fixes such as real-data mixing, entropy bonuses, knowledge distillation, or retrieval-augmented generation, a single principle that explains both the failure mode and the success of these fixes has remained elusive. We present Entropy-Reservoir Bregman Projection (ERBP), an information-geometric framework that unifies these phenomena. We model the closed loop as a stochastic Bregman projection sequence in distribution space. Without external coupling, finite-sample noise forces the system to project onto an ever-shrinking empirical support, causing exponential entropy decay and eventual collapse. Introducing an Entropy Reservoir -- a high-entropy distribution mixed into each projection -- injects a controllable entropy flux that provably stabilises the dynamics. Our theory yields (i) a necessary condition for collapse, (ii) a sufficient condition that guarantees a non-trivial entropy floor, and (iii) closed-form rates that depend only on sample size and the strong-convexity/Lipschitz constants of the Bregman generator. Experiments on large-language-model self-training, Soft Actor-Critic in reinforcement learning, and GAN optimisation validate our predictions and show that disparate stabilisation heuristics correspond to specific reservoir choices and coupling coefficients. ERBP thus transforms a collection of folk remedies into a single, quantitative design rule: monitor and budget your entropy flux.",human
"The bisimulation metric (BSM) is a powerful tool for analyzing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to state similarity between multiple MDPs remains challenging. Prior work has attempted to extend BSM to pairs of MDPs, but a lack of well-established mathematical properties has limited further theoretical analysis between MDPs. In this work, we formally establish a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of MDPs, which is rigorously proven with three fundamental metric properties, i.e., GBSM symmetry, inter-MDP triangle inequality, and a distance bound on identical spaces. Leveraging these properties, we theoretically analyze policy transfer, state aggregation, and sampling-based estimation across MDPs, obtaining explicit bounds that are strictly tighter than existing ones derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.",human
"Graph Neural Networks (GNNs) have emerged as a powerful paradigm for representation learning on graph-structured data. However, many existing GNN architectures struggle with long-range dependencies and over-smoothing, limiting their ability to effectively capture complex graph properties. This work addresses the limitations of traditional message-passing GNNs by introducing a novel Graph Transformer architecture incorporating a learnable structural bias. Our method, termed Structural-Aware Transformer Network (SATNet), augments the standard attention mechanism with a learned embedding reflecting the relative structural positions of nodes within the graph. These structural embeddings are learned through a self-supervised objective that encourages the network to predict the shortest path distances between nodes, thereby injecting graph-specific inductive bias. We evaluate SATNet on several benchmark graph classification and node classification datasets, including ogbn-arxiv and Pascal VOC, demonstrating significant performance improvements compared to state-of-the-art GNNs. Specifically, SATNet achieves a 5.2% improvement in accuracy on the ogbn-arxiv node classification task and a 3.8% improvement in mean average precision on the Pascal VOC graph classification task. These results highlight the effectiveness of incorporating learnable structural bias into Transformer-based GNNs for improved performance on graph-structured data.",ai
"This paper reflects on the literature that rejects the use of Large Language Models (LLMs) in qualitative data analysis. It illustrates through empirical evidence as well as critical reflections why the current critical debate is focusing on the wrong problems. The paper proposes that the focus of researching the use of the LLMs for qualitative analysis is not the method per se, but rather the empirical investigation of an artificial system performing an analysis. The paper builds on the seminal work of Alan Turing and reads the current debate using key ideas from Turing ""Computing Machinery and Intelligence"". This paper therefore reframes the debate on qualitative analysis with LLMs and states that rather than asking whether machines can perform qualitative analysis in principle, we should ask whether with LLMs we can produce analyses that are sufficiently comparable to human analysts. In the final part the contrary views to performing qualitative analysis with LLMs are analysed using the same writing and rhetorical style that Turing used in his seminal work, to discuss the contrary views to the main question.",human
"We investigate the challenge of efficient exploration in sparse-reward, continuous control environments, a persistent bottleneck hindering the application of reinforcement learning to real-world robotic tasks. Existing methods often struggle due to the vastness of the state-action space and the difficulty in discovering rewarding trajectories by chance. To address this, we propose a novel hierarchical reinforcement learning framework, termed Guided Exploration with Learned Dynamics (GELD), which integrates intrinsic motivation derived from a learned dynamics model with a goal-conditioned policy. GELD first learns a dynamics model to predict the next state given a current state and action. Then, an intrinsic reward is assigned based on the prediction error of this model, encouraging exploration of unfamiliar state spaces. A high-level goal-conditioned policy is trained to navigate towards states that maximize this intrinsic reward, while a low-level policy executes actions to achieve the high-level goals. We evaluate GELD across a suite of challenging MuJoCo continuous control tasks with sparse rewards. Results demonstrate that GELD significantly outperforms state-of-the-art exploration techniques, achieving up to a 4x improvement in sample efficiency and successfully learning policies in environments where baseline methods fail to converge. This suggests that learning and leveraging dynamics models can substantially improve exploration in sparse reward reinforcement learning settings.",ai
"Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.",human
"We introduce a high-throughput neural network accelerator that embeds most network layers directly in hardware, minimizing data transfer and memory usage while preserving a degree of flexibility via a small neural processing unit for the final classification layer. By leveraging power-of-two (Po2) quantization for weights, we replace multiplications with simple rewiring, effectively reducing each convolution to a series of additions. This streamlined approach offers high-throughput, energy-efficient processing, making it highly suitable for applications where model parameters remain stable, such as continuous sensing tasks at the edge or large-scale data center deployments. Furthermore, by including a strategically chosen reprogrammable final layer, our design achieves high throughput without sacrificing fine-tuning capabilities. We implement this accelerator in a 7nm ASIC flow using MobileNetV2 as a baseline and report throughput, area, accuracy, and sensitivity to quantization and pruning - demonstrating both the advantages and potential trade-offs of the proposed architecture. We find that for MobileNetV2, we can improve inference throughput by 20x over fully programmable GPUs, processing 1.21 million images per second through a full forward pass while retaining fine-tuning flexibility. If absolutely no post-deployment fine tuning is required, this advantage increases to 67x at 4 million images per second.",human
"Training on disjoint datasets can serve two primary goals: accelerating data processing and enabling federated learning. It has already been established that Kolmogorov-Arnold networks (KANs) are particularly well suited for federated learning and can be merged through simple parameter averaging. While the federated learning literature has mostly focused on achieving training convergence across distributed nodes, the present paper specifically targets acceleration of the training, which depends critically on the choice of an optimisation method and the type of the basis functions. To the best knowledge of the authors, the fastest currently-available combination is the Newton-Kaczmarz method and the piecewise-linear basis functions. Here, it is shown that training on disjoint datasets (or disjoint subsets of the training dataset) can further improve the performance. Experimental comparisons are provided, and all corresponding codes are publicly available.",human
"We address the problem of enhancing automated theorem proving in first-order logic by leveraging large language models (LLMs) to guide proof search. Traditional automated theorem provers often struggle with long and complex proofs due to the vast search space. Our method integrates LLMs to predict promising proof steps, effectively pruning the search space and focusing computational resources on more likely successful derivations. Specifically, we fine-tune an LLM on a dataset of successful proofs, enabling it to suggest relevant inference rules and terms given a current proof state. These suggestions are then incorporated into a standard resolution theorem prover as prioritized search heuristics. We evaluate our approach on a benchmark of challenging theorems from various mathematical domains. Experimental results demonstrate a significant improvement in proof success rate compared to baseline provers that employ solely syntactic or fixed-priority heuristics. The LLM-guided prover achieves a 25% increase in the number of theorems solved within a given time limit. Furthermore, analysis of the generated proof traces reveals that the LLM's suggestions lead to shorter and more direct proofs, suggesting a more efficient exploration of the search space. This work demonstrates the potential of LLMs to augment classical automated reasoning techniques, opening avenues for tackling more complex and previously intractable theorems.",ai
"We investigate the problem of identifying causal relationships from observational text data in the presence of latent confounders. Existing methods often rely on strong assumptions about the data generating process or require extensive manual feature engineering, limiting their applicability in real-world scenarios. We propose a novel approach, Causal Discovery via Textual Instrumental Variables (CD-TIV), which leverages textual proxies as potential instrumental variables (IVs) to infer causal effects. CD-TIV employs a two-stage least squares framework. In the first stage, we identify candidate IVs by assessing the relevance of textual features to the putative cause using a transformer-based model fine-tuned for relevance prediction. In the second stage, we estimate the causal effect of the cause on the effect using the identified IVs within a structural causal model. To address potential violations of the IV assumptions, we introduce a regularization term that penalizes residual confounding based on observational data correlations. Experiments on both synthetic and real-world text corpora, including news articles and social media posts, demonstrate that CD-TIV outperforms existing baselines in terms of accuracy and robustness in causal effect estimation, particularly when dealing with unobserved confounding. Our results highlight the potential of textual proxies as instrumental variables for causal discovery in text data.",ai
"We address the challenge of sparse reward environments in reinforcement learning (RL), specifically focusing on the task of long-horizon reasoning for sequential decision-making. Standard RL algorithms often struggle in these scenarios due to the difficulty of exploring the state space and attributing credit to impactful actions within lengthy sequences. This work introduces a novel hierarchical reinforcement learning framework, termed ""Temporal Abstraction with Learned Intrinsic Motivation"" (TALIM), which decomposes the original task into a hierarchy of sub-tasks. A high-level manager learns to set abstract goals, while low-level workers are trained to achieve these goals using intrinsic rewards designed to encourage exploration and skill acquisition. The intrinsic reward is learned dynamically based on the manager's experience and a novel disagreement-based exploration bonus. We evaluate TALIM on a suite of challenging benchmark environments characterized by sparse rewards and long horizons. Our results demonstrate that TALIM significantly outperforms state-of-the-art RL algorithms, including hierarchical and exploration-based methods. Specifically, TALIM achieves higher success rates and faster learning speeds, indicating its ability to efficiently explore the environment, discover effective policies, and generalize to unseen scenarios. Furthermore, we provide ablation studies that highlight the importance of the learned intrinsic motivation in facilitating effective temporal abstraction and improved overall performance.",ai
"We address the problem of node classification in attributed graphs where label information is sparse and graph structure is complex, often exhibiting heterophily. Traditional Graph Neural Networks (GNNs) frequently falter in such scenarios due to their reliance on homophily assumptions and over-smoothing. We propose a novel Graph Attention Network with Adaptive Kernel Learning (GAKL) to mitigate these limitations. GAKL leverages an attention mechanism modulated by adaptive kernels to dynamically adjust the receptive field of each node, thereby facilitating the aggregation of information from both similar and dissimilar neighbors. The adaptive kernels, implemented as learnable radial basis functions (RBFs), provide flexibility in capturing intricate relationships between node features and structural context. Furthermore, we introduce a regularization term that encourages sparsity in the attention weights, promoting the selection of relevant neighbors. We evaluate GAKL on a suite of benchmark datasets exhibiting varying degrees of heterophily and sparsity. Our results demonstrate that GAKL consistently outperforms state-of-the-art GNN models, achieving significant improvements in node classification accuracy, particularly in challenging heterophilic settings. Ablation studies validate the efficacy of the adaptive kernel learning and attention sparsity regularization components. The findings suggest that GAKL provides a robust and adaptable framework for node classification in complex attributed graphs.",ai
"Visual Place Recognition (VPR) in long-term deployment requires reasoning beyond pixel similarity: systems must make transparent, interpretable decisions that remain robust under lighting, weather and seasonal change. We present Text2Graph VPR, an explainable semantic localization system that converts image sequences into textual scene descriptions, parses those descriptions into structured scene graphs, and reasons over the resulting graphs to identify places. Scene graphs capture objects, attributes and pairwise relations; we aggregate per-frame graphs into a compact place representation and perform retrieval with a dual-similarity mechanism that fuses learned Graph Attention Network (GAT) embeddings and a Shortest-Path (SP) kernel for structural matching. This hybrid design enables both learned semantic matching and topology-aware comparison, and -- critically -- produces human-readable intermediate representations that support diagnostic analysis and improve transparency in the decision process. We validate the system on Oxford RobotCar and MSLS (Amman/San Francisco) benchmarks and demonstrate robust retrieval under severe appearance shifts, along with zero-shot operation using human textual queries. The results illustrate that semantic, graph-based reasoning is a viable and interpretable alternative for place recognition, particularly suited to safety-sensitive and resource-constrained settings.",human
"We investigate a type of lunar calendar known as lists of the 'nights of the moon', found throughout East Polynesia, including Rapa Nui (Easter Island). Using computational methods, we analyzed the lexical and structural divergence of 49 calendric lists from all major archipelagos, each containing about 30 night names. Our results, presented as a rooted phylogenetic tree, show a clear split into two main groups: one including lists from Rapa Nui, Mangareva, and the Marquesas; the other comprising lists from New Zealand, Hawaii, the Cook Islands, the Austral Islands, Tahiti, and the Tuamotu. This pattern aligns with a recent alternative classification of East Polynesian languages into 'Distal' (Marquesan, Mangarevan, Rapanui) and 'Proximal' (Maori, Hawaiian, Tahitian, etc.) subgroups. Since both language and lunar calendars are symbolic systems passed down and changed within communities - and given the geographic isolation of many archipelagos - we interpret this correspondence as evidence that the early divergence of East Polynesian lunar calendars mirrors early population movements and language splits in the region.",human
"We address the challenge of representing and reasoning with complex, hierarchical knowledge structures that exhibit both compositionality and emergent properties. Current knowledge representation formalisms often struggle to efficiently capture the intricate relationships and dependencies inherent in such systems. This paper introduces a novel approach, Hierarchical Concept Embedding Networks (HCENs), which leverage a combination of distributed representations and explicit hierarchical graph structures. HCENs represent concepts as dense vector embeddings while simultaneously encoding their relationships within a directed acyclic graph. The graph structure facilitates efficient reasoning about inheritance, specialization, and aggregation, while the embeddings enable capturing nuanced semantic similarity between concepts. We propose a training methodology that jointly optimizes the embeddings and the graph structure using a combination of knowledge-based constraints and distributional information derived from large text corpora. Experimental results on several benchmark knowledge graph completion and concept categorization tasks demonstrate that HCENs outperform state-of-the-art methods in terms of accuracy and computational efficiency. Furthermore, qualitative analysis reveals that the learned embeddings capture meaningful semantic relationships that are consistent with human intuition. HCENs offer a promising framework for representing and reasoning with complex knowledge in a scalable and interpretable manner.",ai
"Feature and Interaction Importance (FII) methods are essential in supervised learning for assessing the relevance of input variables and their interactions in complex prediction models. In many domains, such as personalized medicine, local interpretations for individual predictions are often required, rather than global scores summarizing overall feature importance. Random Forests (RFs) are widely used in these settings, and existing interpretability methods typically exploit tree structures and split statistics to provide model-specific insights. However, theoretical understanding of local FII methods for RF remains limited, making it unclear how to interpret high importance scores for individual predictions. We propose a novel, local, model-specific FII method that identifies frequent co-occurrences of features along decision paths, combining global patterns with those observed on paths specific to a given test point. We prove that our method consistently recovers the true local signal features and their interactions under a Locally Spike Sparse (LSS) model and also identifies whether large or small feature values drive a prediction. We illustrate the usefulness of our method and theoretical results through simulation studies and a real-world data example.",human
"This paper proposes a structure-aware decoding method based on large language models to address the difficulty of traditional approaches in maintaining both semantic integrity and structural consistency in nested and overlapping entity extraction tasks. The method introduces a candidate span generation mechanism and structured attention modeling to achieve unified modeling of entity boundaries, hierarchical relationships, and cross-dependencies. The model first uses a pretrained language model to obtain context-aware semantic representations, then captures multi-granular entity span features through candidate representation combinations, and introduces hierarchical structural constraints during decoding to ensure consistency between semantics and structure. To enhance stability in complex scenarios, the model jointly optimizes classification loss and structural consistency loss, maintaining high recognition accuracy under multi-entity co-occurrence and long-sentence dependency conditions. Experiments conducted on the ACE 2005 dataset demonstrate significant improvements in Accuracy, Precision, Recall, and F1-Score, particularly in nested and overlapping entity recognition, where the model shows stronger boundary localization and structural modeling capability. This study verifies the effectiveness of structure-aware decoding in complex semantic extraction tasks, provides a new perspective for developing language models with hierarchical understanding, and establishes a methodological foundation for high-precision information extraction.",human
"We investigate the problem of distributionally robust optimization (DRO) in machine learning, specifically focusing on scenarios with limited data and potential covariate shift. Traditional DRO methods often rely on asymptotic arguments and may exhibit poor performance when the available data is scarce or the test distribution deviates significantly from the training distribution. We propose a novel DRO framework leveraging a hybrid approach that combines moment-based ambiguity sets with kernel mean embedding discrepancy measures. This allows us to construct ambiguity sets that are data-adaptive and capture both distributional shape and potential domain shifts. Our method, termed Kernelized Moment-Based Robust Optimization (KMBRO), minimizes the worst-case expected loss within the constructed ambiguity set, solved via a tractable semi-definite program. We derive finite-sample performance guarantees for KMBRO and demonstrate its superior robustness properties compared to existing DRO techniques through extensive numerical experiments on synthetic and real-world datasets, including image classification and time-series forecasting. Our results demonstrate that KMBRO achieves consistently lower out-of-sample risk in the presence of distributional uncertainty, particularly when training data is limited and covariate shift is prevalent. Furthermore, KMBRO exhibits improved stability and generalization performance compared to standard empirical risk minimization and existing DRO formulations.",ai
"Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.",human
"Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.",human
"We address the problem of accurate and efficient lesion segmentation in medical imaging, specifically focusing on Magnetic Resonance Imaging (MRI) of the brain. Manual segmentation, the current gold standard, is time-consuming and susceptible to inter-observer variability. We propose a novel deep learning architecture, a spatially-aware convolutional neural network (SA-CNN), that leverages contextual information and anatomical priors to improve segmentation performance. The SA-CNN incorporates a multi-scale feature extraction pathway combined with an attention mechanism that dynamically weights feature maps based on their spatial relevance to the target lesion. This architecture is trained end-to-end using a dataset of T1-weighted MRI scans with expert-annotated lesions. We evaluate our method on a held-out test set comparing it against state-of-the-art segmentation algorithms, including U-Net and V-Net. Experimental results demonstrate that the SA-CNN achieves a significantly higher Dice Similarity Coefficient (DSC) of 0.82 ± 0.05, compared to 0.75 ± 0.08 for U-Net and 0.78 ± 0.06 for V-Net. Furthermore, the SA-CNN exhibits a reduced false positive rate and improved boundary delineation. These findings indicate that the proposed spatially-aware architecture offers a promising approach for automated and accurate lesion segmentation in medical imaging, potentially leading to improved diagnostic accuracy and reduced clinical workload.",ai
"Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.",human
"We address the challenge of robust causal structure learning from observational data in the presence of latent confounders and sample selection bias. Existing constraint-based and score-based methods often falter under such conditions, leading to inaccurate or incomplete causal graphs. We propose a novel hybrid approach, combining the strengths of both methodologies. Our method, termed ""Selection-Aware Causal Discovery with Latent Variables"" (SACD-LV), first employs a modified PC algorithm incorporating a selection bias correction term derived from observed data patterns. This initial stage identifies potentially confounded or biased relationships. Subsequently, a score-based search, utilizing a novel penalized Bayesian Information Criterion (BIC) tailored to account for latent confounders and selection effects, refines the graph structure. The penalized BIC incorporates terms to penalize the complexity of models with added latent variables and selection mechanisms. We evaluate SACD-LV on synthetic datasets with varying degrees of confounding and selection bias. Results demonstrate that SACD-LV significantly outperforms state-of-the-art algorithms in terms of structural Hamming distance (SHD) and false discovery rate (FDR) in recovering the true causal graph, particularly when dealing with high levels of latent confounding and significant selection pressures. Specifically, SACD-LV achieved a 30-50% reduction in SHD compared to baseline methods in challenging simulated environments. These findings highlight the efficacy of SACD-LV as a robust and accurate tool for causal discovery in complex observational settings.",ai
"We address the challenge of distributional robustness in machine learning models when faced with uncertainty in training data. Specifically, we focus on optimizing models that are robust to adversarial perturbations of the input distribution, a critical concern in real-world deployments where data can be corrupted or manipulated. We propose a novel robust optimization framework that incorporates a data-dependent uncertainty set constructed using Wasserstein distance. This uncertainty set is dynamically adjusted based on the observed data distribution, allowing the model to adapt to different levels of data corruption. Our method leverages a primal-dual approach to efficiently solve the resulting min-max optimization problem. We derive a closed-form solution for the inner maximization problem under certain assumptions, significantly reducing the computational burden. Furthermore, we introduce a regularization term that encourages sparsity in the adversarial perturbation, promoting models that are less sensitive to irrelevant noise. We evaluate our approach on several benchmark datasets for image classification and natural language processing, demonstrating superior performance compared to existing robust optimization techniques. Empirical results show that our method achieves a significant improvement in robustness against adversarial attacks, while maintaining comparable or better accuracy on clean data. Furthermore, we demonstrate the adaptability of our data-dependent uncertainty set, showing that it effectively adjusts to different levels of data corruption, resulting in consistently robust performance. The findings suggest a promising avenue for developing more reliable and trustworthy machine learning models.",ai
"Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. Our central finding challenges model scaling assumptions: the quality of schema design has a greater impact on QA precision than model capacity. We establish three evidence-based principles: normalization that preserves context, semantic naming that reduces ambiguity, and consistent temporal anchoring. Our best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieves 80.39 EM, a 16.8\% improvement over the baseline (68.89 EM).",human
"We address the challenge of training machine learning models on decentralized datasets while guaranteeing differential privacy. Federated learning provides a framework for collaborative model training without direct data sharing, but is still vulnerable to privacy breaches through model updates. We propose a novel differentially private federated learning algorithm, FedSAM-DP, which leverages the Sharpness-Aware Minimization (SAM) optimization technique. FedSAM-DP incorporates differential privacy mechanisms into both the gradient clipping and noise addition steps of SAM, effectively bounding the sensitivity of the update process. We rigorously analyze the privacy guarantees of FedSAM-DP, deriving explicit bounds on the privacy loss in terms of -differential privacy. Empirical evaluations conducted on image classification and natural language processing tasks demonstrate that FedSAM-DP achieves significantly improved accuracy compared to standard differentially private federated learning algorithms, particularly at stringent privacy budgets. Specifically, on the CIFAR-10 dataset, FedSAM-DP achieves a 5% accuracy improvement at compared to conventional differentially private SGD. Our results highlight the effectiveness of incorporating sharpness awareness into privacy-preserving federated learning, offering a promising avenue for training robust and privacy-preserving models in decentralized settings.",ai
"In this work, we conduct systematic research in a challenging area: the automatic evaluation of book-length stories (>100K tokens). Our study focuses on two key questions: (1) understanding which evaluation aspects matter most to readers, and (2) exploring effective methods for evaluating lengthy stories. We introduce the first large-scale benchmark, LongStoryEval, comprising 600 newly published books with an average length of 121K tokens (maximum 397K). Each book includes its average rating and multiple reader reviews, presented as critiques organized by evaluation aspects. By analyzing all user-mentioned aspects, we propose an evaluation criteria structure and conduct experiments to identify the most significant aspects among the 8 top-level criteria. For evaluation methods, we compare the effectiveness of three types: aggregation-based, incremental-updated, and summary-based evaluations. Our findings reveal that aggregation- and summary-based evaluations perform better, with the former excelling in detail assessment and the latter offering greater efficiency. Building on these insights, we further propose NovelCritique, an 8B model that leverages the efficient summary-based framework to review and score stories across specified aspects. NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations. Our datasets and codes are available at https://github.com/DingyiYang/LongStoryEval.",human
"We investigate the problem of distributionally robust optimization (DRO) within machine learning models, specifically focusing on neural network classifiers. Traditional empirical risk minimization (ERM) is susceptible to performance degradation under distribution shift, motivating the need for robust training strategies. We propose a novel DRO framework utilizing a Wasserstein ambiguity set centered around the empirical distribution of the training data. This framework incorporates a learned metric within the Wasserstein distance calculation, allowing the model to adapt the notion of distributional similarity based on feature relevance. The metric is learned jointly with the model parameters via an adversarial training procedure, where an adversary seeks to maximize the worst-case loss within the ambiguity set. We derive a tractable approximation of the inner maximization problem using a Lagrangian dual formulation. Empirical evaluations on benchmark image classification datasets (CIFAR-10, CIFAR-100) and a text classification dataset (AG News) demonstrate that our approach significantly improves robustness against various types of adversarial attacks and common corruptions, compared to ERM and standard DRO techniques with fixed metrics. The proposed method achieves comparable or superior clean accuracy while providing substantially improved worst-case performance, highlighting its effectiveness in mitigating the impact of distributional uncertainty. We provide ablation studies analyzing the impact of the learned metric and the size of the ambiguity set on overall performance.",ai
"Training data detection is critical for enforcing copyright and data licensing, as Large Language Models (LLM) are trained on massive text corpora scraped from the internet. We present SPECTRA, a watermarking approach that makes training data reliably detectable even when it comprises less than 0.001% of the training corpus. SPECTRA works by paraphrasing text using an LLM and assigning a score based on how likely each paraphrase is, according to a separate scoring model. A paraphrase is chosen so that its score closely matches that of the original text, to avoid introducing any distribution shifts. To test whether a suspect model has been trained on the watermarked data, we compare its token probabilities against those of the scoring model. We demonstrate that SPECTRA achieves a consistent p-value gap of over nine orders of magnitude when detecting data used for training versus data not used for training, which is greater than all baselines tested. SPECTRA equips data owners with a scalable, deploy-before-release watermark that survives even large-scale LLM training.",human
"The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.",human
"Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.",human
"**Abstract:** Transformer models have achieved state-of-the-art performance across a wide spectrum of Natural Language Processing (NLP) tasks. However, the computational cost associated with training and deploying these models, particularly with respect to long sequence lengths, presents a significant obstacle for resource-constrained environments and real-time applications. This work investigates strategies for enhancing the efficiency of Transformer models without sacrificing performance on downstream NLP tasks. We propose a novel attention mechanism that leverages a learned sparse approximation of the full attention matrix, focusing computational resources on the most relevant contextual information. This approach, which we term ""Adaptive Sparse Attention"" (ASA), dynamically identifies and attends to a subset of key tokens based on learned similarity scores. We evaluate ASA on several benchmark NLP datasets, including text classification, machine translation, and question answering. Empirical results demonstrate that ASA achieves comparable performance to full attention Transformers while requiring significantly fewer computational resources, specifically reducing the quadratic complexity of the attention mechanism to near-linear complexity. Furthermore, we observe a noticeable improvement in training speed and memory footprint, facilitating the training of larger and more complex Transformer architectures. These findings suggest that ASA represents a promising avenue for developing more efficient and scalable Transformer models for NLP.",ai
"Reinforcement Learning with Verifiable Rewards (RLVR) significantly enhances the reasoning capability of Large Language Models (LLMs). Current RLVR approaches typically conduct training across all generated tokens, but neglect to explore which tokens (e.g., prefix tokens) actually contribute to reasoning. This uniform training strategy spends substantial effort on optimizing low-return tokens, which in turn impedes the potential improvement from high-return tokens and reduces overall training effectiveness. To address this issue, we propose a novel RLVR approach called Progressive Prefix-token Policy Optimization (PPPO), which highlights the significance of the prefix segment of generated outputs. Specifically, inspired by the well-established human thinking theory of Path Dependence, where early-stage thoughts substantially constrain subsequent thinking trajectory, we identify an analogous phenomenon in LLM reasoning termed Beginning Lock-in Effect (BLE). PPPO leverages this finding by focusing its optimization objective on the prefix reasoning process of LLMs. This targeted optimization strategy can positively influence subsequent reasoning processes, and ultimately improve final results. To improve the learning effectiveness of LLMs on how to start reasoning with high quality, PPPO introduces two training strategies: (a) Progressive Prefix Retention, which shapes a progressive learning process by increasing the proportion of retained prefix tokens during training; (b) Continuation Accumulated Reward, which mitigates reward bias by sampling multiple continuations for one prefix token sequence, and accumulating their scores as the reward signal. Extensive experimental results on various reasoning tasks demonstrate that our proposed PPPO outperforms representative RLVR methods, with the accuracy improvements of 18.02% on only 26.17% training tokens.",human
"### Abstract The effective representation of complex relational knowledge remains a significant challenge in enabling automated reasoning and decision-making. Existing knowledge representation formalisms often struggle to balance expressivity with computational tractability, leading to limitations in handling intricate dependencies and uncertain information. This paper introduces a novel approach, **Relational Embedding with Probabilistic Logic (REPL)**, which combines the strengths of relational embeddings and probabilistic logic. REPL utilizes graph neural networks to learn vector representations of entities and relations from knowledge graphs, capturing complex relational patterns. These embeddings are then integrated into a probabilistic logic framework, enabling reasoning under uncertainty and the propagation of probabilistic beliefs through the knowledge graph. Specifically, we define probabilistic logic rules that operate on the embedded representations, allowing for the inference of new facts and the assessment of confidence in existing assertions. We evaluate REPL on several benchmark knowledge graph completion and reasoning tasks, demonstrating its superior performance compared to state-of-the-art methods. Results show that REPL achieves significant improvements in link prediction accuracy and the ability to infer valid conclusions from incomplete or noisy knowledge, while maintaining computational efficiency. The proposed approach offers a promising avenue for advancing knowledge representation and reasoning in complex domains.",ai
"We address the challenge of robust causal effect estimation in observational studies when facing unmeasured confounding and non-linear treatment effects. Traditional methods, such as propensity score matching and inverse probability weighting, can exhibit significant bias under these conditions. We propose a novel approach, the Causal Kernelized Bayesian Neural Network (CKBNN), which integrates the representational power of kernel methods and Bayesian neural networks to model complex relationships between confounders, treatment, and outcome. Specifically, CKBNN employs a neural network to learn a high-dimensional, non-linear feature mapping of confounders, subsequently used to construct a kernel function. This kernel then facilitates a Bayesian regression framework for estimating the conditional average treatment effect (CATE). The Bayesian treatment allows for uncertainty quantification and improved robustness to model misspecification. We evaluate CKBNN on both synthetic and semi-synthetic datasets, demonstrating superior performance compared to state-of-the-art methods, including kernel matching, deep learning-based causal inference, and Bayesian additive regression trees (BART). Our results indicate that CKBNN provides more accurate and reliable CATE estimates, particularly in settings with strong unmeasured confounding and non-linear treatment-confounder interactions, while also providing well-calibrated uncertainty estimates.",ai
"Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.",human
"Vision Language Models (VLMs) have undergone significant advancements, particularly with the emergence of mobile-oriented VLMs, which offer a wide range of application scenarios. However, the substantial computational requirements for training these models present a significant obstacle to their practical application. To address this issue, Low-Rank Adaptation (LoRA) has been proposed. Nevertheless, the standard LoRA with a fixed rank lacks sufficient capability for training mobile VLMs that process both text and image modalities. In this work, we introduce HyDRA, a parameter-efficient fine-tuning framework designed to implement hierarchical and dynamic rank scheduling for mobile VLMs. This framework incorporates two essential optimization strategies: (1) hierarchical optimization, which involves a coarse-grained approach that assigns different ranks to various layers, as well as a fine-grained method that adjusts ranks within individual layers, and (2) dynamic adjustment, which employs an end-to-end automatic optimization using a lightweight performance model to determine and adjust ranks during the fine-tuning process. Comprehensive experiments conducted on popular benchmarks demonstrate that HyDRA consistently outperforms the baseline, achieving a 4.7\% improvement across various model sizes without increasing the number of trainable parameters. In some tasks, it even surpasses full-parameter fine-tuning.",human
"Polymers exhibit complex architectures and diverse properties that place them at the center of contemporary research in chemistry and materials science. As conventional computational techniques, even multi-scale ones, struggle to capture this complexity, quantum computing offers a promising alternative framework for extracting structure-property relationships. Noisy Intermediate-Scale Quantum (NISQ) devices are commonly used to explore the implementation of algorithms, including quantum neural networks for classification tasks, despite ongoing debate regarding their practical impact. We present a hybrid classical-quantum formalism that couples a classical deep neural network for polymer featurization with a single-photon-based quantum classifier native to photonic quantum computing. This pipeline successfully classifies polymer species by their optical gap, with performance in line between CPU-based noisy simulations and a proof-of-principle run on Quandela's Ascella quantum processor. These findings demonstrate the effectiveness of the proposed computational workflow and indicate that chemistryfrelated classification tasks can already be tackled under the constraints of today's NISQ devices.",human
"Recent advancements in machine learning (ML) have yielded powerful models capable of addressing complex problems across diverse domains. However, these advancements often necessitate access to sensitive data, raising significant privacy concerns. Naive application of ML algorithms on such data can inadvertently leak private information, undermining user trust and potentially violating regulatory frameworks. This work investigates the challenge of training accurate ML models while preserving the privacy of the underlying training data. We propose a novel privacy-preserving federated learning framework that combines differential privacy with secure aggregation techniques. Our method employs a client-side differentially private mechanism to add calibrated noise to local model updates before aggregation. Crucially, we incorporate a secure aggregation protocol based on multi-party computation to ensure that the central server only receives the aggregated noisy updates, preventing it from observing individual contributions and further mitigating privacy risks. Empirical evaluation on several benchmark datasets demonstrates that our framework achieves a compelling trade-off between model accuracy and privacy guarantees. Specifically, we observe only a modest decrease in model performance compared to traditional federated learning, while providing strong theoretical privacy guarantees as quantified by the privacy loss parameter (ε). Furthermore, we analyze the impact of different privacy parameters and demonstrate the robustness of our approach against various inference attacks. The results highlight the practical applicability of our framework for deploying privacy-preserving ML in real-world scenarios.",ai
"We investigate the efficacy of transformer-based architectures for sequence-to-sequence learning in natural language processing, specifically focusing on machine translation and text summarization tasks. While recurrent neural networks have historically been dominant, their inherent sequential processing limits parallelization and complicates the handling of long-range dependencies. Transformers, leveraging self-attention mechanisms, offer a potential solution to these challenges by allowing for parallel computation and direct interaction between all positions in the input and output sequences. This study employs a standard transformer model, trained on large-scale datasets using a masked language modeling objective, followed by fine-tuning on task-specific datasets. We present empirical results demonstrating that the transformer architecture achieves state-of-the-art performance on both English-to-French translation and abstractive text summarization benchmarks. Compared to established recurrent and convolutional models, the transformer exhibits superior translation quality, as measured by BLEU score, and produces more coherent and informative summaries, as evaluated by ROUGE scores. Furthermore, we analyze the learned attention weights, revealing the model's ability to capture intricate relationships between words and phrases, providing insights into its superior performance. Our findings underscore the potential of transformers as a robust and scalable solution for a wide range of NLP tasks requiring effective sequence modeling.",ai
"We investigate the problem of efficient exploration in sparse-reward reinforcement learning environments with high-dimensional, continuous state spaces. Conventional exploration strategies often struggle in such scenarios due to the vastness of the state space and the infrequency of informative feedback signals. To address this challenge, we propose a novel approach integrating intrinsic motivation derived from a state-visitation-based curiosity mechanism with a hierarchical reinforcement learning framework. The proposed method, termed Hierarchical Curiosity-Driven Exploration (HCDE), decomposes the learning task into a hierarchy of sub-goals. A high-level policy learns to navigate towards regions of high curiosity, while a low-level policy executes actions to reach these sub-goals. Crucially, the curiosity signal is modulated by a learned estimate of prediction error, focusing exploration on regions where the agent’s understanding is lacking. Experiments conducted on a suite of challenging simulated robotic manipulation tasks demonstrate that HCDE significantly outperforms state-of-the-art exploration algorithms, achieving substantially higher reward rates and faster convergence. Furthermore, ablation studies validate the contribution of each component of HCDE, highlighting the synergistic effect of hierarchical decomposition and curiosity-driven exploration. These results suggest that HCDE provides a promising direction for enabling more effective and efficient learning in complex, sparse-reward environments.",ai
"Graph Neural Networks (GNNs) have demonstrated efficacy in modeling relational data across diverse domains. However, existing GNN architectures often struggle to capture long-range dependencies and complex structural patterns inherent in large, heterogeneous graphs. This limitation hinders their performance in tasks demanding comprehensive contextual understanding, such as knowledge graph reasoning and complex network analysis. We propose a novel GNN framework, Hierarchical Relational Aggregation Network (HRAN), designed to address these challenges. HRAN incorporates a multi-level aggregation mechanism that iteratively combines node representations at varying scales, capturing both local and global graph characteristics. Specifically, HRAN employs a hierarchical clustering algorithm to partition the graph into subgraphs of increasing granularity, followed by a recursive message-passing process within each subgraph. This approach enables the propagation of information across distant nodes, facilitating the discovery of intricate relational patterns. Empirical evaluation on benchmark datasets for node classification and link prediction demonstrates that HRAN significantly outperforms state-of-the-art GNN architectures. For instance, HRAN achieves a 5.3% improvement in micro-F1 score on the OGBN-Arxiv node classification task and a 3.8% gain in mean reciprocal rank on the WN18RR link prediction benchmark. These results underscore HRAN's ability to effectively model complex relational dependencies and enhance performance in graph-based learning tasks.",ai
"We investigate the efficacy of Transformer-based architectures in addressing challenges inherent in Natural Language Processing (NLP), specifically focusing on sequence-to-sequence tasks characterized by long-range dependencies. Traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) often struggle to effectively capture and model these dependencies, leading to suboptimal performance in tasks such as machine translation and text summarization. This work explores the application of the Transformer model, which leverages self-attention mechanisms to directly relate any two words in a sequence, thereby mitigating the limitations of previous architectures. We present a series of experiments comparing Transformer models of varying sizes and configurations to established RNN and CNN baselines on benchmark datasets for machine translation (WMT 2014 English-German) and abstractive text summarization (CNN/DailyMail). The results demonstrate that Transformer models achieve significant improvements in translation quality, as measured by BLEU score, and summarization performance, quantified by ROUGE scores. Furthermore, we analyze the learned attention weights within the Transformer architecture to gain insights into its ability to capture intricate relationships between words and phrases, highlighting its capacity to model complex linguistic phenomena. These findings underscore the potential of Transformer models as a robust and effective solution for a wide range of NLP tasks.",ai
"We address the challenge of achieving efficient and robust coordination in decentralized multi-agent systems operating within dynamically changing environments characterized by communication constraints. Specifically, we focus on scenarios where agents possess incomplete and asynchronous information, making traditional centralized coordination strategies infeasible. We propose a novel Decentralized Partially Observable Markov Decision Process (Dec-POMDP) framework, incorporating a belief-space compression technique based on variational autoencoders (VAEs) coupled with a graph neural network (GNN) for inter-agent communication. This method allows each agent to maintain a compressed belief representation of the global state and infer the intentions of other agents through message passing on the GNN. We evaluate our approach on a cooperative navigation task with varying levels of communication bandwidth and environmental stochasticity. Results demonstrate that our VAE-GNN Dec-POMDP agent significantly outperforms benchmark algorithms, including independent Q-learning and communication-based methods utilizing fixed communication topologies. Furthermore, ablation studies reveal that the VAE-based belief compression enables agents to maintain relevant information within limited communication channels, while the GNN facilitates effective coordination by exploiting the learned relationship between compressed beliefs and agent actions. Our findings highlight the potential of leveraging learned representations and message passing for robust coordination in complex multi-agent systems.",ai
"We investigate the problem of sample inefficiency in reinforcement learning (RL) within sparse reward environments characterized by long-horizon dependencies. Specifically, we address the challenge of learning optimal policies when feedback signals are infrequent and delayed, hindering effective exploration and credit assignment. We propose a novel hierarchical reinforcement learning framework that integrates intrinsic motivation derived from a learned state representation with extrinsic, sparse rewards. Our method, termed Hierarchical Representation-Guided Exploration (HRGE), comprises a high-level manager that proposes subgoals in a learned latent space, and a low-level worker that executes actions to achieve these subgoals. The manager's policy is trained using a reward signal based on the similarity between the current state representation and the proposed subgoal representation, encouraging exploration of diverse state spaces. Extrinsic rewards are used to refine both the manager and worker policies. We evaluate HRGE on a suite of challenging sparse reward environments, demonstrating significant improvements in sample efficiency and asymptotic performance compared to state-of-the-art RL algorithms. Empirical results indicate that HRGE learns effective hierarchical policies capable of overcoming the limitations of sparse rewards and long horizons, achieving substantial gains in learning speed and final reward acquisition.",ai
"We address the problem of enhancing automated theorem proving performance in complex mathematical domains through the integration of learned guidance strategies. Traditional theorem provers often rely on hand-crafted heuristics, leading to brittle performance and limited scalability. We propose a novel method that combines reinforcement learning with graph neural networks to predict promising proof steps within a given proof search state. Specifically, our approach represents proof states as directed acyclic graphs, where nodes correspond to logical formulas and edges represent inference steps. The graph neural network learns to embed these proof states and predict the probability of applying different inference rules, guiding the theorem prover toward successful proofs. We train this model using a curriculum learning approach, gradually increasing the complexity of the theorems presented. Empirical evaluation on a benchmark of propositional logic theorems demonstrates that our learned guidance strategy significantly improves the success rate and reduces the search time compared to baseline theorem provers employing standard heuristics such as unit preference and set-of-support. Furthermore, the learned strategy exhibits improved generalization capabilities, proving theorems that were previously intractable for the baseline systems. Our findings suggest the efficacy of combining reinforcement learning with graph neural networks for automated reasoning and represent a step towards more robust and efficient theorem proving systems.",ai
"As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.",human
"Graph Neural Networks (GNNs) have emerged as a powerful paradigm for learning representations of structured data, finding applications in diverse domains. However, training GNNs, particularly on large graphs, remains a significant computational challenge due to the memory footprint required for storing intermediate node embeddings during message passing. This work addresses the problem of memory efficiency in GNN training by introducing a novel stochastic sampling strategy for reducing the computation graph size. Our method, dubbed Node-Centric Subgraph Sampling (NCSS), focuses on constructing mini-batches centered around individual nodes and adaptively sampling their neighborhood based on node degree and edge importance. This strategy aims to preserve critical structural information while substantially decreasing the number of nodes and edges involved in each training iteration. We theoretically analyze the convergence properties of NCSS-trained GNNs, establishing bounds on the approximation error introduced by the sampling process. Empirically, we evaluate NCSS on several benchmark graph datasets, demonstrating significant reductions in memory consumption and training time compared to existing full-graph and subgraph sampling techniques. Results show that NCSS achieves comparable or superior performance to state-of-the-art GNN models, while requiring substantially less memory, thus enabling the training of deeper and more complex GNN architectures on large-scale graph datasets.",ai
"### Automatic Detection of Pulmonary Nodules via Multi-Scale Contextual Reasoning in Chest CT Scans Accurate and timely detection of pulmonary nodules in chest computed tomography (CT) scans is crucial for early diagnosis and management of lung cancer. However, the subtle appearance and anatomical variability of nodules pose significant challenges to radiologists, leading to potential oversights and diagnostic delays. This work introduces a novel approach for automatic pulmonary nodule detection, leveraging multi-scale contextual reasoning within a deep convolutional neural network (CNN) framework. Our method incorporates a 3D CNN backbone for feature extraction, followed by a multi-scale feature fusion module that integrates information from various receptive fields to capture the diverse characteristics of nodules and their surrounding lung parenchyma. A context-aware refinement network further enhances the discrimination between true nodules and false positives by explicitly modeling spatial relationships and contextual dependencies. Evaluated on the publicly available LUNA16 dataset, our approach achieves a state-of-the-art Continuous Average Recall (CAR) score of 0.912 at 1/8 false positives per scan, outperforming existing methods. Ablation studies demonstrate the effectiveness of the proposed multi-scale feature fusion and context-aware refinement components. The results indicate the potential of our method to significantly improve the accuracy and efficiency of pulmonary nodule detection in clinical practice.",ai
"We address the challenge of effectively integrating diverse knowledge sources within a unified representation to improve downstream reasoning performance. Existing knowledge representation methods often struggle with the inherent heterogeneity and potential inconsistencies across sources, leading to suboptimal knowledge utilization. To mitigate these limitations, we propose a novel framework, termed ""Knowledge Graph Fusion with Attentive Relation Embedding (KGF-ARE),"" which leverages graph neural networks (GNNs) to dynamically integrate knowledge from multiple knowledge graphs. KGF-ARE utilizes an attention mechanism at the relation embedding level to selectively aggregate information from different source graphs based on their relevance to the target task. The attention weights are learned end-to-end during the training process, enabling the model to adaptively prioritize informative relations while mitigating the influence of noisy or irrelevant ones. We evaluate KGF-ARE on several benchmark knowledge graph completion and question answering tasks. Empirical results demonstrate that KGF-ARE outperforms state-of-the-art methods by achieving significant improvements in link prediction accuracy (Mean Rank reduced by 15%) and question answering F1-score (increased by 8%). These findings highlight the efficacy of KGF-ARE in effectively fusing heterogeneous knowledge graphs and enhancing reasoning capabilities.",ai
"We investigate the problem of automated segmentation of pulmonary nodules in computed tomography (CT) scans, a critical task for early lung cancer diagnosis. Manual segmentation is labor-intensive and subject to inter-observer variability, motivating the development of robust and accurate automated methods. Existing approaches often struggle with nodules exhibiting subtle contrast differences, irregular shapes, or those located near the pleura or other anatomical structures. We propose a novel deep learning architecture, termed Multi-Scale Contextual Segmentation Network (MSCS-Net), incorporating a dual-branch encoder-decoder structure. One branch captures fine-grained detail using high-resolution feature maps, while the second branch leverages dilated convolutions to extract broader contextual information. These features are fused at multiple scales within the decoder, facilitating precise boundary delineation and reducing false positives. The MSCS-Net was trained and evaluated on the LIDC-IDRI dataset, comprising 1018 thoracic CT scans. Our method achieved a Dice Similarity Coefficient (DSC) of 0.86 and a Jaccard Index of 0.75, outperforming several state-of-the-art segmentation algorithms. Furthermore, ablation studies demonstrate the efficacy of the multi-scale contextual feature integration. The results indicate the potential of MSCS-Net for improving the accuracy and efficiency of pulmonary nodule segmentation, thereby assisting radiologists in lung cancer screening and diagnosis.",ai
"We address the problem of robust causal effect estimation in the presence of unobserved confounding and measurement error. Existing methods often rely on strong assumptions about the functional form of causal relationships or the nature of the confounding variables, which may not hold in real-world settings. We propose a novel instrumental variable (IV) approach, termed Moment-Restricted Instrumental Variable Regression (MR-IVR), that leverages moment restrictions derived from observational data to identify and estimate causal effects. MR-IVR exploits the orthogonality between the instrument and the error term in the causal model, while simultaneously incorporating moment restrictions to mitigate the impact of weak instruments and measurement error. We formulate the estimation procedure as a convex optimization problem, allowing for efficient and scalable computation. We demonstrate the efficacy of MR-IVR through extensive simulations and on benchmark datasets from the economics and epidemiology literature. Results indicate that MR-IVR consistently outperforms state-of-the-art causal inference methods, particularly in scenarios with weak instruments, high levels of unobserved confounding, and substantial measurement error. Furthermore, MR-IVR provides reliable uncertainty quantification for the estimated causal effects. This work contributes to the development of more robust and reliable causal inference techniques applicable to complex observational datasets.",ai
"This paper presents a unified framework, for the detection, classification, and preliminary localization of anomalies in water distribution networks using multivariate statistical analysis. The approach, termed SICAMS (Statistical Identification and Classification of Anomalies in Mahalanobis Space), processes heterogeneous pressure and flow sensor data through a whitening transformation to eliminate spatial correlations among measurements. Based on the transformed data, the Hotelling's statistic is constructed, enabling the formulation of anomaly detection as a statistical hypothesis test of network conformity to normal operating conditions. It is shown that Hotelling's statistic can serve as an integral indicator of the overall ""health"" of the system, exhibiting correlation with total leakage volume, and thereby enabling approximate estimation of water losses via a regression model. A heuristic algorithm is developed to analyze the time series and classify detected anomalies into abrupt leaks, incipient leaks, and sensor malfunctions. Furthermore, a coarse leak localization method is proposed, which ranks sensors according to their statistical contribution and employs Laplacian interpolation to approximate the affected region within the network. Application of the proposed framework to the BattLeDIM L-Town benchmark dataset demonstrates high sensitivity and reliability in leak detection, maintaining robust performance even under multiple leaks. These capabilities make the method applicable to real-world operational environments without the need for a calibrated hydraulic model.",human
"Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.",human
"Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.",human
"We investigate the efficacy of Transformer-based architectures in natural language processing tasks, focusing on methods for improving their computational efficiency and performance in low-resource settings. A primary challenge in deploying large Transformer models is their quadratic complexity with respect to sequence length, hindering application to long documents and real-time processing. We explore a novel adaptation of sparse attention mechanisms, specifically learned sparsity patterns, integrated within a pre-trained BERT model. This approach aims to reduce computational overhead while preserving contextual understanding. Furthermore, we introduce a transfer learning strategy leveraging multilingual pre-training to enhance performance on downstream tasks in languages with limited labeled data. Empirical evaluation on a suite of benchmark datasets, including text classification and question answering, demonstrates that our sparse attention variant achieves a significant reduction in computational cost (up to 30% decrease in inference time) with minimal degradation in accuracy (less than 1% drop in F1 score). The proposed transfer learning strategy yields substantial improvements (average 5% increase in F1 score) compared to training from scratch in low-resource languages, indicating the potential for efficient and effective deployment of Transformer models in diverse NLP applications.",ai
"Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery. We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations. In MASTER, a multimodal system translates natural language into density functional theory workflows, while higher-level reasoning agents guide discovery through a hierarchy of strategies, including a single agent baseline and three multi-agent approaches: peer review, triage-ranking, and triage-forms. Across two chemical applications, CO adsorption on Cu-surface transition metal (M) adatoms and on M-N-C catalysts, reasoning-driven exploration reduces required atomistic simulations by up to 90% relative to trial-and-error selection. Reasoning trajectories reveal chemically grounded decisions that cannot be explained by stochastic sampling or semantic bias. Altogether, multi-agent collaboration accelerates materials discovery and marks a new paradigm for autonomous scientific exploration.",human
